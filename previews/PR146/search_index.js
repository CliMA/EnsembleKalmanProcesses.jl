var documenterSearchIndex = {"docs":
[{"location":"observations/#Observations","page":"Observations","title":"Observations","text":"","category":"section"},{"location":"observations/","page":"Observations","title":"Observations","text":"The Observations object is used to store the truth for convenience of the user. The ingredients are","category":"page"},{"location":"observations/","page":"Observations","title":"Observations","text":"Samples of the data Vector{Vector{Float}} or Array{Float, 2}. If provided as a 2D array, the samples must be provided as columns. They are stored internally as Vector{Vector{Float}}\nAn optional covariance matrix can be provided.\nThe names of the data in this object as a String or Vector{String}","category":"page"},{"location":"observations/","page":"Observations","title":"Observations","text":"The empirical mean is calculated automatically. If a covariance matrix is not provided, then the empirical covariance is also calculated automatically.","category":"page"},{"location":"observations/#A-simple-example:","page":"Observations","title":"A simple example:","text":"","category":"section"},{"location":"observations/","page":"Observations","title":"Observations","text":"Here is a typical construction of the object:","category":"page"},{"location":"observations/","page":"Observations","title":"Observations","text":"μ = zeros(5)\nΓy = rand(5, 5)\nΓy = Γy' * Γy\nyt = rand(MvNormal(μ, Γy), 100) # generate 100 samples\nname = \"zero-mean mvnormal\"\n\ntrue_data = Observations.Observation(yt, Γy, name)","category":"page"},{"location":"observations/","page":"Observations","title":"Observations","text":"Currently, the data is retrieved by accessing the stored variables, e.g the fifth data sample is given by truth_data.samples[5], or the covariance matrix by truth_data.cov.","category":"page"},{"location":"ensemble_kalman_sampler/#Ensemble-Kalman-Sampling","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampling","text":"","category":"section"},{"location":"ensemble_kalman_sampler/#What-Is-It-and-What-Does-It-Do?","page":"Ensemble Kalman Sampler","title":"What Is It and What Does It Do?","text":"","category":"section"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"The Ensemble Kalman Sampler (EKS) (Garbuno-Inigo et al, 2019, Cleary et al, 2020, Garbuno-Inigo et al, 2020) is a derivative-free method that can be used to solve the inverse problem of finding the optimal model parameters given noisy data. In contrast to Ensemble Kalman Inversion (EKI) (Iglesias et al, 2013), the EKS method approximately samples from the posterior distribution; that is, EKS provides both point estimation (through the mean of the final ensemble) and uncertainty quantification (through the covariance of the final ensemble), unlike EKI, which only provides the former.","category":"page"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"The EKS is an interacting particle system in stochastic differential equation form, and it is based on a dynamic which transforms an arbitrary initial probability distribution into an approximation of the desired posterior distribution over an infinite time horizon – see Garbuno-Inigo et al, 2019, for a comprehensive description of the method. While there are noisy variants of the standard EKI, EKS differs from them in its noise structure (as its noise is added in parameter space, not in  data space), and its update rule explicitly accounts for the prior (rather than having it enter through initialization). The EKS algorithm can be understood as well as an affine invariant system of interacting particles (Garbuno-Inigo et al, 2020) for which a finite-sample correction is introduced to overcome its computational finite-sample implementation. The finite-sample corrected version of EKS is referred to as ALDI for its acronym in (Garbuno-Inigo et al, 2020). ","category":"page"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"Note that in practice the approximate posterior characterization through EKS needs more iterations, and thus more forward model evaluations, than EKI. This is because of the discrete-time implementation of the EKS diffusion process and the need to maintain a stable interacting particle system. However, the posterior approximation through EKS is obtained with less computational effort than a typical Markov Chain Monte Carlo (MCMC) like Metropolis-Hastings.","category":"page"},{"location":"ensemble_kalman_sampler/#Problem-Formulation","page":"Ensemble Kalman Sampler","title":"Problem Formulation","text":"","category":"section"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"The data y and parameter vector theta are assumed to be related according to:","category":"page"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"    y = mathcalG(theta) + eta ","category":"page"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"where mathcalG  mathbbR^p rightarrow mathbbR^d denotes the forward map, y in mathbbR^d is the vector of observations, and eta is the observational noise, which is assumed to be drawn from a d-dimensional Gaussian with distribution mathcalN(0 Gamma_y). The objective of the inverse problem is to compute the unknown parameters theta given the observations y, the known forward map mathcalG, and noise characteristics eta of the process. The full Bayesian characterization for the posterior under the EKS framework requires a p-dimensional prior Gaussian distribution mathcalN(m_theta Gamma_theta).","category":"page"},{"location":"ensemble_kalman_sampler/#Ensemble-Kalman-Sampling-Algorithm","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampling Algorithm","text":"","category":"section"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"The EKS is based on the following update equation for the parameter vector theta^(j)_n of ensemble member j at the n-iteration:","category":"page"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"beginaligned\ntheta_n+1^(* j) = theta_n^(j) - dfracDelta t_nJsum_k=1^Jlangle mathcalG(theta_n^(k)) - barmathcalG_n Gamma_y^-1(mathcalG(theta_n^(j)) - y) rangle theta_n^(k) + fracd+1J left(theta_n^(j) - bar theta_n right) - Delta t_n mathsfC(Theta_n) Gamma_theta^-1 theta_n + 1^(* j)  \ntheta_n + 1^j = theta_n+1^(* j) + sqrt2 Delta t_n mathsfC(Theta_n) xi_n^j \nendaligned","category":"page"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"where the subscript n=1 dots N_textit indicates the iteration, J is the ensemble size (i.e., the number of particles in the ensemble), Delta t_n is an internal adaptive time step (thus no need for the user to specify), Gamma_theta is the prior covariance, and xi_n^(j) sim mathcalN(0 mathrmI_p). barmathcalG_n is the ensemble mean of the forward map mathcalG(theta),","category":"page"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"barmathcalG_n = dfrac1Jsum_k=1^JmathcalG(theta_n^(k))","category":"page"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"The p times p matrix mathsfC(Theta_n), where Theta_n = lefttheta^(j)_nright_j=1^J is the set of all ensemble particles in the n-th iteration, denotes the empirical covariance between particles","category":"page"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"mathsfC(Theta_n) = frac1J sum_k=1^J (theta^(k)_n - bartheta_n) otimes (theta^(k)_n - bartheta_n)","category":"page"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"where bartheta_n is the ensemble mean of the particles,","category":"page"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"bartheta_n = dfrac1Jsum_k=1^Jtheta^(k)_n ","category":"page"},{"location":"ensemble_kalman_sampler/#Constructing-the-Forward-Map","page":"Ensemble Kalman Sampler","title":"Constructing the Forward Map","text":"","category":"section"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"At the core of the forward map mathcalG is the dynamical model PsimathbbR^p rightarrow mathbbR^o (running Psi is usually where the computational heavy-lifting is done), but the map mathcalG may include additional components such as a transformation of the (unbounded) parameters theta to a constrained domain the dynamical model can work with, or some post-processing of the output of Psi to generate the observations. For example, mathcalG may take the following form:","category":"page"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"mathcalG = mathcalH circ Psi circ mathcalT^-1","category":"page"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"where mathcalHmathbbR^o rightarrow mathbbR^d is the observation map and mathcalT is the transformation from the constrained to the unconstrained parameter space, such that mathcalT(phi)=theta. A family of standard transformations and their inverses are available in the ParameterDistributions module.","category":"page"},{"location":"ensemble_kalman_sampler/#How-to-Construct-an-Ensemble-Kalman-Sampler","page":"Ensemble Kalman Sampler","title":"How to Construct an Ensemble Kalman Sampler","text":"","category":"section"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"An EKS object can be created using the EnsembleKalmanProcess constructor by specifying the Sampler type. The constructor takes two arguments, the prior mean prior_mean and the prior covariance prior_cov.","category":"page"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"Creating an EKI object requires as arguments:","category":"page"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"An initial parameter ensemble – an array of size p × N_ens, where N_ens is the  ensemble size;\nThe mean value of the observed data – a vector of length d;\nThe covariance matrix of the observational noise – an array of size d × d;\nThe Sampler(prior_mean, prior_cov) process type, with the mean (a vector of length p) and the covariance (an array of size p x p) of the parameter's prior distribution","category":"page"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"The following example shows how an EKS object is instantiated. The mean of the observational data (obs_mean) and the covariance of the observational noise (obs_cov) are assumed to be defined previously in the code.","category":"page"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"using EnsembleKalmanProcesses\nusing EnsembleKalmanProcesses.ParameterDistributions  # required to create the prior\n\n# Construct prior (see `ParameterDistributions.jl` docs)\nprior = ParameterDistribution(...)\nprior_mean = mean(prior)\nprior_cov = cov(prior)\n\n# Construct initial ensemble\nN_ens = 50  # ensemble size\ninitial_ensemble = construct_initial_ensemble(prior, N_ens)\n\n# Construct ensemble Kalman process\neks_process = Sampler(prior_mean, prior_cov)\neks_obj = EnsembleKalmanProcess(initial_ensemble, obs_mean, obs_noise_cov, eks_process)","category":"page"},{"location":"ensemble_kalman_sampler/#Updating-the-ensemble","page":"Ensemble Kalman Sampler","title":"Updating the ensemble","text":"","category":"section"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"Once the EKS object eks_obj has been initialized, the initial ensemble of particles is iteratively updated by the update_ensemble! function, which takes as arguments the eks_obj and the evaluations of the forward model at each member of the current ensemble. In the following example, the forward map G maps a parameter to the corresponding data – this is done for each parameter in the ensemble, such that the resulting g_ens is of size d x N_ens. The update_ensemble! function then stores the updated ensemble as well as the evaluations of the forward map in eks_obj.","category":"page"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"A typical use of the update_ensemble! function given the EKS object eks_obj, the dynamical model Ψ, and the observation map H (the latter two are assumed to be defined elsewhere, e.g. in a separate module)  may look as follows:","category":"page"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"N_iter = 10 # Number of iterations\n\nfor n in 1:N_iter\n    θ_n = get_u_final(eks_obj) # Get current ensemble\n    ϕ_n = transform_unconstrained_to_constrained(prior, θ_n) # Transform parameters to physical/constrained space\n    G_n = [H(Ψ(ϕ_n[:, i])) for i in 1:J]  # Evaluate forward map\n    g_ens = hcat(G_n...)  # Reformat into `d x N_ens` matrix\n    update_ensemble!(eks_obj, g_ens) # Update ensemble\nend","category":"page"},{"location":"ensemble_kalman_sampler/#Solution","page":"Ensemble Kalman Sampler","title":"Solution","text":"","category":"section"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"The solution of the EKS algorithm is an approximate Gaussian distribution whose mean (θ_post) and covariance (Γ_post) can be extracted from the ''last ensemble'' (i.e., the ensemble after the last iteration). The sample mean of the last ensemble is also the \"optimal\" parameter (θ_optim) for the given calibration problem. These statistics can be accessed as follows:","category":"page"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"using Statistics\n\n# mean of the Gaussian distribution, also the optimal parameter for the calibration problem\nθ_post = mean(get_u_final(eks_obj), dims=2)\n# covariance of the Gaussian distribution\nΓ_post = cov(get_u_final(eks_obj), dims=2)","category":"page"},{"location":"API/Observations/#Observations","page":"Observations","title":"Observations","text":"","category":"section"},{"location":"API/Observations/","page":"Observations","title":"Observations","text":"CurrentModule = EnsembleKalmanProcesses.Observations","category":"page"},{"location":"API/Observations/","page":"Observations","title":"Observations","text":"Observation","category":"page"},{"location":"API/Observations/#EnsembleKalmanProcesses.Observations.Observation","page":"Observations","title":"EnsembleKalmanProcesses.Observations.Observation","text":"Observation{FT <: AbstractFloat}\n\nStructure that contains the observations\n\nFields\n\nsamples::Array{Vector{FT}, 1} where FT<:AbstractFloat\nvector of observational samples, each of length sample_dim\nobs_noise_cov::Union{Nothing, LinearAlgebra.UniformScaling{FT}, AbstractMatrix{FT}, FT} where FT<:AbstractFloat\ncovariance of the observational noise (assumed to be normally     distributed); sample_dim x sample_dim (where sample_dim is the number of     elements in each sample), or a scalar if the sample dim is 1. If not     supplied, obs_noise_cov is set to a diagonal matrix whose non-zero elements     are the variances of the samples, or to a scalar variance in the case of     1d samples. obs_noise_cov is set to nothing if only a single sample is     provided.\nmean::Union{AbstractVector{FT}, FT} where FT<:AbstractFloat\nsample mean\ndata_names::Union{String, AbstractVector{String}}\nnames of the data\n\n\n\n\n\n","category":"type"},{"location":"installation_instructions/#Installation","page":"Installation instructions","title":"Installation","text":"","category":"section"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"EnsembleKalmanProcesses.jl is a registered Julia package. You can install the latest version of EnsembleKalmanProcesses.jl through the built-in package manager. Press ] in the Julia REPL command prompt and","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"julia> ]\n(v1.5) pkg> add EnsembleKalmanProcesses\n(v1.5) pkg> instantiate","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"This will install the latest tagged release of the package.","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"info: But I wanna be on the bleeding edge...\nIf you want the most recent developer's version of the package thenjulia> ]\n(v1.5) pkg> add EnsembleKalmanProcesses#main\n(v1.5) pkg> instantiate","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"You can run the tests via the package manager by:","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"julia> ]\n(v1.5) pkg> test EnsembleKalmanProcesses","category":"page"},{"location":"installation_instructions/#Cloning-the-repository","page":"Installation instructions","title":"Cloning the repository","text":"","category":"section"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"If you are interested in getting your hands dirty and modifying the code then, you can also clone the repository and then instantiate, e.g.,","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"> cd EnsembleKalmanProcesses.jl\n> julia --project -e 'using Pkg; Pkg.instantiate()'","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"You can run the package's tests:","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"> julia --project -e 'using Pkg; Pkg.test()'","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"info: Do I need to clone the repository?\nMost times, cloning the repository in not a necessity. If you only wanna use the package's functionality then merely adding the packages as a dependency on your project will do the job.","category":"page"},{"location":"installation_instructions/#Building-the-documentation-locally","page":"Installation instructions","title":"Building the documentation locally","text":"","category":"section"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"Once the project is built, you can build the project documentation under the docs/ sub-project:","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"> julia --project=docs/ -e 'using Pkg; Pkg.develop(PackageSpec(path=pwd())); Pkg.instantiate()'\n> julia --project=docs/ docs/make.jl","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"The locally rendered HTML documentation can be viewed at docs/build/index.html","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"EditURL = \"https://github.com/CliMA/EnsembleKalmanProcesses.jl/blob/main/examples/LossMinimization/loss_minimization.jl\"","category":"page"},{"location":"literated/loss_minimization/#Minimization-of-simple-loss-functions","page":"Minimization Loss","title":"Minimization of simple loss functions","text":"","category":"section"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"First we load the required packages.","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"using Distributions, LinearAlgebra, Random, Plots\n\nusing EnsembleKalmanProcesses\nusing EnsembleKalmanProcesses.ParameterDistributions\n\nconst EKP = EnsembleKalmanProcesses","category":"page"},{"location":"literated/loss_minimization/#Loss-function-with-single-minimum","page":"Minimization Loss","title":"Loss function with single minimum","text":"","category":"section"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"Here, we minimize the loss function","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"G₁(u) = u - u_* ","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"where u is a 2-vector of parameters and u_* is given; here u_* = (-1 1).","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"u★ = [1, -1]\nG₁(u) = [sqrt((u[1] - u★[1])^2 + (u[2] - u★[2])^2)]\nnothing # hide","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"We set the seed for pseudo-random number generator for reproducibility.","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"rng_seed = 41\nRandom.seed!(rng_seed)\nnothing # hide","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"We set a stabilization level, which can aid the algorithm convergence","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"dim_output = 1\nstabilization_level = 1e-3\nΓ_stabilization = stabilization_level * Matrix(I, dim_output, dim_output)","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"The functional is positive so to minimize it we may set the target to be 0,","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"G_target = [0]\nnothing # hide","category":"page"},{"location":"literated/loss_minimization/#Prior-distributions","page":"Minimization Loss","title":"Prior distributions","text":"","category":"section"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"As we work with a Bayesian method, we define a prior. This will behave like an \"initial guess\" for the likely region of parameter space we expect the solution to live in.","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"prior_distributions = [Parameterized(Normal(0, 1)), Parameterized(Normal(0, 1))]\n\nconstraints = [[no_constraint()], [no_constraint()]]\n\nparameter_names = [\"u1\", \"u2\"]\n\nprior = ParameterDistribution(prior_distributions, constraints, parameter_names)","category":"page"},{"location":"literated/loss_minimization/#Calibration","page":"Minimization Loss","title":"Calibration","text":"","category":"section"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"We choose the number of ensemble members and the number of iterations of the algorithm","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"N_ensemble = 20\nN_iterations = 10\nnothing # hide","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"The initial ensemble is constructed by sampling the prior","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"initial_ensemble = EKP.construct_initial_ensemble(prior, N_ensemble; rng_seed = rng_seed)","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"We then initialize the Ensemble Kalman Process algorithm, with the initial ensemble, the target, the stabilization and the process type (for EKI this is Inversion, initialized with Inversion()).","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"ensemble_kalman_process = EKP.EnsembleKalmanProcess(initial_ensemble, G_target, Γ_stabilization, Inversion())","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"Then we calibrate by (i) obtaining the parameters, (ii) calculate the loss function on the parameters (and concatenate), and last (iii) generate a new set of parameters using the model outputs:","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"for i in 1:N_iterations\n    params_i = get_u_final(ensemble_kalman_process)\n\n    g_ens = hcat([G₁(params_i[:, i]) for i in 1:N_ensemble]...)\n\n    EKP.update_ensemble!(ensemble_kalman_process, g_ens)\nend","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"and visualize the results:","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"u_init = get_u_prior(ensemble_kalman_process)\n\nanim_unique_minimum = @animate for i in 1:N_iterations\n    u_i = get_u(ensemble_kalman_process, i)\n\n    plot(\n        [u★[1]],\n        [u★[2]],\n        seriestype = :scatter,\n        markershape = :star5,\n        markersize = 11,\n        markercolor = :red,\n        label = \"optimum u⋆\",\n    )\n\n    plot!(\n        u_i[1, :],\n        u_i[2, :],\n        seriestype = :scatter,\n        xlims = extrema(u_init[1, :]),\n        ylims = extrema(u_init[2, :]),\n        xlabel = \"u₁\",\n        ylabel = \"u₂\",\n        markersize = 5,\n        markeralpha = 0.6,\n        markercolor = :blue,\n        label = \"particles\",\n        title = \"EKI iteration = \" * string(i),\n    )\nend\nnothing # hide","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"The results show that the minimizer of G_1 is u=u_*.","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"gif(anim_unique_minimum, \"unique_minimum.gif\", fps = 1) # hide","category":"page"},{"location":"literated/loss_minimization/#Loss-function-with-two-minima","page":"Minimization Loss","title":"Loss function with two minima","text":"","category":"section"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"Now let's do an example in which the loss function has two minima. We minimize the loss function","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"G₂(u) = u - v_* u - w_* ","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"where again u is a 2-vector, and v_* and w_* are given 2-vectors. Here, we take v_* = (1 -1) and w_* = (-1 -1).","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"v★ = [1, -1]\nw★ = [-1, -1]\nG₂(u) = [sqrt(((u[1] - v★[1])^2 + (u[2] - v★[2])^2) * ((u[1] - w★[1])^2 + (u[2] - w★[2])^2))]\nnothing # hide","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"The procedure is same as the single-minimum example above.","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"We set the seed for pseudo-random number generator for reproducibility,","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"rng_seed = 10\nRandom.seed!(rng_seed)\nnothing # hide","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"A positive function can be minimized with a target of 0,","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"G_target = [0]","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"We choose the stabilization as in the single-minimum example","category":"page"},{"location":"literated/loss_minimization/#Prior-distributions-2","page":"Minimization Loss","title":"Prior distributions","text":"","category":"section"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"We define the prior. We can place prior information on e.g., u₁, demonstrating a belief that u₁ is more likely to be negative. This can be implemented by setting a bias in the mean of its prior distribution to e.g., -05:","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"prior_distributions = [Parameterized(Normal(-0.5, sqrt(2))), Parameterized(Normal(0, sqrt(2)))]\n\nconstraints = [[no_constraint()], [no_constraint()]]\n\nparameter_names = [\"u1\", \"u2\"]\n\nprior = ParameterDistribution(prior_distributions, constraints, parameter_names)","category":"page"},{"location":"literated/loss_minimization/#Calibration-2","page":"Minimization Loss","title":"Calibration","text":"","category":"section"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"We choose the number of ensemble members, the number of EKI iterations, construct our initial ensemble and the EKI with the Inversion() constructor (exactly as in the single-minimum example):","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"N_ensemble = 20\nN_iterations = 20\n\ninitial_ensemble = EKP.construct_initial_ensemble(prior, N_ensemble; rng_seed = rng_seed)\n\nensemble_kalman_process = EKP.EnsembleKalmanProcess(initial_ensemble, G_target, Γ_stabilization, Inversion())","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"We calibrate by (i) obtaining the parameters, (ii) calculating the loss function on the parameters (and concatenate), and last (iii) generate a new set of parameters using the model outputs:","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"for i in 1:N_iterations\n    params_i = get_u_final(ensemble_kalman_process)\n\n    g_ens = hcat([G₂(params_i[:, i]) for i in 1:N_ensemble]...)\n\n    EKP.update_ensemble!(ensemble_kalman_process, g_ens)\nend","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"and visualize the results:","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"u_init = get_u_prior(ensemble_kalman_process)\n\nanim_two_minima = @animate for i in 1:N_iterations\n    u_i = get_u(ensemble_kalman_process, i)\n\n    plot(\n        [v★[1]],\n        [v★[2]],\n        seriestype = :scatter,\n        markershape = :star5,\n        markersize = 11,\n        markercolor = :red,\n        label = \"optimum v⋆\",\n    )\n\n    plot!(\n        [w★[1]],\n        [w★[2]],\n        seriestype = :scatter,\n        markershape = :star5,\n        markersize = 11,\n        markercolor = :green,\n        label = \"optimum w⋆\",\n    )\n\n    plot!(\n        u_i[1, :],\n        u_i[2, :],\n        seriestype = :scatter,\n        xlims = extrema(u_init[1, :]),\n        ylims = extrema(u_init[2, :]),\n        xlabel = \"u₁\",\n        ylabel = \"u₂\",\n        markersize = 5,\n        markeralpha = 0.6,\n        markercolor = :blue,\n        label = \"particles\",\n        title = \"EKI iteration = \" * string(i),\n    )\nend\nnothing # hide","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"Our bias in the prior shifts the initial ensemble into the negative u_1 direction, and thus increases the likelihood (over different instances of the random number generator) of finding the minimizer u=w_*.","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"gif(anim_two_minima, \"two_minima.gif\", fps = 1) # hide","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"This page was generated using Literate.jl.","category":"page"},{"location":"parameter_distributions/#Prior-distributions","page":"Prior distributions","title":"Prior distributions","text":"","category":"section"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"We provide a flexible setup for storing prior distribution with the ParameterDistributions module found in src/ParameterDistributions.jl.","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"One can create a full parameter distribution using three inputs:","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"A Distribution, given as a ParameterDistributionType object,\nAn array of Constraints, given as a Array{ConstraintType} object,\nA Name, given as a String.","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"One can also provide arrays of the triple (1., 2., 3.) to create more complex distributions.","category":"page"},{"location":"parameter_distributions/#A-simple-example:","page":"Prior distributions","title":"A simple example:","text":"","category":"section"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"Task: We wish to create a prior for a one-dimensional parameter. Our problem dictates that this parameter is bounded between 0 and 1. Prior knowledge dictates it is around 0.7. The parameter is called point_seven.","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"Solution: We should use a Normal distribution with the predefined \"bounded\" constraint.","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"Let's initialize the constraint first,","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"constraint = [bounded(0, 1)]","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"This automatically sets up the following transformation to the  constrained space (and also its inverse)","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"transform_unconstrained_to_constrained(x) = exp(x) / (exp(x) + 1)","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"The prior should be around 0.7 (in the constrained space), and one can find that the push-forward of a particular normal distribution, namely, transform_unconstrained_to_constrained(Normal(mean = 1, sd = 0.5)) gives a prior pdf with 95% of its mass between [0.5, 0.88].","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"distribution = Parameterized(Normal(1, 0.5))","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"Finally we attach the name","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"name = \"point_seven\"","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"and the distribution is created by:","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"prior = ParameterDistribution(distribution, constraint, name)","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"using Distributions\nusing Plots\nPlots.default(lw=2)\n\nN = 50\nx_eval = collect(-3:6/400:3)\n\n#bounded in [0.0, 1.0]\ntransform_unconstrained_to_constrained(x) = exp(x) / (exp(x) + 1)\ndist= pdf.(Normal(1, 0.5), x_eval)\nconstrained_x_eval = transform_unconstrained_to_constrained.(x_eval)\n\np1 = plot(x_eval, dist,) \nvline!([1.0]) \ntitle!(\"Normal(1, 0.5)\")\n\np2 = plot(constrained_x_eval, dist) \nvline!([transform_unconstrained_to_constrained(1.0)]) \ntitle!(\"Transformed Normal(1, 0.5)\")","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"The pdf of the Normal distribution and its transform look like:","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"p = plot(p1, p2, legend=false, size = (900, 450)) #hide","category":"page"},{"location":"parameter_distributions/#.-The-ParameterDistributionType","page":"Prior distributions","title":"1. The ParameterDistributionType","text":"","category":"section"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"The ParameterDistributionType has two flavours for building a distribution:","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"The Parameterized type is initialized using a Julia Distributions.jl object. Samples are drawn randomly from the distribution object\nThe Samples type is initialized using a two dimensional array. Samples are drawn randomly (with replacement) from the columns of the provided array.","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"One can use combinations of these distributions to construct a full parameter distribution.","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"note: Note\nWe recommend these distributions be unbounded (see about constraints below), as our methods do not preserve constraints directly.","category":"page"},{"location":"parameter_distributions/#.-The-ConstraintType","page":"Prior distributions","title":"2. The ConstraintType","text":"","category":"section"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"Our implemented algorithms do not work in constrained parameter space directly. Therefore, constraints are tackled by the mappings transform_constrained_to_unconstrained and transform_unconstrained_to_constrained. The mappings are built from either predefined or user-defined constraint functions held in the ConstraintType. ","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"In this section we call parameters are one-dimensional. Every parameter must have an associated independent ConstraintType, therefore we for each ParameterDistributionType of dimension p the user must provide a p-dimensional Array{ConstraintType}.","category":"page"},{"location":"parameter_distributions/#Predefined-ConstraintTypes","page":"Prior distributions","title":"Predefined ConstraintTypes","text":"","category":"section"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"We provide some ConstraintTypes, which apply different transformations internally to enforce bounds on physical parameter spaces. The types have the following constructors:","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"no_constraint(), no transform is required for this parameter,\nbounded_below(lower_bound), the physical parameter has a (provided) lower bound,\nbounded_above(upper_bound), the physical parameter has a (provided) upper bound,\nbounded(lower_bound,upper_bound), the physical parameter has the (provided) bounds.","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"Users can also define their own transformations by directly creating a ConstraintType object with their own mappings.","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"note: Note\nIt is up to the user to ensure any custom mappings transform_constrained_to_unconstrained and transform_unconstrained_to_constrained are inverses of each other.","category":"page"},{"location":"parameter_distributions/#.-The-name","page":"Prior distributions","title":"3. The name","text":"","category":"section"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"This is simply an identifier for the parameters later on.","category":"page"},{"location":"parameter_distributions/#.-The-power-of-the-normal-distribution","page":"Prior distributions","title":"4. The power of the normal distribution","text":"","category":"section"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"The combination of different normal distributions with these predefined constraints, provides a surprising breadth of prior distributions.","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"note: Note\nWe highly recommend users start with Normal distribution and predefined ConstraintType: these offer the best computational benefits and clearest interpretation of the methods.","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"For each for the predefined ConstraintTypes, we present animations of the resulting prior distribution for","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"distribution = Parameterized(Normal(mean, sd))","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"where: ","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"we fix the mean value to 0, and range over the standard deviation,\nwe fix the standard deviation to 1 and range over the mean value.","category":"page"},{"location":"parameter_distributions/#Without-constraints:-constraint-[no_constraints()]","page":"Prior distributions","title":"Without constraints: constraint = [no_constraints()]","text":"","category":"section"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"using Distributions\nusing Plots\nPlots.default(lw=2)\n\nN = 50\nx_eval = collect(-5:10/200:5)\nmean_varying = collect(-3:6/(N+1):3)\nsd_varying = collect(0.1:2.9/(N+1):3)\n\n# no constraint\ntransform_unconstrained_to_constrained(x) = x\n\nmean0norm(n) = pdf.(Normal(0, sd_varying[n]), x_eval)\nsd1norm(n) = pdf.(Normal(mean_varying[n], 1), x_eval)\nconstrained_x_eval = transform_unconstrained_to_constrained.(x_eval)\n\np1 = plot(constrained_x_eval, mean0norm.(1))\nvline!([transform_unconstrained_to_constrained(0)])\n\np2 = plot(constrained_x_eval, sd1norm.(1))\nvline!([transform_unconstrained_to_constrained(mean_varying[1])])\n\np = plot(p1, p2, layout=(1, 2), size = (900, 450), legend = false)\n \nanim_unbounded = @animate for n = 1:length(mean_varying)\n   #set new y data \n   p[1][1][:y] = mean0norm(n) \n   p[1][:title] = \"Transformed Normal(0, \" * string(round(sd_varying[n], digits=3)) * \")\" \n   p[2][1][:y] = sd1norm(n) \n   p[2][2][:x] = [transform_unconstrained_to_constrained(mean_varying[n]),\n                  transform_unconstrained_to_constrained(mean_varying[n]),\n                  transform_unconstrained_to_constrained(mean_varying[n])]\n\n   p[2][:title] = \"Transformed Normal(\" * string(round(mean_varying[n], digits=3)) * \", 1)\"\nend ","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"gif(anim_unbounded, \"anim_unbounded.gif\", fps = 5) # hide","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"The following generates the transformed Normal(0.5, 1) distribution","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"using EnsembleKalmanProcesses.ParameterDistributions\nusing Distributions \n\ndistribution = Parameterized(Normal(0.5, 1)) \nconstraint = [no_constraint()]\nname = \"unbounded_parameter\"\nprior = ParameterDistribution(distribution, constraint, name)","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"where no_constraint() automatically defines the identity constraint map","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"transform_unconstrained_to_constrained(x) = x","category":"page"},{"location":"parameter_distributions/#Bounded-below-by-0:-constraint-[bounded_below(0)]","page":"Prior distributions","title":"Bounded below by 0: constraint = [bounded_below(0)]","text":"","category":"section"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"using Distributions\nusing Plots\nPlots.default(lw=2)\n\nN = 50\nx_eval = collect(-5:10/400:5)\nmean_varying = collect(-1:5/(N+1):4)\nsd_varying = collect(0.1:3.9/(N+1):4)\n\n#bounded below by 0\ntransform_unconstrained_to_constrained(x) = exp(x)\n\nmean0norm(n) = pdf.(Normal(0,sd_varying[n]), x_eval)\nsd1norm(n) = pdf.(Normal(mean_varying[n], 1), x_eval)\nconstrained_x_eval = transform_unconstrained_to_constrained.(x_eval)\n\np1 = plot(constrained_x_eval, mean0norm.(1))\nvline!([transform_unconstrained_to_constrained(0)])\n\np2 = plot(constrained_x_eval, sd1norm.(1))\nvline!([transform_unconstrained_to_constrained(mean_varying[1])])\n\np = plot(p1,p2, layout=(1,2), size = (900,450), legend=false)\n \nanim_bounded_below = @animate for n = 1:length(mean_varying) \n   #set new y data  \n   p[1][1][:y] = mean0norm(n) \n   p[1][:title] = \"Transformed Normal(0, \" * string(round(sd_varying[n], digits=3)) * \")\"\n   p[2][1][:y] = sd1norm(n) \n   p[2][2][:x] = [transform_unconstrained_to_constrained(mean_varying[n]),\n                  transform_unconstrained_to_constrained(mean_varying[n]),\n                  transform_unconstrained_to_constrained(mean_varying[n])]\n   p[2][:title] = \"Transformed Normal(\" * string(round(mean_varying[n], digits=3)) * \", 1)\"\nend ","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"gif(anim_bounded_below, \"anim_bounded_below.gif\", fps = 5) # hide","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"The following generates the transformed Normal(0.5, 1) distribution","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"using EnsembleKalmanProcesses.ParameterDistributions\nusing Distributions \n\ndistribution = Parameterized(Normal(0.5, 1)) \nconstraint = [bounded_below(0)]\nname = \"bounded_below_parameter\"\nprior = ParameterDistribution(distribution, constraint, name)","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"where bounded_below(0) automatically defines the constraint map","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"transform_unconstrained_to_constrained(x) = exp(x)","category":"page"},{"location":"parameter_distributions/#Bounded-above-by-10.0:-constraint-[bounded_above(10)]","page":"Prior distributions","title":"Bounded above by 10.0: constraint = [bounded_above(10)]","text":"","category":"section"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"using Distributions\nusing Plots\nPlots.default(lw=2)\n\nN = 50\nx_eval = collect(-5:4/400:5)\nmean_varying = collect(-1:5/(N+1):4)\nsd_varying = collect(0.1:3.9/(N+1):4)\n\n#bounded above by 10.0\ntransform_unconstrained_to_constrained(x) = 10 - exp(x)\n\nmean0norm(n) = pdf.(Normal(0, sd_varying[n]), x_eval)\nsd1norm(n) = pdf.(Normal(mean_varying[n], 1), x_eval)\nconstrained_x_eval = transform_unconstrained_to_constrained.(x_eval)\n\np1 = plot(constrained_x_eval, mean0norm.(1))\nvline!([transform_unconstrained_to_constrained(0)])\n\np2 = plot(constrained_x_eval, sd1norm.(1))\nvline!([transform_unconstrained_to_constrained(mean_varying[1])])\n\np = plot(p1, p2, layout=(1, 2), size = (900, 450), legend=false)\n \nanim_bounded_above = @animate for n = 1:length(mean_varying)[1]\n  #set new y data\n   p[1][1][:y] = mean0norm(n)\n   p[1][:title] = \"Transformed Normal(0, \" * string(round(sd_varying[n], digits=3)) * \")\"\n   p[2][1][:y] = sd1norm(n)\n   p[2][2][:x] = [transform_unconstrained_to_constrained(mean_varying[n]),\n                  transform_unconstrained_to_constrained(mean_varying[n]),\n                  transform_unconstrained_to_constrained(mean_varying[n])]\n   p[2][:title] = \"Transformed Normal(\" * string(round(mean_varying[n], digits=3)) * \", 1)\"\nend ","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"gif(anim_bounded_above, \"anim_bounded_above.gif\", fps = 5) # hide","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"The following generates the transformed Normal(0.5, 1) distribution","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"using EnsembleKalmanProcesses.ParameterDistributions\nusing Distributions\n\ndistribution = Parameterized(Normal(0.5, 1))\nconstraint = [bounded_above(10)]\nname = \"bounded_above_parameter\"\nprior = ParameterDistribution(distribution, constraint, name)","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"where bounded_above(10) automatically defines the constraint map","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"transform_unconstrained_to_constrained(x) = 10 - exp(x)","category":"page"},{"location":"parameter_distributions/#Bounded-in-between-5-and-10:-constraint-[bounded(5,-10)]","page":"Prior distributions","title":"Bounded in  between 5 and 10: constraint = [bounded(5, 10)]","text":"","category":"section"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"using Distributions\nusing Plots\nPlots.default(lw=2)\n\nN = 50\nx_eval = collect(-10:20/400:10)\nmean_varying = collect(-3:2/(N+1):3)\nsd_varying = collect(0.1:0.9/(N+1):10)\n\n#bounded in [5.0, 10.0]\ntransform_unconstrained_to_constrained(x) = (10 * exp(x) + 5) / (exp(x) + 1)\n\nmean0norm(n) = pdf.(Normal(0, sd_varying[n]), x_eval)\nsd1norm(n) = pdf.(Normal(mean_varying[n], 1), x_eval)\nconstrained_x_eval = transform_unconstrained_to_constrained.(x_eval)\n\np1 = plot(constrained_x_eval, mean0norm.(1))\nvline!([transform_unconstrained_to_constrained(0)])\n\np2 = plot(constrained_x_eval, sd1norm.(1))\nvline!([transform_unconstrained_to_constrained(mean_varying[1])])\n\np = plot(p1, p2, layout=(1, 2), size = (900, 450), legend=false)\n \nanim_bounded = @animate for n = 1:length(mean_varying)\n   #set new y data\n   p[1][1][:y] = mean0norm(n)\n   p[1][:title] = \"Transformed Normal(0, \" * string(round(sd_varying[n], digits=3)) * \")\"\n   p[2][1][:y] = sd1norm(n)\n   p[2][2][:x] = [transform_unconstrained_to_constrained(mean_varying[n]),\n                  transform_unconstrained_to_constrained(mean_varying[n]),\n                  transform_unconstrained_to_constrained(mean_varying[n])]\n   p[2][:title] = \"Transformed Normal(\" * string(round(mean_varying[n], digits=3)) * \", 1)\"\nend ","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"gif(anim_bounded, \"anim_bounded.gif\", fps = 10) # hide","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"The following generates the transformed Normal(0.5, 1) distribution","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"using EnsembleKalmanProcesses.ParameterDistributions\nusing Distributions\n\ndistribution = Parameterized(Normal(0.5, 1)) \nconstraint = [bounded(5, 10)]\nname = \"bounded_parameter\"\nprior = ParameterDistribution(distribution, constraint, name)","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"where bounded(5, 10) automatically defines the constraint map","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"transform_unconstrained_to_constrained(x) = (10 * exp(x) + 5) / (exp(x) + 1)","category":"page"},{"location":"parameter_distributions/#A-more-involved-example:","page":"Prior distributions","title":"A more involved example:","text":"","category":"section"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"We create a 6-dimensional parameter distribution from 2 triples.","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"The first triple is a 4-dimensional distribution with the following constraints on parameters in physical space:","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"c1 = [no_constraint(),     # no constraints\n      bounded_below(-1.0), # provide lower bound\n      bounded_above(0.4),  # provide upper bound\n      bounded(-0.1, 0.2)]  # provide lower and upper bound","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"We choose to use a multivariate normal to represent its distribution in the transformed (unbounded) space. Here we take a tridiagonal covariance matrix.","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"diag_val = 0.5 * ones(4)\nudiag_val = 0.25 * ones(3)\nmean = ones(4)\ncovariance = SymTridiagonal(diagonal_val, udiag_val)\nd1 = Parameterized(MvNormal(mean, covariance)) # 4D multivariate normal","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"We also provide a name","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"name1 = \"constrained_mvnormal\"","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"The second triple is a 2-dimensional one. It is only given by 4 samples in the transformed space - (where one will typically generate samples). It is bounded in the first dimension by the constraint shown, there is a user provided transform for the second dimension - using the default constructor.","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"d2 = Samples([1.0 3.0; 5.0 7.0; 9.0 11.0; 13.0 15.0]) # 4 samples of 2D parameter space\ntransform = (x -> 3 * x + 14)\ninverse_transform = (x -> (x - 14) / 3)\nc2 = [bounded(10, 15),\n      Constraint(transform, inverse_transform)]\nname2 = \"constrained_sampled\"","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"The full prior distribution for this setting is created with arrays of our two triples.","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"u = ParameterDistribution([d1, d2], [c1, c2], [name1, name2])","category":"page"},{"location":"parameter_distributions/#Other-functions","page":"Prior distributions","title":"Other functions","text":"","category":"section"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"These functions typically return a Dict with ParameterDistribution.name as a keys, or an Array if requested:","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"get_name: returns the names\nget_distribution: returns the Julia Distribution object if it is Parameterized\nmean, var, cov, sample: mean,variance,covariance or samples the Julia Distribution if Parameterized, or draws from the list of samples if Samples extends the StatsBase definitions\ntransform_unconstrained_to_constrained: apply the constraint mappings\ntransform_constrained_to_unconstrained: apply the inverse constraint mappings","category":"page"},{"location":"glossary/#Glossary","page":"Glossary","title":"Glossary","text":"","category":"section"},{"location":"glossary/","page":"Glossary","title":"Glossary","text":"The following list includes the names and symbols of recurring concepts in EnsembleKalmanProcesses.jl. Some of these variables do not appear in the codebase, which relies on array programming for performance.  Contributions to the codebase require following this notational convention. Similarly, if you find inconsistencies in the documentation or codebase, please report an issue on GitHub.","category":"page"},{"location":"glossary/","page":"Glossary","title":"Glossary","text":"Name Symbol (Theory/Docs) Symbol (Code)\nParameter vector, Parameters (unconstrained space) theta θ\nParameter vector size, Number of parameters p N_par\nEnsemble size J N_ens\nEnsemble particles, members theta^(j) \nNumber of iterations N_rm it N_iter\nObservation vector, Observations, Data vector y y\nObservation vector size, Data vector size d N_obs\nObservational noise eta obs_noise\nObservational noise covariance Gamma_y obs_noise_cov\nHilbert space inner product langle phi  Gamma^-1 psi rangle \nForward map mathcalG G\nDynamical model Psi Ψ\nTransform map (constrained to unconstrained) mathcalT T\nObservation map mathcalH H\nPrior covariance (unconstrained space) Gamma_theta prior_cov\nPrior mean (unconstrained space) m_theta prior_mean","category":"page"},{"location":"examples/lorenz_example/#Lorenz-96-example","page":"Lorenz","title":"Lorenz 96 example","text":"","category":"section"},{"location":"examples/lorenz_example/#Overview","page":"Lorenz","title":"Overview","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"The Lorenz 96 (hereafter L96) example is a toy-problem for the application of the EnsembleKalmanProcesses.jl optimization and approximate uncertainty quantification methodologies. Here is L96 with additional periodic-in-time forcing, we try to determine parameters (sinusoidal amplitude and stationary component of the forcing) from some output statistics. The standard L96 equations are implemented with an additional forcing term with time dependence. The output statistics which are used for learning are the finite time-averaged variances.","category":"page"},{"location":"examples/lorenz_example/#Lorenz-96-equations","page":"Lorenz","title":"Lorenz 96 equations","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"The standard single-scale L96 equations are implemented. The Lorenz 96 system (Lorenz, 1996) is given by ","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"fracd x_id t = (x_i+1 - x_i-2) x_i-1 - x_i + F","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"with i indicating the index of the given longitude. The number of longitudes is given by N. The boundary conditions are given by","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"x_-1 = x_N-1  x_0 = x_N  x_N+1 = x_1","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"The time scaling is such that the characteristic time is 5 days (Lorenz, 1996).  For very small values of F, the solutions x_i decay to F after the initial transient feature. For moderate values of F, the solutions are periodic, and for larger values of F, the system is chaotic. The solution variance is a function of the forcing magnitude. Variations in the base state as a function of time can be imposed through a time-dependent forcing term F(t).","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"A temporal forcing term is defined","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"F = F_s + A sin(omega t)","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"with steady-state forcing F_s, transient forcing amplitude A, and transient forcing frequency omega. The total forcing F must be within the chaotic regime of L96 for all time given the prescribed N.","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"The L96 dynamics are solved with RK4 integration.","category":"page"},{"location":"examples/lorenz_example/#Structure","page":"Lorenz","title":"Structure","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"The main code is located in Lorenz_example.jl which provides the functionality to run the L96 dynamical system, extract time-averaged statistics from the L96 states, and use the time-average statistics for optimization and uncertainty quantification.","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"The L96 system is solved in GModel.jl according to the time integration settings specified in LSettings and the L96 parameters specified in LParams. The types of statistics to be collected are detailed in GModel.jl.","category":"page"},{"location":"examples/lorenz_example/#Lorenz-dynamics-inputs","page":"Lorenz","title":"Lorenz dynamics inputs","text":"","category":"section"},{"location":"examples/lorenz_example/#Dynamics-settings","page":"Lorenz","title":"Dynamics settings","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"The use of the transient forcing term is with the flag, dynamics. Stationary forcing is dynamics=1 (A=0) and transient forcing is used with dynamics=2 (Aneq0). The default parameters are specified in Lorenz_example.jl and can be modified as necessary. The system is solved over time horizon 0 to tend at fixed time step dt.","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"N = 36\ndt = 1/64\nt_start = 800","category":"page"},{"location":"examples/lorenz_example/#Inverse-problem-settings","page":"Lorenz","title":"Inverse problem settings","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"The states are integrated over time Ts_days to construct the time averaged statistics for use by the optimization. The specification of the statistics to be gathered from the states are provided by stats_type. The Ensemble Kalman Process (EKP) settings are","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"N_ens = 20 # number of ensemble members\nN_iter = 5 # number of EKI iterations","category":"page"},{"location":"examples/lorenz_example/#Setting-up-the-Inverse-Problem","page":"Lorenz","title":"Setting up the Inverse Problem","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"The goal is to learn F_s and A based on the time averaged statistics in a perfect model setting. The true parameters are","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"F_true = 8. # Mean F\nA_true = 2.5 # Transient F amplitude\nω_true = 2. * π / (360. / τc) # Frequency of the transient F\nparams_true = [F_true, A_true]\nparam_names = [\"F\", \"A\"]","category":"page"},{"location":"examples/lorenz_example/#Priors","page":"Lorenz","title":"Priors","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"We use normal priors without constraints","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"prior_means = [F_true+1.0, A_true+0.5]\nprior_stds = [2.0, 0.5*A_true]\nd1 = Parameterized(Normal(prior_means[1], prior_stds[1]))\nd2 = Parameterized(Normal(prior_means[2], prior_stds[2]))\nprior_distns = [d1, d2]\nc1 = no_constraint()\nc2 = no_constraint()\nconstraints = [[c1], [c2]]\nprior_names = param_names\npriors = ParameterDistribution(prior_distns, constraints, prior_names)","category":"page"},{"location":"examples/lorenz_example/#Observational-Noise","page":"Lorenz","title":"Observational Noise","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"The observational noise can be generated using the L96 system or prescribed, as specified by var_prescribe. ","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"var_prescribe==false The observational noise is constructed by generating independent instantiations of the L96 statistics of interest at the true parameters for different initial conditions. The empirical covariance matrix is constructed.","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"var_prescribe==true The observational noise is prescribed as a Gaussian distribution with prescribed mean and variance.","category":"page"},{"location":"examples/lorenz_example/#Running-the-Example","page":"Lorenz","title":"Running the Example","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"The L96 parameter estimation can be run using julia --project Lorenz_example.jl","category":"page"},{"location":"examples/lorenz_example/#Solution-and-Output","page":"Lorenz","title":"Solution and Output","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"The output will provide the estimated parameters.","category":"page"},{"location":"examples/lorenz_example/#Printed-output","page":"Lorenz","title":"Printed output","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"# EKI results: Has the ensemble collapsed toward the truth?\nprintln(\"True parameters: \")\nprintln(params_true)\nprintln(\"\\nEKI results:\")\nprintln(mean(get_u_final(ekiobj), dims=2))","category":"page"},{"location":"examples/lorenz_example/#Saved-output","page":"Lorenz","title":"Saved output","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"The parameters and forward model outputs will be saved in parameter_storage.jld2 and data_storage.jld2, respectively. The data will be saved in the directory output.","category":"page"},{"location":"examples/lorenz_example/#Plots","page":"Lorenz","title":"Plots","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"A scatter plot of the parameter estimates compared to the true parameters will be provided in the directory output.","category":"page"},{"location":"examples/template_example/#Template-example","page":"Template","title":"Template example","text":"","category":"section"},{"location":"examples/template_example/","page":"Template","title":"Template","text":"We provide the following template for how the tools may be applied.","category":"page"},{"location":"examples/template_example/","page":"Template","title":"Template","text":"For small examples typically have 2 files.","category":"page"},{"location":"examples/template_example/","page":"Template","title":"Template","text":"DynamicalModel.jl Contains the dynamical model Psi and the observation map mathcalH. The inputs should be the so-called free parameters (in the constrained/physical space that is the input domain of the dynamical model) we are interested in learning, and the output should be the measured data.\nThe example script which contains the inverse problem setup and solve","category":"page"},{"location":"examples/template_example/#The-structure-of-the-example-script","page":"Template","title":"The structure of the example script","text":"","category":"section"},{"location":"examples/template_example/","page":"Template","title":"Template","text":"First we create the data and the setting for the model","category":"page"},{"location":"examples/template_example/","page":"Template","title":"Template","text":"Set up the forward model.\nConstruct/load the truth data. Store this data conveniently in the Observations.Observation object","category":"page"},{"location":"examples/template_example/","page":"Template","title":"Template","text":"Then we set up the inverse problem","category":"page"},{"location":"examples/template_example/","page":"Template","title":"Template","text":"Define the prior distributions. Use the ParameterDistribution object\nDecide on which process tool you would like to use (we recommend you begin with Inversion()). Then initialize this with the relevant constructor\ninitialize the EnsembleKalmanProcess object","category":"page"},{"location":"examples/template_example/","page":"Template","title":"Template","text":"Then we solve the inverse problem, in a loop perform the following for as many iterations as required:","category":"page"},{"location":"examples/template_example/","page":"Template","title":"Template","text":"Obtain the current parameter ensemble\nTransform them from the unbounded computational space to the physical space\ncall the forward map on the ensemble of parameters, producing an ensemble of measured data\ncall the update_ensemble! function to generate a new parameter ensemble based on the new data","category":"page"},{"location":"examples/template_example/","page":"Template","title":"Template","text":"One can then obtain the solution, dependent on the process type.","category":"page"},{"location":"ensemble_kalman_inversion/#Ensemble-Kalman-Inversion","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"","category":"section"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"One of the ensemble Kalman processes implemented in EnsembleKalmanProcesses.jl is the ensemble Kalman inversion (Iglesias et al, 2013). The ensemble Kalman inversion (EKI) is a derivative-free ensemble optimization method that seeks to find the optimal parameters theta in mathbbR^p in the inverse problem","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"y = mathcalG(theta) + eta ","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"where mathcalG denotes the forward map, y in mathbbR^d is the vector of observations and eta  in mathbbR^d is additive observational noise. Note that p is the size of the parameter vector theta and d the size of the observation vector y.","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"Here, we take eta sim mathcalN(0 Gamma_y) from a d-dimensional multivariate normal distribution with zero mean and covariance matrix Gamma_y.  This noise structure aims to represent the correlations between observations.","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"The parameter vector of the j-th ensemble member at the n-th iteration is theta^(j)_n. The EKI update equation for parameter vector theta^(j) is","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"theta_n+1^(j) = theta_n^(j) - dfracDelta t_nJsum_k=1^J left langle mathcalG(theta_n^(k)) - barmathcalG_n    Gamma_y^-1 left ( mathcalG(theta_n^(j)) - y right ) right rangle theta_n^(k) ","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"where the subscript n=1 dots N_rm it indicates the iteration, J is the number of members in the ensemble, barmathcalG_n is the mean value of mathcalG(theta_n) across ensemble members,","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"barmathcalG_n = dfrac1Jsum_k=1^JmathcalG(theta_n^(k)) ","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"and angle brackets denote the Euclidean inner product. By multiplying with Gamma_y^-1 we render the inner product non-dimensional.","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"The EKI algorithm is considered converged when the ensemble achieves sufficient consensus/collapse in parameter space. The final estimate bartheta_N_rm it is taken to be the ensemble mean at the final iteration,","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"bartheta_N_rm it = dfrac1Jsum_k=1^Jtheta_N_rm it^(k)","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"For typical applications, a near-optimal solution theta can be found after as few as 10 iterations of the algorithm. The obtained solution is optimal in the sense of the mean squared error loss, details can be found in Iglesias et al (2013). The algorithm performs better with larger ensembles. As a rule of thumb, the number of members in the ensemble should be larger than 10p, although the optimal ensemble size may depend on the problem setting and the computational power available.","category":"page"},{"location":"ensemble_kalman_inversion/#Constructing-the-Forward-Map","page":"Ensemble Kalman Inversion","title":"Constructing the Forward Map","text":"","category":"section"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"The forward map mathcalG maps the space of unconstrained parameters theta in mathbbR^p to the outputs y in mathbbR^d. In practice, the user may not have access to such a map directly. Consider a situation where the goal is to learn a set of parameters phi of a dynamical model Psi mathbbR^p rightarrow mathbbR^o, given observations y in mathbbR^d and a set of constraints on the value of phi. Then, the forward map may be constructed as","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"mathcalG = mathcalH circ Psi circ mathcalT^-1","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"where mathcalH mathbbR^o rightarrow mathbbR^d is the observation map and mathcalT is the transformation map from constrained to unconstrained parameter spaces, such that mathcalT(phi) = theta. A family of standard transformation maps and their inverse are available in the ParameterDistributions module.","category":"page"},{"location":"ensemble_kalman_inversion/#Creating-the-EKI-Object","page":"Ensemble Kalman Inversion","title":"Creating the EKI Object","text":"","category":"section"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"An ensemble Kalman inversion object can be created using the EnsembleKalmanProcess constructor by specifying the Inversion() process type.","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"Creating an ensemble Kalman inversion object requires as arguments:","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"An initial parameter ensemble, Array{Float, 2} of size [p × J];\nThe mean value of the observed outputs, a vector of size [d];\nThe covariance of the observational noise, a matrix of size [d × d]\nThe Inversion() process type.","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"A typical initialization of the Inversion() process takes a user-defined prior, a summary of the observation statistics given by the mean y and covariance obs_noise_cov, and a desired number of members in the ensemble,","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"using EnsembleKalmanProcesses\nusing EnsembleKalmanProcesses.ParameterDistributions\n\nJ = 50  # number of ensemble members\ninitial_ensemble = construct_initial_ensemble(prior, J) # Initialize ensemble from prior\n\nekiobj = EnsembleKalmanProcess(initial_ensemble, y, obs_noise_cov, Inversion())","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"See the Prior distributions section to learn about the construction of priors in EnsembleKalmanProcesses.jl. The prior is assumed to be over the unconstrained parameter space where theta is defined. For applications where enforcing parameter bounds is necessary, the ParameterDistributions module provides functions to map from constrained to unconstrained space and vice versa. ","category":"page"},{"location":"ensemble_kalman_inversion/#Updating-the-Ensemble","page":"Ensemble Kalman Inversion","title":"Updating the Ensemble","text":"","category":"section"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"Once the ensemble Kalman inversion object ekiobj has been initialized, any number of updates can be performed using the inversion algorithm.","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"A call to the inversion algorithm can be performed with the update_ensemble! function. This function takes as arguments the ekiobj and the evaluations of the forward map at each member of the current ensemble. The update_ensemble! function then stores the new updated ensemble and the inputted forward map evaluations in ekiobj. ","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"A typical use of the update_ensemble! function given the ensemble Kalman inversion object ekiobj, the dynamical model Ψ and the observation map H is","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"N_iter = 20 # Number of steps of the algorithm\n\nfor n in 1:N_iter\n    θ_n = get_u_final(ekiobj) # Get current ensemble\n    ϕ_n = transform_unconstrained_to_constrained(prior, θ_n) # Transform parameters to physical/constrained space\n    G_n = [H(Ψ((ϕ_n[:, i])) for i in 1:J]\n    g_ens = hcat(G_n...) # Evaluate forward map\n    \n    update_ensemble!(ekiobj, g_ens) # Update ensemble\nend","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"In the previous update, note that the parameters stored in ekiobj are given in the unconstrained Gaussian space where the EKI algorithm is performed. The map mathcalT^-1 between this unconstrained space and the (possibly constrained) physical space of parameters is encoded in the prior object. The dynamical model Ψ accepts as inputs the parameters in (possibly constrained) physical space, so it is necessary to apply transform_unconstrained_to_constrained before evaluations. See the Prior distributions section for more details on parameter transformations.","category":"page"},{"location":"ensemble_kalman_inversion/#Solution","page":"Ensemble Kalman Inversion","title":"Solution","text":"","category":"section"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"The EKI algorithm drives the initial ensemble, sampled from the prior, towards the support region of the posterior distribution. The algorithm also drives the ensemble members towards consensus. The optimal parameter θ_optim found by the algorithm is given by the mean of the last ensemble (i.e., the ensemble after the last iteration),","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"using Statistics\n\nθ_optim = mean(get_u_final(ekiobj), dims=2)","category":"page"},{"location":"API/Localizers/#Localizers","page":"Localizers","title":"Localizers","text":"","category":"section"},{"location":"API/Localizers/","page":"Localizers","title":"Localizers","text":"CurrentModule = EnsembleKalmanProcesses.Localizers","category":"page"},{"location":"API/Localizers/","page":"Localizers","title":"Localizers","text":"Localizer\nRBF\nBernoulliDropout","category":"page"},{"location":"API/Localizers/#EnsembleKalmanProcesses.Localizers.Localizer","page":"Localizers","title":"EnsembleKalmanProcesses.Localizers.Localizer","text":"Uniform kernel constructor\n\n\n\n\n\nDelta kernel localizer constructor\n\n\n\n\n\nRBF kernel localizer constructor\n\n\n\n\n\nRandomized Bernoulli dropout kernel localizer constructor\n\n\n\n\n\n","category":"type"},{"location":"API/Localizers/#EnsembleKalmanProcesses.Localizers.RBF","page":"Localizers","title":"EnsembleKalmanProcesses.Localizers.RBF","text":"RBF{FT <: Real} <: LocalizationMethod\n\nRadial basis function localization method. Covariance terms are damped as d(i,j)= |i-j|/l increases, following a Gaussian.\n\nFields\n\nlengthscale::Real\nLength scale defining the RBF kernel\n\n\n\n\n\n","category":"type"},{"location":"API/Localizers/#EnsembleKalmanProcesses.Localizers.BernoulliDropout","page":"Localizers","title":"EnsembleKalmanProcesses.Localizers.BernoulliDropout","text":"BernoulliDropout{FT <: Real} <: LocalizationMethod\n\nLocalization method that drops cross-covariance terms with probability 1-p, retaining a Hermitian structure.\n\nFields\n\nprob::Real\nProbability of keeping a given cross-covariance term\n\n\n\n\n\n","category":"type"},{"location":"API/ParameterDistributions/#ParameterDistributions","page":"ParameterDistributions","title":"ParameterDistributions","text":"","category":"section"},{"location":"API/ParameterDistributions/","page":"ParameterDistributions","title":"ParameterDistributions","text":"CurrentModule = EnsembleKalmanProcesses.ParameterDistributions","category":"page"},{"location":"API/ParameterDistributions/","page":"ParameterDistributions","title":"ParameterDistributions","text":"Parameterized\nSamples\nConstraint\nParameterDistribution\nno_constraint\nbounded_below\nbounded_above\nbounded\nlength(c::CType) where {CType <: ConstraintType}\nsize(c::CType) where {CType <: ConstraintType}\nn_samples\nget_name\nget_dimensions\nget_n_samples\nget_all_constraints\nbatch\nget_distribution\nsample\nget_logpdf\nmean\nvar\ncov\ntransform_constrained_to_unconstrained\ntransform_unconstrained_to_constrained","category":"page"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.Parameterized","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.Parameterized","text":"Parameterized <: ParameterDistributionType\n\nA distribution constructed from a parametrized formula (e.g Julia Distributions.jl)\n\n\n\n\n\n","category":"type"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.Samples","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.Samples","text":"Samples{FT <: Real} <: ParameterDistributionType\n\nA distribution comprised of only samples, stored as columns of parameters.\n\n\n\n\n\n","category":"type"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.Constraint","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.Constraint","text":"Constraint <: ConstraintType\n\nContains two functions to map between constrained and unconstrained spaces.\n\n\n\n\n\n","category":"type"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution","text":"ParameterDistribution\n\nStructure to hold a parameter distribution, always stored as an array of distributions.\n\n\n\n\n\n","category":"type"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.no_constraint","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.no_constraint","text":"no_constraint()\n\nConstructs a Constraint with no constraints, enforced by maps x -> x and x -> x.\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.bounded_below","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.bounded_below","text":"bounded_below(lower_bound::FT) where {FT <: Real}\n\nConstructs a Constraint with provided lower bound, enforced by maps x -> log(x - lower_bound) and x -> exp(x) + lower_bound.\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.bounded_above","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.bounded_above","text":"bounded_above(upper_bound::FT) where {FT <: Real}\n\nConstructs a Constraint with provided upper bound, enforced by maps x -> log(upper_bound - x) and x -> upper_bound - exp(x).\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.bounded","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.bounded","text":"bounded(lower_bound::FT, upper_bound::FT) where {FT <: Real}\n\nConstructs a Constraint with provided upper and lower bounds, enforced by maps x -> log((x - lower_bound) / (upper_bound - x)) and x -> (upper_bound * exp(x) + lower_bound) / (exp(x) + 1).\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#Base.length-Tuple{CType} where CType<:EnsembleKalmanProcesses.ParameterDistributions.ConstraintType","page":"ParameterDistributions","title":"Base.length","text":"length(c<:ConstraintType)\n\nA constraint has length 1. \n\n\n\n\n\n","category":"method"},{"location":"API/ParameterDistributions/#Base.size-Tuple{CType} where CType<:EnsembleKalmanProcesses.ParameterDistributions.ConstraintType","page":"ParameterDistributions","title":"Base.size","text":"size(c<:ConstraintType)\n\nA constraint has size 1.\n\n\n\n\n\n","category":"method"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.n_samples","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.n_samples","text":"n_samples(d<:Samples)\n\nThe number of samples in the array.\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.get_name","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.get_name","text":"get_name(pd::ParameterDistribution)\n\nReturns a list of ParameterDistribution names.\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.get_dimensions","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.get_dimensions","text":"get_dimensions(pd::ParameterDistribution)\n\nThe number of dimensions of the parameter space.\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.get_n_samples","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.get_n_samples","text":"get_n_samples(pd::ParameterDistribution)\n\nThe number of samples in a Samples distribution\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.get_all_constraints","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.get_all_constraints","text":"get_all_constraints(pd::ParameterDistribution)\n\nReturns the (flattened) array of constraints of the parameter distribution.\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.batch","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.batch","text":"batch(pd::ParameterDistribution)\n\nReturns a list of contiguous [collect(1:i), collect(i+1:j),... ] used to split parameter arrays by distribution dimensions.\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.get_distribution","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.get_distribution","text":"get_distribution(pd::ParameterDistribution)\n\nReturns a Dict of ParameterDistribution distributions, with the parameter names as dictionary keys. For parameters represented by Samples, the samples are returned as a 2D (parameter_dimension x n_samples) array.\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#StatsBase.sample","page":"ParameterDistributions","title":"StatsBase.sample","text":"sample([rng], pd::ParameterDistribution, [n_draws])\n\nDraws n_draws samples from the parameter distributions pd. Returns an array, with  parameters as columns. rng is optional and defaults to Random.GLOBAL_RNG. n_draws is  optional and defaults to 1. \n\n\n\n\n\nsample([rng], d::Samples, [n_draws])\n\nDraws n_draws samples from the parameter distributions d. Returns an array, with  parameters as columns. rng is optional and defaults to Random.GLOBAL_RNG. n_draws is  optional and defaults to 1. \n\n\n\n\n\nsample([rng], d::Parameterized, [n_draws])\n\nDraws n_draws samples from the parameter distributions d. Returns an array, with  parameters as columns. rng is optional and defaults to Random.GLOBAL_RNG. n_draws is  optional and defaults to 1. \n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.get_logpdf","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.get_logpdf","text":"logpdf(pd::ParameterDistribution, xarray::Array{<:Real,1})\n\nObtains the independent logpdfs of the parameter distributions at xarray (non-Samples Distributions only), and returns their sum.\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#Statistics.mean","page":"ParameterDistributions","title":"Statistics.mean","text":"mean(pd::ParameterDistribution)\n\nReturns a concatenated mean of the parameter distributions. \n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#Statistics.var","page":"ParameterDistributions","title":"Statistics.var","text":"var(pd::ParameterDistribution)\n\nReturns a flattened variance of the distributions\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#Statistics.cov","page":"ParameterDistributions","title":"Statistics.cov","text":"cov(pd::ParameterDistribution)\n\nReturns a dense blocked (co)variance of the distributions.\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.transform_constrained_to_unconstrained","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.transform_constrained_to_unconstrained","text":"transform_constrained_to_unconstrained(pd::ParameterDistribution, xarray::Array{<:Real,1})\n\nApply the transformation to map (possibly constrained) parameters xarray into the unconstrained space.\n\n\n\n\n\ntransform_constrained_to_unconstrained(pd::ParameterDistribution, xarray::Array{<:Real,2})\n\nApply the transformation to map (possibly constrained) parameter samples xarray into the unconstrained space. Here, xarray contains parameters as columns and samples as rows.\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.transform_unconstrained_to_constrained","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.transform_unconstrained_to_constrained","text":"transform_unconstrained_to_constrained(pd::ParameterDistribution, xarray::Array{<:Real,1})\n\nApply the transformation to map parameters xarray from the unconstrained space into (possibly constrained) space.\n\n\n\n\n\ntransform_unconstrained_to_constrained(pd::ParameterDistribution, xarray::Array{<:Real,2})\n\nApply the transformation to map parameter samples xarray from the unconstrained space into (possibly constrained) space. Here, xarray contains parameters as columns and samples as rows.\n\n\n\n\n\ntransform_unconstrained_to_constrained(pd::ParameterDistribution, xarray::Array{Array{<:Real,2},1})\n\nApply the transformation to map parameter sample ensembles xarray from the unconstrained space into (possibly constrained) space. Here, xarray is an iterable of parameters sample ensembles for different EKP iterations.\n\n\n\n\n\n","category":"function"},{"location":"examples/ClimateMachine_example/#HPC-interfacing-example:-ClimateMachine","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"","category":"section"},{"location":"examples/ClimateMachine_example/#Overview","page":"HPC interfacing example: ClimateMachine","title":"Overview","text":"","category":"section"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"This examples uses EnsembleKalmanProcesses.jl to calibrate a climate model, showcasing a workflow which is compatible with HPC resources managed with the SLURM workload manager. The workflow is based on read-write input/output files, and as such it is capable of interfacing with dynamical models in different code languages, or with complicated processing stages. The dynamical model for this example is ClimateMachine.jl, an Earth system model currently under development at CliMA.","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"The calibration example makes use of a simple single atmospheric column model configuration with two learnable parameters that control turbulent mixing processes in the lower troposphere. It is also a perfect model experiment, in the sense that the ground truth is generated using the same model and a prescribed combination of parameters. The parameters used to generate the ground truth are (C_smag, C_drag) = (0.21, 0.0011). The evolution of the atmosphere for this setup is strongly influenced by C_smag, and very weakly by C_drag. Thus, we expect the EKP to recover C_smag from observations.","category":"page"},{"location":"examples/ClimateMachine_example/#Prerequisites","page":"HPC interfacing example: ClimateMachine","title":"Prerequisites","text":"","category":"section"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"This example requires ClimateMachine.jl to be installed in the same parent directory as EnsembleKalmanProcesses.jl. You may install ClimateMachine.jl directly from GitHub,","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"$ git clone https://github.com/CliMA/ClimateMachine.jl.git","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"Change into the ClimateMachine.jl directory with ","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"$ cd ClimateMachine.jl","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"and install all the required packages with:","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"$ julia --project -e 'using Pkg; pkg\"instantiate\";'","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"Pre-compile the packages to allow the ClimateMachine.jl to start faster:","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"$ julia --project -e 'using Pkg; pkg\"precompile\"'","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"You can find more information about ClimateMachine.jl here. ClimateMachine.jl is a rapidly evolving software and this example may stop working in the future, please open an issue if you find that to be the case!","category":"page"},{"location":"examples/ClimateMachine_example/#Structure","page":"HPC interfacing example: ClimateMachine","title":"Structure","text":"","category":"section"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"The example makes use of julia and bash scripts for interactions with the workload manager and running multiple forward model evaluations in parallel. The user-triggered script ekp_calibration.sbatch initializes and controls the flow of the calibration process, which in this case is a SLURM queue with job dependencies. The calibration bash scripts, in order of execution and with their associated julia scripts, are","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"ekp_init_calibration: Calls init_calibration.jl, which samples the initial parameter ensemble from a specified prior. The initial ensemble is stored in a set of parameter files.\nekp_single_cm_run: Script called in parallel by the workload manager. Each copy of the script submits a single forward model run (i.e., a ClimateMachine.jl run) given a specific pair of parameters (C_smag, C_drag) read from a corresponding file. The output of each forward model run is stored in a separate NetCDF file.\nekp_cont_calibration: Calls sstep_calibration.jl, which reads from a NetCDF file the output generated by ClimateMachine.jl in step 2 and performs an iteration of the Ensemble Kalman Inversion algorithm, updating the parameter ensemble. The new parameters are stored in new parameter files.","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"This flow follows steps 1->2->3->2->3->... for a user-specified number of iterations.","category":"page"},{"location":"examples/ClimateMachine_example/#Running-the-Example","page":"HPC interfacing example: ClimateMachine","title":"Running the Example","text":"","category":"section"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"From the parent directory of ClimateMachine.jl and EnsembleKalmanProcesses.jl, change into the example directory with","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"$ cd EnsembleKalmanProcesses.jl/examples/ClimateMachine","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"and install all the required packages for the example with:","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"$ julia --project -e 'using Pkg; pkg\"instantiate\";'","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"To run the example using a SLURM workload manager, simply do:","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"$ sbatch ekp_calibration.sbatch","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"The dynamical model outputs (i.e, Psi(phi)) for all runs of ClimateMachine.jl will be stored in NetCDF format in directories identifiable by their version number. Refer to the files version_XX.txt to identify each run with each ensemble member within the XX iteration of the Ensemble Kalman Process. ","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"In this example, the parameters are defined through a Gaussian prior in init_calibration.jl. Hence, the transform from constrained to unconstrained space is the identity map, and we have phi=theta. Overall the forward map mathcalG(theta) is given by applying an observation map mathcalH to the dynamical model output Psi. In this case, mathcalH returns the time average of the horizontal velocity over a specified 30 min interval after initialization. Therefore, the observation vector y contains a time-averaged vertical profile of the horizontal velocity.","category":"page"},{"location":"examples/ClimateMachine_example/#Calibration-Solution","page":"HPC interfacing example: ClimateMachine","title":"Calibration Solution","text":"","category":"section"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"To aggregate the parameter ensembles theta^(1) theta^(2) dots theta^(J) generated during the calibration process, you may use the agg_clima_ekp(...) function located in helper_funcs.jl,","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"$ julia --project","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"include(joinpath(@__DIR__, \"helper_funcs.jl\"))\n\nagg_clima_ekp(2) # This generates the output containing the ensembles for each iteration, input is the number of parameters","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"This will create the JLD file ekp_clima.jld. We may read the file as follows","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"using JLD\n\nθ = load(\"ekp_clima.jld\")[\"ekp_u\"]\nprintln(typeof(θ)) # Array{Array{Float64,2},1}, outer dimension is N_iter, inner Array{Float64,2} of size = (J, p)","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"The optimal parameter vector determined by the ensemble Kalman inversion is the ensemble mean of the particles after the last iteration. Following the previous script,","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"using Statistics\n\nθ_opt = mean(θ[end], dims=1)","category":"page"},{"location":"API/EnsembleKalmanProcesses/#EnsembleKalmanProcesses","page":"EnsembleKalmanProcesses","title":"EnsembleKalmanProcesses","text":"","category":"section"},{"location":"API/EnsembleKalmanProcesses/","page":"EnsembleKalmanProcesses","title":"EnsembleKalmanProcesses","text":"CurrentModule = EnsembleKalmanProcesses","category":"page"},{"location":"API/EnsembleKalmanProcesses/","page":"EnsembleKalmanProcesses","title":"EnsembleKalmanProcesses","text":"Inversion\nSampler\nEnsembleKalmanProcess\nget_u\nget_g\nget_u_final\nget_N_iterations\nconstruct_initial_ensemble\nfind_ekp_stepsize\nupdate_ensemble!","category":"page"},{"location":"API/EnsembleKalmanProcesses/#EnsembleKalmanProcesses.Inversion","page":"EnsembleKalmanProcesses","title":"EnsembleKalmanProcesses.Inversion","text":"Inversion <: Process\n\nAn ensemble Kalman Inversion process\n\n\n\n\n\n","category":"type"},{"location":"API/EnsembleKalmanProcesses/#EnsembleKalmanProcesses.Sampler","page":"EnsembleKalmanProcesses","title":"EnsembleKalmanProcesses.Sampler","text":"Sampler{FT<:AbstractFloat,IT<:Int} <: Process\n\nAn ensemble Kalman Sampler process.\n\n\n\n\n\n","category":"type"},{"location":"API/EnsembleKalmanProcesses/#EnsembleKalmanProcesses.EnsembleKalmanProcess","page":"EnsembleKalmanProcesses","title":"EnsembleKalmanProcesses.EnsembleKalmanProcess","text":"EnsembleKalmanProcess{FT <: AbstractFloat, IT <: Int, P <: Process}\n\nStructure that is used in Ensemble Kalman processes.\n\nFields\n\nu::Array{EnsembleKalmanProcesses.DataContainers.DataContainer{FT}} where FT<:AbstractFloat\narray of stores for parameters (u), each of size [N_par × N_ens]\nobs_mean::Vector{FT} where FT<:AbstractFloat\nvector of the observed vector size [N_obs]\nobs_noise_cov::Union{LinearAlgebra.UniformScaling{FT}, AbstractMatrix{FT}} where FT<:AbstractFloat\ncovariance matrix of the observational noise, of size [N_obs × N_obs]\nN_ens::Int64\nensemble size\ng::Array{EnsembleKalmanProcesses.DataContainers.DataContainer{FT}} where FT<:AbstractFloat\nArray of stores for forward model outputs, each of size  [N_obs × N_ens]\nerr::Vector{FT} where FT<:AbstractFloat\nvector of errors\nΔt::Vector{FT} where FT<:AbstractFloat\nvector of timesteps used in each EK iteration\nprocess::EnsembleKalmanProcesses.Process\nthe particular EK process (Inversion or Sampler or Unscented or SparseInversion)\nrng::Random.AbstractRNG\nRandom number generator object (algorithm + seed) used for sampling and noise, for reproducibility. Defaults to Random.GLOBAL_RNG.\nfailure_handler::FailureHandler\nstruct storing failsafe update directives, implemented for (Inversion, SparseInversion, Unscented)\nlocalizer::EnsembleKalmanProcesses.Localizers.Localizer\nLocalization kernel, implemented for (Inversion, SparseInversion, Unscented)\n\n\n\n\n\n","category":"type"},{"location":"API/EnsembleKalmanProcesses/#EnsembleKalmanProcesses.get_u","page":"EnsembleKalmanProcesses","title":"EnsembleKalmanProcesses.get_u","text":"get_u(ekp::EnsembleKalmanProcess, iteration::IT; return_array=true) where {IT <: Integer}\n\nGet for the EKI iteration. Returns a DataContainer object unless array is specified.\n\n\n\n\n\nget_u(ekp::EnsembleKalmanProcess; return_array=true)\n\nGet for the EKI iteration. Returns a DataContainer object unless array is specified.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcesses/#EnsembleKalmanProcesses.get_g","page":"EnsembleKalmanProcesses","title":"EnsembleKalmanProcesses.get_g","text":"get_g(ekp::EnsembleKalmanProcess, iteration::IT; return_array=true) where {IT <: Integer}\n\nGet for the EKI iteration. Returns a DataContainer object unless array is specified.\n\n\n\n\n\nget_g(ekp::EnsembleKalmanProcess; return_array=true)\n\nGet for the EKI iteration. Returns a DataContainer object unless array is specified.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcesses/#EnsembleKalmanProcesses.get_u_final","page":"EnsembleKalmanProcesses","title":"EnsembleKalmanProcesses.get_u_final","text":"get_u_final(ekp::EnsembleKalmanProcess, return_array=true)\n\nGet the final or prior iteration of parameters or model ouputs, returns a DataContainer Object if return_array is false.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcesses/#EnsembleKalmanProcesses.get_N_iterations","page":"EnsembleKalmanProcesses","title":"EnsembleKalmanProcesses.get_N_iterations","text":"get_N_iterations(ekp::EnsembleKalmanProcess\n\nGet number of times update has been called (equals size(g), or size(u)-1).\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcesses/#EnsembleKalmanProcesses.construct_initial_ensemble","page":"EnsembleKalmanProcesses","title":"EnsembleKalmanProcesses.construct_initial_ensemble","text":"construct_initial_ensemble(rng::AbstractRNG, prior::ParameterDistribution, N_ens::IT; rng_seed::Union{IT, Nothing} = nothing)\n\nConstruct the initial parameters, by sampling N_ens samples from specified prior distribution. Returned with parameters as columns.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcesses/#EnsembleKalmanProcesses.find_ekp_stepsize","page":"EnsembleKalmanProcesses","title":"EnsembleKalmanProcesses.find_ekp_stepsize","text":"find_ekp_stepsize(ekp::EnsembleKalmanProcess{FT, IT, Inversion}, g::AbstractMatrix{FT}; cov_threshold::FT=0.01) where {FT}\n\nFind largest stepsize for the EK solver that leads to a reduction of the determinant of the sample covariance matrix no greater than cov_threshold. \n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcesses/#EnsembleKalmanProcesses.update_ensemble!","page":"EnsembleKalmanProcesses","title":"EnsembleKalmanProcesses.update_ensemble!","text":"update_ensemble!(\n    ekp::EnsembleKalmanProcess{FT, IT, Inversion},\n    g::AbstractMatrix{FT};\n    cov_threshold::FT = 0.01,\n    Δt_new::Union{Nothing, FT} = nothing,\n    deterministic_forward_map::Bool = true,\n    failed_ens = nothing,\n) where {FT, IT}\n\nUpdates the ensemble according to an Inversion process. \n\nInputs:\n\nekp :: The EnsembleKalmanProcess to update.\ng :: Model outputs, they need to be stored as a N_obs × N_ens array (i.e data are columms).\ncov_threshold :: Threshold below which the reduction in covariance determinant results in a warning.\nΔt_new :: Time step to be used in the current update.\ndeterministicforwardmap :: Whether output g comes from a deterministic model.\nfailed_ens :: Indices of failed particles. If nothing, failures are computed as columns of g  with NaN entries.\n\n\n\n\n\nupdate_ensemble!(\n    ekp::EnsembleKalmanProcess{FT, IT, SparseInversion{FT}},\n    g::AbstractMatrix{FT};\n    cov_threshold::FT = 0.01,\n    Δt_new = nothing,\n    deterministic_forward_map = true,\n    failed_ens = nothing,\n) where {FT, IT}\n\nUpdates the ensemble according to a SparseInversion process. \n\nInputs:\n\nekp :: The EnsembleKalmanProcess to update.\ng :: Model outputs, they need to be stored as a N_obs × N_ens array (i.e data are columms).\ncov_threshold :: Threshold below which the reduction in covariance determinant results in a warning.\nΔt_new :: Time step to be used in the current update.\ndeterministicforwardmap :: Whether output g comes from a deterministic model.\nfailed_ens :: Indices of failed particles. If nothing, failures are computed as columns of g  with NaN entries.\n\n\n\n\n\nupdate_ensemble!(\n    ekp::EnsembleKalmanProcess{FT, IT, Sampler{FT}},\n    g::AbstractMatrix{FT};\n    failed_ens = nothing,\n) where {FT, IT}\n\nUpdates the ensemble according to a Sampler process. \n\nInputs:\n\nekp :: The EnsembleKalmanProcess to update.\ng :: Model outputs, they need to be stored as a N_obs × N_ens array (i.e data are columms).\nfailed_ens :: Indices of failed particles. If nothing, failures are computed as columns of g  with NaN entries.\n\n\n\n\n\nupdate_ensemble!(\n    uki::EnsembleKalmanProcess{FT, IT, Unscented},\n    g_in::AbstractMatrix{FT};\n    Δt_new = nothing,\n    failed_ens = nothing,\n) where {FT <: AbstractFloat, IT <: Int}\n\nUpdates the ensemble according to an Unscented process. \n\nInputs:\n\nuki :: The EnsembleKalmanProcess to update.\ngin :: Model outputs, they need to be stored as a `Nobs × N_ens` array (i.e data are columms).\nΔt_new :: Time step to be used in the current update.\nfailed_ens :: Indices of failed particles. If nothing, failures are computed as columns of g  with NaN entries.\n\n\n\n\n\n","category":"function"},{"location":"unscented_kalman_inversion/#Unscented-Kalman-Inversion","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"","category":"section"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"One of the ensemble Kalman processes implemented in EnsembleKalmanProcesses.jl is the unscented Kalman inversion (Huang et al, 2021). The unscented Kalman inversion (UKI) is a derivative-free ensemble optimization method that seeks to find the optimal parameters theta in mathbbR^p in the inverse problem","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":" y = mathcalG(theta) + eta","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"where mathcalG denotes the forward map, y in mathbbR^d is the vector of observations and eta sim mathcalN(0 Gamma_y) is additive Gaussian observational noise. Note that p is the size of the parameter vector theta and d is taken to be the size of the observation vector y. The UKI algorithm has the following properties","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"UKI has a given ensemble size and requires only 2p + 1 particles in general.\nUKI has uncertainty quantification capability, it gives both mean and covariance approximation (no ensemble collapse and no empirical variance inflation) of the posterior distribution, the 3-sigma confidence interval covers the truth parameters for perfect models.","category":"page"},{"location":"unscented_kalman_inversion/#Algorithm","page":"Unscented Kalman Inversion","title":"Algorithm","text":"","category":"section"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"The UKI applies unscented Kalman filter for the following stochastic dynamical system ","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"beginaligned\n  textrmevolution    theta_n+1 = r + alpha (theta_n  - r) +  omega_n+1 omega_n+1 sim mathcalN(0Sigma_omega)\n  textrmobservation  y_n+1 = mathcalG(theta_n+1) + nu_n+1 nu_n+1 sim mathcalN(0Sigma_nu)\nendaligned","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"The free parameters in the UKI are alpha r Sigma_nu Sigma_omega. The UKI updates both the mean m_n and covariance C_n estimations of the parameter vector theta as following","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"Prediction step :","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"beginaligned\n    hatm_n+1 =  r+alpha(m_n-r)\n    hatC_n+1 =  alpha^2 C_n + Sigma_omega\nendaligned","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"Generate sigma points :","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"beginaligned\n    hattheta_n+1^0 = hatm_n+1 \n    hattheta_n+1^j = hatm_n+1 + c_j sqrthatC_n+1_j quad (1leq jleq N_theta) \n    hattheta_n+1^j+N_theta = hatm_n+1 - c_j sqrthatC_n+1_jquad (1leq jleq N_theta)\nendaligned","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"Analysis step :","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"   beginaligned\n        haty^j_n+1 = mathcalG(hattheta^j_n+1) qquad haty_n+1 = haty^0_n+1\n         hatC^theta p_n+1 = sum_j=1^2N_thetaW_j^c\n        (hattheta^j_n+1 - hatm_n+1 )(haty^j_n+1 - haty_n+1)^T \n        hatC^pp_n+1 = sum_j=1^2N_thetaW_j^c\n        (haty^j_n+1 - haty_n+1 )(haty^j_n+1 - haty_n+1)^T + Sigma_nu\n        m_n+1 = hatm_n+1 + hatC^theta p_n+1(hatC^pp_n+1)^-1(y - haty_n+1)\n        C_n+1 = hatC_n+1 - hatC^theta p_n+1(hatC^pp_n+1)^-1hatC^theta p_n+1^T\n    endaligned","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"The unscented transformation parameters are","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"    beginaligned\n    c_j = sqrtN_theta +lambda qquad W_j^c = frac12(N_theta+lambda)(j=1cdots2N_theta)\n    lambda = a^2 (N_theta + kappa) - N_theta quad a=minsqrtfrac4N_theta + kappa  1quad  kappa = 0\n    endaligned","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"And sqrtC_j is the j-th column of the Cholesky factor of C. ","category":"page"},{"location":"unscented_kalman_inversion/#Choosing-of-free-parameters","page":"Unscented Kalman Inversion","title":"Choosing of free parameters","text":"","category":"section"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"The free parameters in the unscented Kalman inversion are alpha r Sigma_nu Sigma_omega, which are chosen based on theorems developed in Huang et al, 2021","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"the vector r is set to be the prior mean\nthe scalar alpha in (01 is a regularization parameter, which is used to overcome ill-posedness and overfitting. A practical guide is \nWhen the observation noise is negligible, and there are more observations than parameters (identifiable inverse problem) alpha = 1 (no regularization)\nOtherwise alpha  1. The smaller alpha is, the closer UKI will converge to the prior mean.\nthe matrix Sigma_nu is the artificial observation error covariance. We set Sigma_nu = 2 Gamma_y, which makes the inverse problem consistent. \nthe matrix Sigma_omega is the artificial evolution error covariance. We set Sigma_omega = (2 - alpha^2)Lambda. We choose Lambda as following\nwhen there are more observations than parameters (identifiable inverse problem), Lambda = C_n, which is updated as the estimated covariance C_n in the n-th every iteration. This guarantees the converged covariance matrix is a good approximation to the posterior covariance matrix with an uninformative prior.\notherwise Lambda = C_0, this allows that the converged covariance matrix is a weighted average between the posterior covariance matrix with an uninformative prior and C_0.","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"In a nutshell, users only need to change the alpha (α_reg), and the frequency to update the Lambda (update_freq). The user can first try α_reg = 1.0 and update_freq = 0.","category":"page"},{"location":"unscented_kalman_inversion/#Implementation","page":"Unscented Kalman Inversion","title":"Implementation","text":"","category":"section"},{"location":"unscented_kalman_inversion/#Initialization","page":"Unscented Kalman Inversion","title":"Initialization","text":"","category":"section"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"An unscented Kalman inversion object can be created using the EnsembleKalmanProcess constructor by specifying the Unscented() process type.","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"Creating an ensemble Kalman inversion object requires as arguments:","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"The mean value of the observed outputs, a vector of size [d];\nThe covariance of the observational noise, a matrix of size [d × d];\nThe Unscented() process type.","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"The initialization of the Unscented() process requires prior mean and prior covariance, and the the size of the observation d. And user defined hyperparameters  α_reg and update_freq.","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"using EnsembleKalmanProcesses\nusing EnsembleKalmanProcesses.ParameterDistributions\n\n\n# need to choose regularization factor α ∈ (0,1],  \n# when you have enough observation data α=1: no regularization\nα_reg =  1.0\n# update_freq 1 : approximate posterior covariance matrix with an uninformative prior\n#             0 : weighted average between posterior covariance matrix with an uninformative prior and prior\nupdate_freq = 0\n\nprocess = Unscented(prior_mean, prior_cov; α_reg = α_reg, update_freq = update_freq)\nukiobj = EnsembleKalmanProcess(truth_sample, truth.obs_noise_cov, process)\n","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"Note that no information about the forward map is necessary to initialize the Inversion process. The only forward map information required by the inversion process consists of model evaluations at the ensemble elements, necessary to update the ensemble.","category":"page"},{"location":"unscented_kalman_inversion/#Constructing-the-Forward-Map","page":"Unscented Kalman Inversion","title":"Constructing the Forward Map","text":"","category":"section"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"At the core of the forward map mathcalG is the dynamical model PsimathbbR^p rightarrow mathbbR^o (running Psi is usually where the computational heavy-lifting is done), but the map mathcalG may include additional components such as a transformation of the (unbounded) parameters theta to a constrained domain the dynamical model can work with, or some post-processing of the output of Psi to generate the observations. For example, mathcalG may take the following form:","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"mathcalG = mathcalH circ Psi circ mathcalT^-1","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"where mathcalHmathbbR^o rightarrow mathbbR^d is the observation map and mathcalT is the transformation from the constrained to the unconstrained parameter space, such that mathcalT(phi)=theta. A family of standard transformations and their inverses are available in the ParameterDistributions module.","category":"page"},{"location":"unscented_kalman_inversion/#Updating-the-Ensemble","page":"Unscented Kalman Inversion","title":"Updating the Ensemble","text":"","category":"section"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"Once the unscented Kalman inversion object UKIobj has been initialized, any number of updates can be performed using the inversion algorithm.","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"A call to the inversion algorithm can be performed with the update_ensemble! function. This function takes as arguments the UKIobj and the evaluations of the forward map at each element of the current ensemble. The update_ensemble! function then stores the new updated ensemble and the inputted forward map evaluations in UKIobj.","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"The forward map mathcalG maps the space of unconstrained parameters theta to the outputs y in mathbbR^d. In practice, the user may not have access to such a map directly. And the map is a composition of several functions. The update_ensemble! uses only the evalutaions g_ens but not the forward map  ","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"For implementational reasons, the update_ensemble is performed by computing analysis stage first, followed by a prediction of the next sigma ensemble. And the first prediction is done in the initialization.","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"N_iter = 20 # Number of steps of the algorithm\n \nfor n in 1:N_iter\n\n    # define black box parameter to observation map, \n    # with certain parameter transformation related to imposing some constraints\n    # i.e. θ -> e^θ  -> G(e^θ) = y\n    θ_n = get_u_final(ukiobj) # Get current ensemble\n    ϕ_n = transform_unconstrained_to_constrained(prior, θ_n) # Transform parameters to physical/constrained space\n    G_n = [H(Ψ(ϕ_n[:, i])) for i in 1:J]  # Evaluate forward map\n    g_ens = hcat(G_n...)  # Reformat into `d x N_ens` matrix\n    EnsembleKalmanProcesses.update_ensemble!(ukiobj, g_ens) # Update ensemble\nend","category":"page"},{"location":"unscented_kalman_inversion/#Solution","page":"Unscented Kalman Inversion","title":"Solution","text":"","category":"section"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"The solution of the unscented Kalman inversion algorithm is a Gaussian distribution whose mean and covariance can be extracted from the ''last ensemble'' (i.e., the ensemble after the last iteration). The sample mean of the last ensemble is also the \"optimal\" parameter (θ_optim) for the given calibration problem. These statistics can be accessed as follows: ","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"# mean of the Gaussian distribution, also the optimal parameter for the calibration problem\nθ_optim = ukiobj.process.u_mean[end], \n# covariance of the Gaussian distribution\nsigma = ukiobj.process.uu_cov[end]","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"There are two examples: Lorenz96 and Cloudy.","category":"page"},{"location":"examples/Cloudy_example/#Cloudy-Example","page":"Cloudy","title":"Cloudy Example","text":"","category":"section"},{"location":"examples/Cloudy_example/#Overview","page":"Cloudy","title":"Overview","text":"","category":"section"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"This example is based on Cloudy, a microphysics model that simulates how cloud droplets collide and coalesce into larger drops. Collision-coalescence is a crucial process for the formation of rain. ","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"Cloudy is initialized with a mass distribution of the cloud droplets; this distribution is then evolved in time, with more and more droplets colliding and combining into bigger drops according to the droplet-droplet interactions specified by a collision-coalescence kernel. The evolution is completely determined by the shape of the initial distribution and the form of the kernel.","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"This example shows how ensemble Kalman methods can be used to learn the parameters of the initial cloud droplet mass distribution from observations of the moments of that mass distribution at a later time. The collision-coalescence kernel is assumed to be known, but one could also learn the parameters of the kernel instead of the parameters of the droplet distribution (or both).","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"Cloudy is used here in a \"perfect model\" (aka \"known truth\") setting, which means that the \"observations\" are generated by Cloudy itself, by running it with the true parameter values. In more realistic applications, this parameter estimation procedure will use actual measurements of cloud properties to obtain an estimated droplet mass distribution at a previous time.","category":"page"},{"location":"examples/Cloudy_example/#Prerequisites","page":"Cloudy","title":"Prerequisites","text":"","category":"section"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"In order to run this example, you need to install Cloudy.jl (the \"#master\" lets you install the current master branch):","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"pkg > add Cloudy#master","category":"page"},{"location":"examples/Cloudy_example/#Structure","page":"Cloudy","title":"Structure","text":"","category":"section"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"The file Cloudy_example_eki.jl sets up the inverse problem and solves it using ensemble Kalman inversion, and the file Cloudy_example_uki.jl does the same using unscented Kalman inversion. The file DynamicalModel.jl provides the functionality to run the dynamical model Psi, which in this example is Cloudy.","category":"page"},{"location":"examples/Cloudy_example/#Running-the-Example","page":"Cloudy","title":"Running the Example","text":"","category":"section"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"Once Cloudy is installed, the examples can be run from the julia REPL:","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"# Solve inverse problem using ensemble Kalman inversion\ninclude(\"Cloudy_example_eki.jl\")","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"or","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"# Solve inverse problem using unscented Kalman inversion\ninclude(\"Cloudy_example_uki.jl\")","category":"page"},{"location":"examples/Cloudy_example/#What-Does-Cloudy-Do?","page":"Cloudy","title":"What Does Cloudy Do?","text":"","category":"section"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"The mathematical starting point of Cloudy is the stochastic collection equation (SCE; sometimes also called Smoluchowski equation after Marian Smoluchowski), which describes the time rate of change of f = f(m t), the mass distribution function of liquid water droplets, due to the process of collision and coalescence. The distribution function f depends on droplet mass m and time t and is defined such that f(m) text dm denotes the number of droplets with masses in the interval m m + dm per unit volume. ","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"The stochastic collection equation is an integro-differential equation that can be written as ","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"    fracpartial f(m t)partial t = frac12 int_m=0^infty f(m t) f(m-m t)  mathcalC(m m-m)textdm - f(m t) int_m=0^infty f(m t)mathcalC(m m) textdm ","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"where mathcalC(m m) is the collision-coalescence kernel, which  encapsulates the physics of droplet-droplet interactions – it describes the rate at which two drops of masses m and m come into contact and coalesce into a drop of mass m + m. The first term on the right-hand side of the SCE describes the rate of increase of the number of drops having a mass m due to collision and coalescence of drops of masses m and m-m (where the factor frac12 avoids double counting), while the second term describes the rate of reduction of drops of mass m due to collision and coalescence of drops having a mass m with other drops. ","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"We can rewrite the SCE in terms of the moments M_k of f, which are the prognostic variables in Cloudy. They are defined by","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"    M_k = int_0^infty m^k f(m t) textdm","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"The time rate of change of the k-th moment of f is obtained by multiplying the SCE by m^k and integrating over the entire range of droplet masses (from m=0 to infty), which yields","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"    fracpartial M_k(t)partial t = frac12int_0^infty left((m+m)^k - m^k - m^kright) mathcalC(m m)f(m t)f(m t)  textdm textdm  (1)","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"In this example, the kernel is set to be constant – mathcalC(m m) = B = textconst – and the cloud droplet mass distribution is assumed to be a textGamma(k_t theta_t) distribution, scaled by a factor N_0t which denotes the droplet number concentration:","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"f(m t) = fracN_0tGamma(k_t)theta_t^k m^k_t-1 exp(-mtheta_t)","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"The parameter vector phi_t= N_0t k_t theta_t changes over time (as indicated by the subscript t), as the shape of the distribution evolves. In fact, there is a priori no reason to assume that the distribution would retain its Gamma shape over time, but this is a common assumption that is made in order to solve the closure problem (without this assumption, one would have to keep track of infinitely many moments of the mass distribution in order to uniquely identify the distribution f at each time step, which is obviously not practicable).","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"For Gamma mass distribution functions, specifying the first three moments (M_0, M_1, and M_2) is sufficient to uniquely determine the parameter vector phi_t, hence Cloudy solves equation (1) for k = 0 1 2. This mapping of the parameters of the initial cloud droplet mass distribution to the (zeroth-, first-, and second-order) moments of the distribution at a specified end time is done by DynamicalModel.jl.","category":"page"},{"location":"examples/Cloudy_example/#Setting-up-the-Inverse-Problem","page":"Cloudy","title":"Setting up the Inverse Problem","text":"","category":"section"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"The goal is to learn the distribution  parameters at time t = 0, phi_0 = N_00 k_0 theta_0, from observations y = M_0(t_end) M_1(t_end) M_2(t_end) of the zeroth-, first-, and second-order moments of the distribution at time t_end  0 (where t_end = 1.0 in this example). This is a known truth experiment, in which the true parameters phi_0 texttrue are defined to be:","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"N0_true = 300.0  # number of particles (scaling factor for Gamma distribution)\nθ_true = 1.5597  # scale parameter of Gamma distribution\nk_true = 0.0817  # shape parameter of Gamma distribution","category":"page"},{"location":"examples/Cloudy_example/#Priors","page":"Cloudy","title":"Priors","text":"","category":"section"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"All three parameters have to be strictly positive, so we put lower bounds [lbound_N0, lbound_0, lbound_k] on them and use a shifted log transformation, mathcalT(x) = log(x - textlbound), to map the parameter vector phi_0 to the parameter vector theta = mathcalT(N_00) mathcalT(k_0)mathcalT(theta_0) which is normally distributed:","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"mathcalT(N_00)sim mathcalN(45 10)  mathcalT(k_0) sim mathcalN(00 20) mathcalT(theta_0) sim mathcalN(-10 10). ","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"The ensemble Kalman algorithm is then performed in the unconstrained \"theta-space\" (note: don't confuse the parameter vector theta with the theta_t parameter of the Gamma distribution). A more detailed treatment of these kinds of transformations from constrained/physical to unconstrained/computational parameters is found here.","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"In the code, the priors are constructed as follows:","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"par_names = [\"N0\", \"θ\", \"k\"]\n\nc1 = bounded_below(lbound_N0)\nc2 = bounded_below(lbound_θ)\nc3 = bounded_below(lbound_k)\nconstraints = [[c1], [c2], [c3]]\n\n# We choose to use normal distributions to represent the prior distributions of\n# the parameters in the unconstrained space\nd1 = Parameterized(Normal(4.5, 1.0))  #truth is 5.19\nd2 = Parameterized(Normal(0.0, 2.0))  #truth is 0.378\nd3 = Parameterized(Normal(-1.0, 1.0)) #truth is -2.51\ndistributions = [d1, d2, d3]\n\npriors = ParameterDistribution(distributions, constraints, par_names)","category":"page"},{"location":"examples/Cloudy_example/#Observational-Noise","page":"Cloudy","title":"Observational Noise","text":"","category":"section"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"Cloudy produces output  y = M_0(t_end) M_1(t_end) M_2(t_end), which is assumed to be related to the parameter vector theta according to:","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"    y = mathcalG(theta) + eta","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"where mathcalG = Psi circ mathcalT^-1 is the forward map, and the observational noise eta is assumed to be drawn from a  3-dimensional  Gaussian  with distribution mathcalN(0 Gamma_y). In a perfect model setting, the observational noise represents the internal model variability. Since Cloudy is a purely deterministic model, there is no straightforward way of coming up with a covariance Gamma_y for this internal noise. We decide to use a diagonal covariance with the following entries (variances):","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"Γy = convert(Array, Diagonal([100.0, 5.0, 30.0]))","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"Artificial observations (\"truth samples\") are then generated by adding random samples from eta to G_t, the forward map evaluated for the true parameters:","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"for i in 1:n_samples\n    y_t[:, i] = G_t .+ rand(MvNormal(μ, Γy))\nend\n\ntruth = Observations.Observation(y_t, Γy, data_names)","category":"page"},{"location":"examples/Cloudy_example/#Solution-and-Output","page":"Cloudy","title":"Solution and Output","text":"","category":"section"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"Cloudy_example_eki.jl: The optimal parameter vector determined by the ensemble Kalman inversion is the ensemble mean of the particles after the last iteration, which is printed to standard output. An output directory is created, where two files are stored: parameter_storage_eki.jld2 and data_storage_eki.jld2, which contain all parameters and model output from the ensemble Kalman iterations, respectively (both as DataContainers.DataContainer objects). In addition, an animation is produced that shows the evolution of the ensemble of particles over subsequent iterations of the optimization.\nCloudy_example_uki.jl: In addition to a point estimate of the optimal parameter (which is again given by the ensemble mean of the last iteration and printed to standard output), Unscented Kalman inversion also provides a covariance approximation of the posterior distribution. Together, the mean and covariance allow for the reconstruction of a Gaussian approximation of the posterior distribution. The evolution of this Gaussian approximation over subsequent iterations is shown as an animation. All parameters as well as the model output from the unscented Kalman inversion are stored in an output directory, as parameter_storage_uki.jld2 and data_storage_uki.jld2.","category":"page"},{"location":"examples/Cloudy_example/#Playing-Around","page":"Cloudy","title":"Playing Around","text":"","category":"section"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"If you want to play around with the Cloudy examples, you can e.g. change the type or the parameters of the initial cloud droplet mass distribution (see Cloudy.ParticleDistributions for the available distributions), by modifying these lines:","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"ϕ_true = [N0_true, θ_true, k_true]\ndist_true = ParticleDistributions.GammaPrimitiveParticleDistribution(ϕ_true...)","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"(Don't forget to also change dist_type accordingly).","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"You can also experiment with different noise covariances (Γy), priors, vary the number of iterations (N_iter) or ensemble particles (N_ens), etc.","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"EditURL = \"https://github.com/CliMA/EnsembleKalmanProcesses.jl/blob/main/examples/SparseLossMinimization/loss_minimization_sparse_eki.jl\"","category":"page"},{"location":"literated/loss_minimization_sparse_eki/#Minimization-of-simple-loss-functions-with-sparse-EKI","page":"Sparse Minimization Loss","title":"Minimization of simple loss functions with sparse EKI","text":"","category":"section"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"First we load the required packages.","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"using Distributions, LinearAlgebra, Random, Plots\n\nusing EnsembleKalmanProcesses\nusing EnsembleKalmanProcesses.ParameterDistributions\nconst EKP = EnsembleKalmanProcesses","category":"page"},{"location":"literated/loss_minimization_sparse_eki/#Loss-function-with-single-minimum","page":"Sparse Minimization Loss","title":"Loss function with single minimum","text":"","category":"section"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"Here, we minimize the loss function","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"G₁(u) = u - u_* ","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"where u is a 2-vector of parameters and u_* is given; here u_* = (1 0).","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"u★ = [1, 0]\nG₁(u) = [sqrt((u[1] - u★[1])^2 + (u[2] - u★[2])^2)]\nnothing # hide","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"We set the seed for pseudo-random number generator for reproducibility.","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"rng_seed = 41\nRandom.seed!(rng_seed)\nnothing # hide","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"We set a stabilization level, which can aid the algorithm convergence","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"dim_output = 1\nstabilization_level = 1e-3\nΓ_stabilization = stabilization_level * Matrix(I, dim_output, dim_output)","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"The functional is positive so to minimize it we may set the target to be 0,","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"G_target = [0]\nnothing # hide","category":"page"},{"location":"literated/loss_minimization_sparse_eki/#Prior-distributions","page":"Sparse Minimization Loss","title":"Prior distributions","text":"","category":"section"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"As we work with a Bayesian method, we define a prior. This will behave like an \"initial guess\" for the likely region of parameter space we expect the solution to live in.","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"prior_distributions = [Parameterized(Normal(0, 2)), Parameterized(Normal(0, 2))]\n\nconstraints = [[no_constraint()], [no_constraint()]]\n\nparameter_names = [\"u1\", \"u2\"]\n\nprior = ParameterDistribution(prior_distributions, constraints, parameter_names)","category":"page"},{"location":"literated/loss_minimization_sparse_eki/#Calibration","page":"Sparse Minimization Loss","title":"Calibration","text":"","category":"section"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"We choose the number of ensemble members and the number of iterations of the algorithm","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"N_ensemble = 20\nN_iterations = 10\nnothing # hide","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"The initial ensemble is constructed by sampling the prior","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"initial_ensemble = EKP.construct_initial_ensemble(prior, N_ensemble; rng_seed = rng_seed)","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"Sparse EKI parameters","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"γ = 1.0\nthreshold_value = 1e-2\nreg = 1e-3\nuc_idx = [1, 2]\n\nprocess = SparseInversion(γ, threshold_value, uc_idx, reg)","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"We then initialize the Ensemble Kalman Process algorithm, with the initial ensemble, the target, the stabilization and the process type (for sparse EKI this is SparseInversion).","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"ensemble_kalman_process = EKP.EnsembleKalmanProcess(initial_ensemble, G_target, Γ_stabilization, process)","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"Then we calibrate by (i) obtaining the parameters, (ii) calculate the loss function on the parameters (and concatenate), and last (iii) generate a new set of parameters using the model outputs:","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"for i in 1:N_iterations\n    params_i = get_u_final(ensemble_kalman_process)\n\n    g_ens = hcat([G₁(params_i[:, i]) for i in 1:N_ensemble]...)\n\n    EKP.update_ensemble!(ensemble_kalman_process, g_ens)\nend","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"and visualize the results:","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"u_init = get_u_prior(ensemble_kalman_process)\n\nanim_unique_minimum = @animate for i in 1:N_iterations\n    u_i = get_u(ensemble_kalman_process, i)\n\n    plot(\n        [u★[1]],\n        [u★[2]],\n        seriestype = :scatter,\n        markershape = :star5,\n        markersize = 11,\n        markercolor = :red,\n        label = \"optimum u⋆\",\n    )\n\n    plot!(\n        u_i[1, :],\n        u_i[2, :],\n        seriestype = :scatter,\n        xlims = extrema(u_init[1, :]),\n        ylims = extrema(u_init[2, :]),\n        xlabel = \"u₁\",\n        ylabel = \"u₂\",\n        markersize = 5,\n        markeralpha = 0.6,\n        markercolor = :blue,\n        label = \"particles\",\n        title = \"EKI iteration = \" * string(i),\n    )\nend\nnothing # hide","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"The results show that the minimizer of G_1 is u=u_*.","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"gif(anim_unique_minimum, \"unique_minimum_sparse.gif\", fps = 1) # hide","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"This page was generated using Literate.jl.","category":"page"},{"location":"#EnsembleKalmanProcesses","page":"Home","title":"EnsembleKalmanProcesses","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"EnsembleKalmanProcesses.jl (EKP) is a library of derivative-free Bayesian optimization techniques based on the Ensemble Kalman Filters, a well known family of approximate filters used for data assimilation. Currently, the following methods are implemented in the library:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Ensemble Kalman Inversion (EKI) - The traditional optimization technique based on the Ensemble Kalman Filter EnKF (Iglesias, Law, Stuart, 2013),\nEnsemble Kalman Sampler (EKS) - also obtains a Gaussian Approximation of the posterior distribution, through a Monte Carlo integration (Garbuno-Inigo, Hoffmann, Li, Stuart, 2020),\nUnscented Kalman Inversion (UKI) - also obtains a Gaussian Approximation of the posterior distribution, through a quadrature based integration approach (Huang, Schneider, Stuart, 2020),\nSparsity-inducing Ensemble Kalman Inversion (SEKI) - Additionally adds approximate L^0 and L^1 penalization to the EKI (Schneider, Stuart, Wu, 2020).","category":"page"},{"location":"","page":"Home","title":"Home","text":"Module Purpose\nEnsembleKalmanProcesses.jl Collection of all tools\nEnsembleKalmanProcess.jl Implementations of EKI, EKS, UKI and SEKI\nObservations.jl Structure to hold observational data\nParameterDistributions.jl Structures to hold prior and posterior distributions\nDataContainers.jl Structure to hold model parameters and outputs\nLocalizers.jl Covariance localization kernels","category":"page"},{"location":"#Authors","page":"Home","title":"Authors","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"EnsembleKalmanProcesses.jl is being developed by the Climate Modeling Alliance.","category":"page"},{"location":"API/DataContainers/#DataContainers","page":"DataContainers","title":"DataContainers","text":"","category":"section"},{"location":"API/DataContainers/","page":"DataContainers","title":"DataContainers","text":"CurrentModule = EnsembleKalmanProcesses.DataContainers","category":"page"},{"location":"API/DataContainers/","page":"DataContainers","title":"DataContainers","text":"DataContainer\nPairedDataContainer\nsize","category":"page"},{"location":"API/DataContainers/#EnsembleKalmanProcesses.DataContainers.DataContainer","page":"DataContainers","title":"EnsembleKalmanProcesses.DataContainers.DataContainer","text":"DataContainer{FT <: Real}\n\nContainer to store data samples as columns in an array.\n\n\n\n\n\n","category":"type"},{"location":"API/DataContainers/#EnsembleKalmanProcesses.DataContainers.PairedDataContainer","page":"DataContainers","title":"EnsembleKalmanProcesses.DataContainers.PairedDataContainer","text":"PairedDataContainer{FT <: Real}\n\nstores input - output pairs as data containers, there must be an equal number of inputs and outputs\n\n\n\n\n\n","category":"type"},{"location":"API/DataContainers/#Base.size","page":"DataContainers","title":"Base.size","text":"size(c<:ConstraintType)\n\nA constraint has size 1.\n\n\n\n\n\nsize(dc::DataContainer, idx::IT) where {IT <: Integer}\n\nReturns the size of the stored data. If idx provided, it returns the size along dimension idx.\n\n\n\n\n\nsize(pdc::PairedDataContainer, idx::IT) where {IT <: Integer}\n\nReturns the sizes of the inputs and ouputs along dimension idx (if provided)\n\n\n\n\n\n","category":"function"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"EditURL = \"https://github.com/CliMA/EnsembleKalmanProcesses.jl/blob/main/examples/AerosolActivation/aerosol_activation.jl\"","category":"page"},{"location":"literated/aerosol_activation/#Aerosol-Activation-Example","page":"Aerosol activation","title":"Aerosol Activation Example","text":"","category":"section"},{"location":"literated/aerosol_activation/#Overview","page":"Aerosol activation","title":"Overview","text":"","category":"section"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"This example is based on AerosolActivation module which is a part of the   CloudMicrophysics.jl package. The AerosolActivation module computes the total number and mass   of aerosol particles that get activated and become cloud droplets,   given the atmospheric conditions and   the initial aerosol size distribution and properties. See the AerosolActivation module   docs   for derivation and description of all input parameters.","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"In this example we use the ensemble Kalman methods to learn   two parameters that describe the chemical composition of aerosol particles   based on the observed total number and mass of activated particles. The AerosolActivation model is used here in a \"perfect model\" setting,   meaning that the observations are generated by the same module   we are calibrating.","category":"page"},{"location":"literated/aerosol_activation/#Prerequisites","page":"Aerosol activation","title":"Prerequisites","text":"","category":"section"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"The example depends on some standard Julia libraries,   as well as the CliMA packages:   EnsembleKalmanProcess.jl,   CLIMAParameters.jl and   CloudMicrophysics.jl. To ensure that all the dependencies are met   start Julia using the Project.toml file provided in the example   and run the Julia package manager to download all the dependecies.","category":"page"},{"location":"literated/aerosol_activation/#Example","page":"Aerosol activation","title":"Example","text":"","category":"section"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"We begin by importing some standard Julia modules, the Ensemble Kalman Process modules, CLIMA Parameter modules and Aerosol Activation modules.","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"using Plots\nusing Distributions\nusing LinearAlgebra\nusing Random\nrng_seed = 44\n\nimport EnsembleKalmanProcesses\nconst EKP = EnsembleKalmanProcesses\n\nimport CLIMAParameters\nconst CP = CLIMAParameters\nstruct EarthParameterSet <: CP.AbstractEarthParameterSet end\nconst param_set = EarthParameterSet()\n\nimport CloudMicrophysics\nconst AM = CloudMicrophysics.AerosolModel\nconst AA = CloudMicrophysics.AerosolActivation\n\nimport Thermodynamics\nconst TD = Thermodynamics\nnothing # hide","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"Next, we provide the information about the priors of the parameters we want to learn. We are calibrating two parameters decribing the aerosol properties - namely the aerosol molar mass and the osmotic coefficient.","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"parameter_names = [\"molar_mass\", \"osmotic_coeff\"]\nnothing # hide","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"In this test we do know the parameter values. We use them to test the convergence of EKP for aerosol activation.","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"molar_mass_true = 0.058443\nosmotic_coeff_true = 0.9\ndefault_params = [molar_mass_true, osmotic_coeff_true]\nnothing # hide","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"Both parameters have to be positive definite, therefore we define the constraints to be bounded below by zero.","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"c1 = EKP.ParameterDistributions.bounded_below(0.0)\nc2 = EKP.ParameterDistributions.bounded_below(0.0)\nconstraints = [[c1], [c2]]\nnothing # hide","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"We don't have any other prior knowledge about the calibrated parameters. Defining the prior distribution in the constrained space to be normal with the mean equal to zero and the standard deviation equal to one, ensures that the default parameter values are well within the assumed prior pdf.","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"d1 = EKP.ParameterDistributions.Parameterized(Distributions.Normal(0, 1))\nd2 = EKP.ParameterDistributions.Parameterized(Distributions.Normal(0, 1))\ndistributions = [d1, d2]\nnothing # hide","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"This concludes the setup of priors.","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"priors = EKP.ParameterDistributions.ParameterDistribution(distributions, constraints, parameter_names)\nnothing # hide","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"Next we define the atmospheric conditions for which the calibration will take place, (air temperature in K, air pressure in Pa vertical velocity in m/s and vapor specific humidity assuming its saturated in kg/kg) This can be changed later to include more than one (T p w) combination in the calibration process","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"T = 283.15\np = 1e5\nw = 5.0\np_vs = TD.saturation_vapor_pressure(param_set, T, TD.Liquid())\nq_vs = 1 / (1 - CP.Planet.molmass_ratio(param_set) * (p_vs - p) / p_vs)\nq = TD.PhasePartition(q_vs, 0.0, 0.0)\nnothing # hide","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"We also define the aerosol size distribution (lognormal, 1 mode) with (mean radius in m, geometric stdev, number concentration 1/m³). These can also be changed later to include different initial size distributions.","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"r_dry = 0.243e-6\nstdev = 1.4\nN = 100.0 * 1e6 # since 1/cm³ = 1e6 1/m³\nnothing # hide","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"Finally, we define additional parameters that describe the aerosol properties. The chosen aerosol is sea salt.","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"dissoc_seasalt = 2.0\nsoluble_mass_frac_seasalt = 1.0\nrho_seasalt = 2170.0;\nnothing # hide","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"We define a wrapper function that runs the aerosol activation module with two input parameters that will be calibrated by EKP. The output observations are the number and mass of activated aerosol.","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"function run_activation_model(molar_mass_calibrated, osmotic_coeff_calibrated)\n\n    accum_mode_seasalt = AM.Mode_B(\n        r_dry,\n        stdev,\n        N,\n        (1.0,),\n        (soluble_mass_frac_seasalt,),\n        (osmotic_coeff_calibrated,),\n        (molar_mass_calibrated,),\n        (dissoc_seasalt,),\n        (rho_seasalt,),\n        1,\n    )\n\n    aerosol_distr = AM.AerosolDistribution((accum_mode_seasalt,))\n    N_act = AA.total_N_activated(param_set, aerosol_distr, T, p, w, q)\n    M_act = AA.total_M_activated(param_set, aerosol_distr, T, p, w, q)\n    return [N_act, M_act]\nend\nnothing # hide","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"This example is run in a \"perfect model setting\", meaning the model that we calibrate is also used to generate observations. We use the total number and mass of activated aerosol particles as our observational data.","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"observation_data_names = [\"N_act\", \"M_act\"];\nnothing # hide","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"We generate artificial truth samples based on the default values of parameters we are calibrating.","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"n_samples = 10\nG_t = run_activation_model(molar_mass_true, osmotic_coeff_true)\ny_t = zeros(length(G_t), n_samples)\n\nΓy = convert(Array, LinearAlgebra.Diagonal([0.01 * G_t[1], 0.01 * G_t[2]]))\nμ = zeros(length(G_t));\nnothing # hide","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"And add noise to the generated truth sample.","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"for i in 1:n_samples\n    y_t[:, i] = G_t .+ rand(Distributions.MvNormal(μ, Γy))\nend\n\ntruth_array = EKP.Observations.Observation(y_t, Γy, observation_data_names)\nnothing # hide","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"One could try for the truth to be a mean of the generated array. Or do the calibration for all individual truth samples and then compute the mean of calibrated parameters. For now we are just taking one truth array member.","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"truth_sample = truth_array.samples[1];\nnothing # hide","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"We use 50 ensemble members and do 10 iterations.","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"N_ens = 50\nN_iter = 10\n\ninitial_par = EKP.construct_initial_ensemble(priors, N_ens; rng_seed)\nekiobj = EKP.EnsembleKalmanProcess(initial_par, truth_sample, truth_array.obs_noise_cov, EKP.Inversion(), Δt = 1)\nnothing # hide","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"Finally, we can run the Ensemble Kalman Process calibration.","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"ϕ_n_values = []\nfor n in 1:N_iter\n    θ_n = EKP.get_u_final(ekiobj)\n\n    ϕ_n = mapslices(x -> EKP.ParameterDistributions.transform_unconstrained_to_constrained(priors, x), θ_n; dims = 1)\n\n    G_n = [run_activation_model(ϕ_n[:, i]...) for i in 1:N_ens]\n\n    G_ens = hcat(G_n...)\n    EKP.update_ensemble!(ekiobj, G_ens)\n\n    global ϕ_n_values = vcat(ϕ_n_values, [ϕ_n])\nend\nnothing # hide","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"We define some simple functions for plotting the data.","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"function plot_ensemble_scatter(id)\n\n    ensemble_member = 1:N_ens\n\n    if id == 1\n        ylabel = \"Molar mass [kg/mol]\"\n        filename = \"molar_mass_scatter.svg\"\n    end\n    if id == 2\n        ylabel = \"Osmotic coefficient [-]\"\n        filename = \"osmotic_coeff_scatter.svg\"\n    end\n\n    plot(\n        ensemble_member,\n        ϕ_n_values[1][id, 1:N_ens],\n        seriestype = :scatter,\n        xlabel = \"Ensemble Number\",\n        ylabel = ylabel,\n        legend = false,\n    )\n\n    for it in 2:N_iter\n        plot!(ensemble_member .+ ((it - 1) * 50), ϕ_n_values[it][id, 1:N_ens], seriestype = :scatter, legend = false)\n    end\n\n    current()\n    savefig(filename)\nend\n\nfunction plot_ensemble_means(id)\n\n    number_of_iters = 1:N_iter\n    means = zeros(N_iter)\n\n    for it in 1:N_iter\n        means[it] = mean(ϕ_n_values[it][id, 1:N_ens])\n    end\n\n    if id == 1\n        ylabel = \"Molar mass [kg/mol]\"\n        filename = \"molar_mass_average.svg\"\n    end\n    if id == 2\n        ylabel = \"Osmotic coefficient [-]\"\n        filename = \"osmotic_coeff_average.svg\"\n    end\n\n    plot(\n        number_of_iters,\n        means,\n        markershape = :star5,\n        xticks = number_of_iters,\n        xlabel = \"Iteration Number\",\n        ylabel = ylabel,\n        label = \"Ensemble Mean\",\n    )\n    hline!([default_params[id]], label = \"true value\")\n\n    savefig(filename)\nend\nnothing # hide","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"We plot the ensemble members and the ensemble mean for the molar mass and osmotic coefficient.","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"plot_ensemble_scatter(1)\nplot_ensemble_means(1)\nplot_ensemble_scatter(2)\nplot_ensemble_means(2)","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"(Image: ) (Image: ) (Image: ) (Image: )","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"Finally, we test that the parameter values obtained via EnsembleKalmanProcesses.jl are close to the known true parameter values.","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"molar_mass_ekp = round(mean(ϕ_n_values[N_iter][1, 1:N_ens]), digits = 6)\nosmotic_coeff_ekp = round(mean(ϕ_n_values[N_iter][2, 1:N_ens]), digits = 6)\n\nprintln(\"Molar mass [kg/mol]: \", molar_mass_ekp, \" vs \", molar_mass_true)\nprintln(\"Osmotic coefficient [-]: \", osmotic_coeff_ekp, \" vs \", osmotic_coeff_true)","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"This page was generated using Literate.jl.","category":"page"}]
}
