<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Parallelism and HPC · EnsembleKalmanProcesses.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.svg" alt="EnsembleKalmanProcesses.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">EnsembleKalmanProcesses.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../installation_instructions/">Installation instructions</a></li><li><input class="collapse-toggle" id="menuitem-3" type="checkbox"/><label class="tocitem" for="menuitem-3"><span class="docs-label">Examples</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../literated/sinusoid_example/">Simple example</a></li><li><a class="tocitem" href="../examples/Cloudy_example/">Cloudy</a></li><li><a class="tocitem" href="../examples/lorenz_example/">Lorenz</a></li><li><a class="tocitem" href="../literated/loss_minimization/">Minimization Loss</a></li><li><a class="tocitem" href="../literated/loss_minimization_sparse_eki/">Sparse Minimization Loss</a></li><li><a class="tocitem" href="../literated/aerosol_activation/">Aerosol activation</a></li><li><a class="tocitem" href="../examples/sinusoid_example_toml/">TOML interface</a></li><li><a class="tocitem" href="../examples/ClimateMachine_example/">HPC interfacing example: ClimateMachine</a></li><li><a class="tocitem" href="../examples/template_example/">Template</a></li></ul></li><li><a class="tocitem" href="../ensemble_kalman_inversion/">Ensemble Kalman Inversion</a></li><li><a class="tocitem" href="../ensemble_kalman_sampler/">Ensemble Kalman Sampler</a></li><li><a class="tocitem" href="../unscented_kalman_inversion/">Unscented Kalman Inversion</a></li><li><a class="tocitem" href="../parameter_distributions/">Prior distributions</a></li><li><a class="tocitem" href="../internal_data_representation/">Internal data representation</a></li><li><a class="tocitem" href="../localization/">Localization and SEC</a></li><li class="is-active"><a class="tocitem" href>Parallelism and HPC</a><ul class="internal"><li><a class="tocitem" href="#Case-1:-Parallel-code-within-Julia"><span>Case 1: Parallel code within Julia</span></a></li><li><a class="tocitem" href="#Running-examples:"><span>Running examples:</span></a></li></ul></li><li><a class="tocitem" href="../observations/">Observations</a></li><li><input class="collapse-toggle" id="menuitem-12" type="checkbox"/><label class="tocitem" for="menuitem-12"><span class="docs-label">API</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../API/ParameterDistributions/">ParameterDistributions</a></li><li><a class="tocitem" href="../API/Observations/">Observations</a></li><li><a class="tocitem" href="../API/DataContainers/">DataContainers</a></li><li><a class="tocitem" href="../API/EnsembleKalmanProcess/">EnsembleKalmanProcess</a></li><li><a class="tocitem" href="../API/Inversion/">Inversion</a></li><li><a class="tocitem" href="../API/Unscented/">Unscented</a></li><li><a class="tocitem" href="../API/Sampler/">Sampler</a></li><li><a class="tocitem" href="../API/SparseInversion/">SparseInversion</a></li><li><a class="tocitem" href="../API/TOMLInterface/">TOML Interface</a></li><li><a class="tocitem" href="../API/Localizers/">Localizers</a></li></ul></li><li><a class="tocitem" href="../contributing/">Contributing</a></li><li><a class="tocitem" href="../glossary/">Glossary</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Parallelism and HPC</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Parallelism and HPC</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/CliMA/EnsembleKalmanProcesses.jl/blob/main/docs/src/parallel_hpc.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="parallel-hpc"><a class="docs-heading-anchor" href="#parallel-hpc">Parallelism and High Performance Computing (HPC)</a><a id="parallel-hpc-1"></a><a class="docs-heading-anchor-permalink" href="#parallel-hpc" title="Permalink"></a></h1><p>One benefit of ensemble methods is their ability to be parallelized. The parallellism occurs outside of the EKP update, and so is not present in the source code. On this page we provide suggestions for parallelization for two types of problems:</p><ol><li>Run a parallel loop or map within your current Julia session.</li><li>Run a parallel loop through a read/write file interface and workload manager. We provide some utilities to read/write files in a TOML format.</li></ol><h2 id="Case-1:-Parallel-code-within-Julia"><a class="docs-heading-anchor" href="#Case-1:-Parallel-code-within-Julia">Case 1: Parallel code within Julia</a><a id="Case-1:-Parallel-code-within-Julia-1"></a><a class="docs-heading-anchor-permalink" href="#Case-1:-Parallel-code-within-Julia" title="Permalink"></a></h2><p>Let&#39;s look at the simple example for the <a href="../examples/lorenz_example/#Lorenz-example">Lorenz 96 dynamical system</a>. In particular we&#39;ll focus on the evaluations of the Lorenz 96 solver explicitly written in <a href="https://github.com/CliMA/EnsembleKalmanProcesses.jl/blob/main/examples/Lorenz/GModel.jl"><code>GModel.jl</code></a> that contains the following loop over the <code>N_ens</code>-sized ensemble:</p><pre><code class="language-julia hljs">function run_ensembles(settings, lorenz_params, nd, N_ens)
    g_ens = zeros(nd, N_ens)
    for i in 1:N_ens
        # run the model with the current parameters, i.e., map θ to G(θ)
        g_ens[:, i] = lorenz_forward(settings, lorenz_params[i])
    end
    return g_ens
end</code></pre><p>Each ensemble member <code>i</code> runs the Lorenz 96 model with <code>settings</code> configuration, and model parameters <code>lorenz_params[i]</code>. The runs do not interact with each other, and the user has several options to parallelize.</p><h2 id="Running-examples:"><a class="docs-heading-anchor" href="#Running-examples:">Running examples:</a><a id="Running-examples:-1"></a><a class="docs-heading-anchor-permalink" href="#Running-examples:" title="Permalink"></a></h2><p>All of the following example cases are covered in <a href="https://github.com/CliMA/EnsembleKalmanProcesses.jl/blob/main/examples/Lorenz/distributed_Lorenz_example.jl"><code>distributed_Lorenz_example.jl</code></a>. At the top of file uncomment one of the following options</p><ol><li><code>case = multithread</code> </li><li><code>case = pmap</code> </li><li><code>case = distfor</code> </li></ol><h3 id="Multithreading,-@threads"><a class="docs-heading-anchor" href="#Multithreading,-@threads">Multithreading, <code>@threads</code></a><a id="Multithreading,-@threads-1"></a><a class="docs-heading-anchor-permalink" href="#Multithreading,-@threads" title="Permalink"></a></h3><p>To parallelize with multithreading, julia must call the file with a prespecified number of threads. For example, for 4 threads, </p><pre><code class="nohighlight hljs">$ julia --project -t 4 distributed_Lorenz_example.jl</code></pre><p>We exploit the multithreading over <code>N_ens</code> ensemble members in this example with the following loop in <a href="https://github.com/CliMA/EnsembleKalmanProcesses.jl/blob/main/examples/Lorenz/GModel_multithread.jl"><code>GModel_multithread.jl</code></a>:</p><pre><code class="language-julia hljs">function run_ensembles(settings, lorenz_params, nd, N_ens)
    g_ens = zeros(nd, N_ens)
    Threads.@threads for i in 1:N_ens
        # run the model with the current parameters, i.e., map θ to G(θ)
        g_ens[:, i] = lorenz_forward(settings, lorenz_params[i])
    end
    return g_ens
end</code></pre><p>You can read more about multi-threading <a href="https://docs.julialang.org/en/v1/manual/multi-threading/">here</a>.</p><h3 id="Parallel-map,-pmap"><a class="docs-heading-anchor" href="#Parallel-map,-pmap">Parallel map, <code>pmap</code></a><a id="Parallel-map,-pmap-1"></a><a class="docs-heading-anchor-permalink" href="#Parallel-map,-pmap" title="Permalink"></a></h3><p>When using multiple processes, the Julia environment must first be loaded on each worker processor. We include these lines in the main file</p><pre><code class="language-julia hljs">using Distributed
addprocs(4; exeflags = &quot;--project&quot;)</code></pre><p>And we would call the file is called by</p><pre><code class="nohighlight hljs">$ julia --project distributed_Lorenz_example</code></pre><p>This ensures that we obtain <code>4</code> worker processes that are loaded with julia&#39;s current environment specified by <code>--project</code> (unlike when calling <code>julia --project -p 4</code>). We use  <code>pmap</code> to apply a function to each element of the list (i.e the ensemble member configurations). For example, see the following code from <a href="https://github.com/CliMA/EnsembleKalmanProcesses.jl/blob/main/examples/Lorenz/GModel_pmap.jl"><code>GModel_pmap.jl</code></a>,</p><pre><code class="language-julia hljs">using Distributed
function run_ensembles(settings, lorenz_params, nd, N_ens)
    g_ens = zeros(nd, N_ens)
    g_ens[:, :] = vcat(pmap(x -&gt; lorenz_forward(settings, x), lorenz_params)...)
    return g_ens
end</code></pre><p>If <code>pmap</code> is called within a module, that module will also need to be loaded on all workers. For this we use the macro <code>@everywhere module XYZ</code>.</p><p>You can read more about <code>pmap</code> <a href="https://docs.julialang.org/en/v1/manual/distributed-computing/#Parallel-Map-and-Loops">here</a>.</p><h3 id="Distributed-loop,-@distributed-for"><a class="docs-heading-anchor" href="#Distributed-loop,-@distributed-for">Distributed loop, <code>@distributed for</code></a><a id="Distributed-loop,-@distributed-for-1"></a><a class="docs-heading-anchor-permalink" href="#Distributed-loop,-@distributed-for" title="Permalink"></a></h3><p>When using multiple processes, the Julia environment must also be loaded on each worker processor. We include these lines in the main file</p><pre><code class="language-julia hljs">using Distributed
addprocs(4; exeflags = &quot;--project&quot;)</code></pre><p>And we would call the file is called by</p><pre><code class="nohighlight hljs">$ julia --project distributed_Lorenz_example</code></pre><p>When using distributed loops, it is necessary to be able to write to shared memory. To do this we use the <a href="https://docs.julialang.org/en/v1/manual/distributed-computing/#man-shared-arrays">SharedArrays</a> package. For example, see the following distributed loop in <a href="https://github.com/CliMA/EnsembleKalmanProcesses.jl/blob/main/examples/Lorenz/GModel_distfor.jl"><code>GModel_distfor</code></a> </p><pre><code class="language-julia hljs">using Distributed
using SharedArrays
function run_ensembles(settings, lorenz_params, nd, N_ens)
    g_ens = SharedArray{Float64}(nd, N_ens)
    @sync @distributed for i in 1:N_ens
        # run the model with the current parameters, i.e., map θ to G(θ)
        g_ens[:, i] = lorenz_forward(settings, lorenz_params[i])
    end
    return g_ens
end</code></pre><p><code>@sync</code> forces the code to wait until all processes in the <code>@distributed for</code> loop are complete before continuing.</p><p>If <code>@distributed for</code> is used within another module, that module will also need to be loaded on each worker processor. For this we use the macro <code>@everywhere module XYZ</code>.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p><code>@distributed for</code> is most performant when there is a large ensemble, <code>N_ens</code>, and the forward map is computationally cheap. Otherwise, <code>pmap</code> is usually the preferred choice.</p></div></div><p>You can read more about <code>@distributed</code> <a href="https://docs.julialang.org/en/v1/manual/distributed-computing/#Parallel-Map-and-Loops">here</a></p><h3 id="Case-2:-HPC-interface"><a class="docs-heading-anchor" href="#Case-2:-HPC-interface">Case 2: HPC interface</a><a id="Case-2:-HPC-interface-1"></a><a class="docs-heading-anchor-permalink" href="#Case-2:-HPC-interface" title="Permalink"></a></h3><p>Some applications involve interfacing with non-Julia code or using HPC workload managers. In these cases we suggest using an alternative workflow where one interleaves scripts that launch EKP updates and scripts that runs the model. One possible implementation is the following loop</p><ul><li><p>Step <code>0</code>(a). Write an ensemble of parameter files <code>parameters_0_i</code> for <code>i = 1:N_ens</code>, with each parameter file containing a sample from the prior distribution.</p></li><li><p>Step <code>0</code>(b). Construct EKP EKP object and save in <code>jld2</code> format e.g. <code>ekpobject.jld2</code>.</p></li><li><p>for <code>n = 1,..., N_it</code>, do:</p><ul><li>Step <code>n</code>(a). Run <code>N_ens</code> forward models, with forward model <code>i</code> running with the corresponding parameter file <code>parameters_{n-1}_i</code>. Write processed output data to file <code>data_{n-1}_i</code>.</li><li>Step <code>n</code>(b). Run the ensemble update, by loading both <code>ekpobject.jld2</code> and reading in the parameter files <code>data_{n-1}_i</code> for <code>i = 1:N_ens</code>. Perform the EKP update step. Write new parameter files <code>parameters_n_i</code> for <code>i = 1:N_ens</code>. Save the ensemble object in <code>ekpobject.jld2</code></li><li>iterate <code>n -&gt; n+1</code>.</li></ul></li></ul><p>In <a href="../examples/ClimateMachine_example/#HPC-interfacing-example:-ClimateMachine">HPC interfacing example: ClimateMachine</a> we implement a similar loop to interface with a SLURM workload manager for HPC. Here, <code>sbatch</code> scripts are used to run each component of the calibration procedure. The outer loop over the EKP iterations lives in the overarching <code>sbatch</code> script, and for each iteration, the inner loop are realised as &quot;arrays&quot; of slurm jobs (<code>1, ..., N_ens</code>), launched for each ensemble member. The code excerpt below, taken from <a href="https://github.com/CliMA/EnsembleKalmanProcesses.jl/blob/main/examples/ClimateMachine/ekp_calibration.sbatch"><code>ekp_calibration.sbatch</code></a> for details), illustrates this procedure:</p><pre><code class="language-csh hljs"># First call to calibrate.jl will create the ensemble files from the priors
id_init_ens=$(sbatch --parsable ekp_init_calibration.sbatch)
for it in $(seq 1 1 $n_it)
do
# Parallel runs of forward model
if [ &quot;$it&quot; = &quot;1&quot; ]; then
    id_ens_array=$(sbatch --parsable --kill-on-invalid-dep=yes --dependency=afterok:$id_init_ens --array=1-$n ekp_single_cm_run.sbatch $it)
else
    id_ens_array=$(sbatch --parsable --kill-on-invalid-dep=yes --dependency=afterok:$id_ek_upd --array=1-$n ekp_single_cm_run.sbatch $it)
fi
id_ek_upd=$(sbatch --parsable --kill-on-invalid-dep=yes --dependency=afterok:$id_ens_array --export=n=$n ekp_cont_calibration.sbatch $it)
done</code></pre><p>Here a dependency tree is set up in SLURM, which iterates calls to the scripts <code>ekp_single_cm_run.sbatch</code> (which runs the forward model on HPC) and <code>ekp_cont_calibration.sbatch</code> (which performs an EKI update). We find this a smooth workflow that uses HPC resources well, and can likely be set up on other workload managers.</p><p>For more details see the <a href="https://github.com/CliMA/EnsembleKalmanProcesses.jl/tree/main/examples/ClimateMachine">code</a> and docs for the <a href="../examples/ClimateMachine_example/#HPC-interfacing-example:-ClimateMachine">HPC interfacing example: ClimateMachine</a>.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../localization/">« Localization and SEC</a><a class="docs-footer-nextpage" href="../observations/">Observations »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Tuesday 6 December 2022 22:13">Tuesday 6 December 2022</span>. Using Julia version 1.7.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
