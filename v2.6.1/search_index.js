var documenterSearchIndex = {"docs":
[{"location":"API/Sampler/#Ensemble-Kalman-Sampler","page":"Sampler","title":"Ensemble Kalman Sampler","text":"","category":"section"},{"location":"API/Sampler/#EnsembleKalmanProcesses.Sampler","page":"Sampler","title":"EnsembleKalmanProcesses.Sampler","text":"Sampler{FT<:AbstractFloat, T <:SamplerType} <: Process\n\nAn ensemble Kalman Sampler process. with type Sampler Type (e.g., ALDI or EKS).\n\nConstructor\n\nSampler(prior::ParameterDistribution) # ALDI update (samplertype=\"aldi\") Sampler(prior::ParameterDistribution; samplertype = \"eks\") # EKS update\n\nFields\n\nprior_mean::Vector{FT} where FT<:AbstractFloat: Mean of Gaussian parameter prior in unconstrained space\nprior_cov::Union{LinearAlgebra.UniformScaling{FT}, AbstractMatrix{FT}} where FT<:AbstractFloat: Covariance of Gaussian parameter prior in unconstrained space\n\nConstructors\n\nSampler(prior; sampler_type)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/EnsembleKalmanSampler.jl:36.\n\n\n\n\n\n","category":"type"},{"location":"API/Sampler/#EnsembleKalmanProcesses.eks_update","page":"Sampler","title":"EnsembleKalmanProcesses.eks_update","text":"eks_update(\n    ekp::EnsembleKalmanProcess,\n    u::AbstractArray{FT<:Real, 2},\n    g::AbstractArray{FT<:Real, 2},\n    process::Sampler{FT<:Real, EKS}\n) -> Any\n\n\nReturns the updated parameter vectors given their current values and the corresponding forward model evaluations, using the sampler algorithm of (Garbuno-Iñigo Hoffmann Li Stuart 2019)\n\nThe current implementation assumes that rows of u and g correspond to ensemble members, so it requires passing the transpose of the u and g arrays associated with ekp.\n\n\n\n\n\neks_update(\n    ekp::EnsembleKalmanProcess,\n    u::AbstractArray{FT<:Real, 2},\n    g::AbstractArray{FT<:Real, 2},\n    process::Sampler{FT<:Real, ALDI}\n) -> Any\n\n\nReturns the updated parameter vectors given their current values and the corresponding forward model evaluations, using the sampler algorithm of (Garbuno-Iñigo Nüsken Reich 2020)\n\nThe current implementation assumes that rows of u and g correspond to ensemble members, so it requires passing the transpose of the u and g arrays associated with ekp.\n\n\n\n\n\n","category":"function"},{"location":"API/Unscented/#Unscented-Kalman-Inversion","page":"Unscented","title":"Unscented Kalman Inversion","text":"","category":"section"},{"location":"API/Unscented/#EnsembleKalmanProcesses.Unscented","page":"Unscented","title":"EnsembleKalmanProcesses.Unscented","text":"Unscented{FT<:AbstractFloat, IT<:Int} <: Process\n\nAn unscented Kalman Inversion process.\n\nFields\n\nu_mean::Any: an interable of arrays of size N_parameters containing the mean of the parameters (in each uki iteration a new array of mean is added), note - this is not the same as the ensemble mean of the sigma ensemble as it is taken prior to prediction\nuu_cov::Any: an iterable of arrays of size (N_parameters x N_parameters) containing the covariance of the parameters (in each uki iteration a new array of cov is added), note - this is not the same as the ensemble cov of the sigma ensemble as it is taken prior to prediction\nobs_pred::Any: an iterable of arrays of size N_y containing the predicted observation (in each uki iteration a new array of predicted observation is added)\nc_weights::AbstractVecOrMat{FT} where FT<:AbstractFloat: weights in UKI\nmean_weights::AbstractVector{FT} where FT<:AbstractFloat\ncov_weights::AbstractVector{FT} where FT<:AbstractFloat\nN_ens::Int64: number of particles 2N+1 or N+2\nΣ_ω::AbstractMatrix{FT} where FT<:AbstractFloat: covariance of the artificial evolution error\nΣ_ν_scale::AbstractFloat: covariance of the artificial observation error\nα_reg::AbstractFloat: regularization parameter\nr::AbstractVector{FT} where FT<:AbstractFloat: regularization vector\nupdate_freq::Int64: update frequency\nimpose_prior::Bool: using augmented system (Tikhonov regularization with Kalman inversion in Chada     et al 2020 and Huang et al (2022)) to regularize the inverse problem, which also imposes prior     for posterior estimation.\nprior_mean::Any: prior mean - defaults to initial mean\nprior_cov::Any: prior covariance - defaults to initial covariance\niter::Int64: current iteration number\n\nConstructors\n\nUnscented(\n    u0_mean::AbstractVector{FT},\n    uu0_cov::AbstractMatrix{FT};\n    α_reg::FT = 1.0,\n    update_freq::IT = 0,\n    modified_unscented_transform::Bool = true,\n    impose_prior::Bool = false,\n    prior_mean::Any,\n    prior_cov::Any,\n    sigma_points::String = symmetric\n) where {FT <: AbstractFloat, IT <: Int}\n\nConstruct an Unscented Inversion Process.\n\nInputs:\n\nu0_mean: Mean at initialization.\nuu0_cov: Covariance at initialization.\nα_reg: Hyperparameter controlling regularization toward the prior mean (0 < α_reg ≤ 1),\n\ndefault should be 1, without regulariazion.\n\nupdate_freq: Set to 0 when the inverse problem is not identifiable, \n\nnamely the inverse problem has multiple solutions, the covariance matrix   will represent only the sensitivity of the parameters, instead of   posterior covariance information; set to 1 (or anything > 0) when   the inverse problem is identifiable, and the covariance matrix will   converge to a good approximation of the posterior covariance with an   uninformative prior.\n\nmodified_unscented_transform: Modification of the UKI quadrature given in Huang et al (2021).\nimpose_prior: using augmented system (Tikhonov regularization with Kalman inversion in Chada   et al 2020 and Huang et al (2022)) to regularize the inverse problem, which also imposes prior   for posterior estimation. If impose_prior == true, prior mean and prior cov must be provided.   This is recommended to use, especially when the number of observations is smaller than the number   of parameters (ill-posed inverse problems). When this is used, other regularizations are turned off  automatically.\nprior_mean: Prior mean used for regularization.\nprior_cov: Prior cov used for regularization.\nsigma_points: String of sigma point type, it can be symmetric with 2N_par+1   ensemble members or simplex with N_par+2 ensemble members.\n\nUnscented(\n    u_mean,\n    uu_cov,\n    obs_pred,\n    c_weights,\n    mean_weights,\n    cov_weights,\n    N_ens,\n    Σ_ω,\n    Σ_ν_scale,\n    α_reg,\n    r,\n    update_freq,\n    impose_prior,\n    prior_mean,\n    prior_cov,\n    iter\n)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/UnscentedKalmanInversion.jl:57.\n\nUnscented(\n    u0_mean,\n    uu0_cov;\n    α_reg,\n    update_freq,\n    modified_unscented_transform,\n    impose_prior,\n    prior_mean,\n    prior_cov,\n    sigma_points\n)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/UnscentedKalmanInversion.jl:91.\n\nUnscented(prior; kwargs...)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/UnscentedKalmanInversion.jl:210.\n\n\n\n\n\n","category":"type"},{"location":"API/Unscented/#EnsembleKalmanProcesses.construct_sigma_ensemble","page":"Unscented","title":"EnsembleKalmanProcesses.construct_sigma_ensemble","text":"construct_sigma_ensemble(\n    process::Unscented,\n    x_mean::Array{FT},\n    x_cov::AbstractMatrix{FT},\n) where {FT <: AbstractFloat, IT <: Int}\n\nConstruct the sigma ensemble based on the mean x_mean and covariance x_cov.\n\n\n\n\n\n","category":"function"},{"location":"API/Unscented/#EnsembleKalmanProcesses.construct_mean","page":"Unscented","title":"EnsembleKalmanProcesses.construct_mean","text":"construct_mean(\n    uki::EnsembleKalmanProcess{FT, IT, Unscented},\n    x::AbstractVecOrMat{FT};\n    mean_weights = uki.process.mean_weights,\n) where {FT <: AbstractFloat, IT <: Int}\n\nconstructs mean x_mean from an ensemble x.\n\n\n\n\n\n","category":"function"},{"location":"API/Unscented/#EnsembleKalmanProcesses.construct_cov","page":"Unscented","title":"EnsembleKalmanProcesses.construct_cov","text":"construct_cov(\n    uki::EnsembleKalmanProcess{FT, IT, Unscented},\n    x::AbstractVecOrMat{FT},\n    x_mean::Union{FT, AbstractVector{FT}, Nothing} = nothing;\n    cov_weights = uki.process.cov_weights,\n) where {FT <: AbstractFloat, IT <: Int}\n\nConstructs covariance xx_cov from ensemble x and mean x_mean.\n\n\n\n\n\nconstruct_cov(\n    uki::EnsembleKalmanProcess{FT, IT, Unscented},\n    x::AbstractMatrix{FT},\n    x_mean::AbstractVector{FT},\n    obs_mean::AbstractMatrix{FT},\n    y_mean::AbstractVector{FT};\n    cov_weights = uki.process.cov_weights,\n) where {FT <: AbstractFloat, IT <: Int, P <: Process}\n\nConstructs covariance xy_cov from ensemble x and mean x_mean, ensemble obs_mean and mean y_mean.\n\n\n\n\n\n","category":"function"},{"location":"API/Unscented/#EnsembleKalmanProcesses.update_ensemble_prediction!","page":"Unscented","title":"EnsembleKalmanProcesses.update_ensemble_prediction!","text":"update_ensemble_prediction!(process::Unscented, Δt::FT) where {FT <: AbstractFloat}\n\nUKI prediction step : generate sigma points.\n\n\n\n\n\n","category":"function"},{"location":"API/Unscented/#EnsembleKalmanProcesses.update_ensemble_analysis!","page":"Unscented","title":"EnsembleKalmanProcesses.update_ensemble_analysis!","text":"update_ensemble_analysis!(\n    uki::EnsembleKalmanProcess{FT<:Real, IT<:Int64, TU<:TransformUnscented},\n    u_p_full::AbstractMatrix,\n    g_full::AbstractMatrix,\n    u_idx::Vector{Int64},\n    g_idx::Vector{Int64}\n)\n\n\nUKI analysis step  : g is the predicted observations  Ny x N_ens matrix\n\n\n\n\n\nupdate_ensemble_analysis!(\n    uki::EnsembleKalmanProcess{FT, IT, Unscented},\n    u_p::AbstractMatrix{FT},\n    g::AbstractMatrix{FT},\n) where {FT <: AbstractFloat, IT <: Int}\n\nUKI analysis step  : g is the predicted observations  Ny x N_ens matrix\n\n\n\n\n\n","category":"function"},{"location":"API/Unscented/#EnsembleKalmanProcesses.construct_initial_ensemble-Union{Tuple{UorTU}, Tuple{EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution, UorTU}} where UorTU<:Union{TransformUnscented, Unscented}","page":"Unscented","title":"EnsembleKalmanProcesses.construct_initial_ensemble","text":"construct_initial_ensemble(\n    prior::EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution,\n    process::Union{TransformUnscented, Unscented};\n    constrained\n) -> Any\n\n\nConstructs the initial ensemble for the Unscented or TransformUnscented process.  Returned with parameters as columns in unconstrained space by default (constrain by settingconstrained=true`)\n\nNOTE: This function is created just to see what the initial sigma_ensemble will be without constructing the EKP object. Do not pass the initial ensemble into the EnsembleKalmanProcess object.\n\n\n\n\n\n","category":"method"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcess","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcess","text":"","category":"section"},{"location":"API/EnsembleKalmanProcess/#Primary-objects-and-functions","page":"EnsembleKalmanProcess","title":"Primary objects and functions","text":"","category":"section"},{"location":"API/EnsembleKalmanProcess/#Getter-functions","page":"EnsembleKalmanProcess","title":"Getter functions","text":"","category":"section"},{"location":"API/EnsembleKalmanProcess/#errors_api","page":"EnsembleKalmanProcess","title":"Error metrics","text":"","category":"section"},{"location":"API/EnsembleKalmanProcess/#scheduler_api","page":"EnsembleKalmanProcess","title":"Learning Rate Schedulers","text":"","category":"section"},{"location":"API/EnsembleKalmanProcess/#Failure-and-NaN-handling","page":"EnsembleKalmanProcess","title":"Failure and NaN handling","text":"","category":"section"},{"location":"API/EnsembleKalmanProcess/#process_api","page":"EnsembleKalmanProcess","title":"Process-specific","text":"","category":"section"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.EnsembleKalmanProcess","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.EnsembleKalmanProcess","text":"EnsembleKalmanProcess{FT <: AbstractFloat, IT <: Int, P <: Process}\n\nStructure that is used in Ensemble Kalman processes.\n\nFields\n\nu::Array{EnsembleKalmanProcesses.DataContainers.DataContainer{FT}} where FT<:AbstractFloat: array of stores for parameters (u), each of size [N_par × N_ens]\nobservation_series::ObservationSeries: Container for the observation(s) - and minibatching mechanism\nN_ens::Int64: ensemble size\ng::Array{EnsembleKalmanProcesses.DataContainers.DataContainer{FT}} where FT<:AbstractFloat: Array of stores for forward model outputs, each of size  [N_obs × N_ens]\nerror_metrics::Dict: Dict of error metric\nscheduler::EnsembleKalmanProcesses.LearningRateScheduler: Scheduler to calculate the timestep size in each EK iteration\naccelerator::EnsembleKalmanProcesses.Accelerator: accelerator object that informs EK update steps, stores additional state variables as needed\nΔt::Vector{FT} where FT<:AbstractFloat: stored vector of timesteps used in each EK iteration\nupdate_groups::AbstractVector: vector of update groups, defining which parameters should be updated by which data\nprocess::EnsembleKalmanProcesses.Process: the particular EK process (Inversion etc.)\nrng::Random.AbstractRNG: Random number generator object (algorithm + seed) used for sampling and noise, for reproducibility. Defaults to Random.GLOBAL_RNG.\nfailure_handler::FailureHandler: struct storing failsafe update directives\nlocalizer::EnsembleKalmanProcesses.Localizers.Localizer: Localization kernel, implemented for (Inversion, SparseInversion, Unscented)\nnan_tolerance::AbstractFloat: Fraction of allowable NaNs in model output before an ensemble member is considered a failed member and handled by failure handler\nnan_row_values::Union{Nothing, AbstractVector}: Default-value vector to impute over entire-NaN rows of data (get_obs(ekp) used if value nothing)\nverbose::Bool: Whether to print diagnostics for each EK iteration\n\nGeneric constructor\n\nEnsembleKalmanProcess(\n    params::AbstractMatrix{FT},\n    observation_series::OS,\n    obs_noise_cov::Union{AbstractMatrix{FT}, UniformScaling{FT}},\n    process::P;\n    scheduler = DefaultScheduler(1),\n    Δt = FT(1),\n    rng::AbstractRNG = Random.GLOBAL_RNG,\n    failure_handler_method::FM = IgnoreFailures(),\n    nan_tolerance = 0.1,\n    nan_row_values = nothing,\n    localization_method::LM = NoLocalization(),\n    verbose::Bool = false,\n) where {FT <: AbstractFloat, P <: Process, FM <: FailureHandlingMethod, LM <: LocalizationMethod, OS <: ObservationSeries}\n\nInputs:\n\nparams                 :: Initial parameter ensemble\nobservation_series     :: Container for observations (and possible minibatching)\nprocess                :: Algorithm used to evolve the ensemble\nscheduler              :: Adaptive timestep calculator \nΔt                     :: Initial time step or learning rate\nrng                    :: Random number generator\nfailure_handler_method :: Method used to handle particle failures\nnan_tolerance          :: Fraction of allowable NaNs in ensemble member before considered failure (0.1 by default)\nnan_row_values         :: Default-value vector to impute over entire-NaN rows of data (get_obs(ekp) used if value nothing)\nlocalization_method    :: Method used to localize sample covariances\nverbose                :: Whether to print diagnostic information\n\nOther constructors:\n\nEnsembleKalmanProcess(\n    u,\n    observation_series,\n    N_ens,\n    g,\n    error_metrics,\n    scheduler,\n    accelerator,\n    Δt,\n    update_groups,\n    process,\n    rng,\n    failure_handler,\n    localizer,\n    nan_tolerance,\n    nan_row_values,\n    verbose\n)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/EnsembleKalmanProcess.jl:209.\n\nEnsembleKalmanProcess(\n    params,\n    observation_series,\n    process,\n    configuration;\n    update_groups,\n    rng,\n    nan_tolerance,\n    nan_row_values,\n    verbose\n)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/EnsembleKalmanProcess.jl:244.\n\nEnsembleKalmanProcess(\n    params,\n    observation_series,\n    process;\n    scheduler,\n    accelerator,\n    failure_handler_method,\n    localization_method,\n    Δt,\n    update_groups,\n    rng,\n    nan_tolerance,\n    nan_row_values,\n    verbose\n)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/EnsembleKalmanProcess.jl:342.\n\nEnsembleKalmanProcess(params, observation, args; kwargs...)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/EnsembleKalmanProcess.jl:401.\n\nEnsembleKalmanProcess(\n    params,\n    obs,\n    obs_noise_cov,\n    args;\n    kwargs...\n)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/EnsembleKalmanProcess.jl:411.\n\nEnsembleKalmanProcess(\n    observation_series,\n    process;\n    kwargs...\n)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/UnscentedKalmanInversion.jl:256.\n\nEnsembleKalmanProcess(observation, process; kwargs...)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/UnscentedKalmanInversion.jl:267.\n\nEnsembleKalmanProcess(\n    obs_mean,\n    obs_noise_cov,\n    process;\n    kwargs...\n)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/UnscentedKalmanInversion.jl:278.\n\n\n\n\n\n","category":"type"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.construct_initial_ensemble","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.construct_initial_ensemble","text":"construct_initial_ensemble(\n    rng::Random.AbstractRNG,\n    prior::EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution,\n    N_ens::Int64;\n    constrained\n) -> Any\n\n\nConstruct the initial parameters, by sampling N_ens samples from specified prior distribution. Returned with parameters as columns in unconstrained space by default (constrain by setting constrained=true)\n\nNote: Unscented and TransformUnscented processes require different arguments than the other processes\n\n\n\n\n\nconstruct_initial_ensemble(\n    prior::EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution,\n    process::Union{TransformUnscented, Unscented};\n    constrained\n) -> Any\n\n\nConstructs the initial ensemble for the Unscented or TransformUnscented process.  Returned with parameters as columns in unconstrained space by default (constrain by settingconstrained=true`)\n\nNOTE: This function is created just to see what the initial sigma_ensemble will be without constructing the EKP object. Do not pass the initial ensemble into the EnsembleKalmanProcess object.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.update_ensemble!","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.update_ensemble!","text":"update_ensemble!(\n    ekp::EnsembleKalmanProcess,\n    g_in::AbstractMatrix;\n    multiplicative_inflation,\n    additive_inflation,\n    additive_inflation_cov,\n    s,\n    Δt_new,\n    ekp_kwargs...\n) -> Any\n\n\nUpdates the ensemble according to an Inversion process. Inputs:\n\nekp :: The EnsembleKalmanProcess to update.\ng :: Model outputs, they need to be stored as a N_obs × N_ens array (i.e data are columms).\nmultiplicative_inflation :: Flag indicating whether to use multiplicative inflation.\nadditive_inflation :: Flag indicating whether to use additive inflation.\nadditiveinflationcov ::  specifying an additive inflation matrix (default is the prior covariance) assumed positive semi-definite      If false (default), parameter covariance from the current iteration is used.\ns :: Scaling factor for time step in inflation step.\nekpkwargs :: Keyword arguments to pass to standard ekp updateensemble!.\n\n\n\n\n\nupdate_ensemble!(\n    ekp::EnsembleKalmanProcess{FT, IT, II<:Inversion},\n    g::AbstractArray{FT, 2},\n    process::Inversion,\n    u_idx::Vector{Int64},\n    g_idx::Vector{Int64};\n    deterministic_forward_map,\n    failed_ens,\n    kwargs...\n) -> Any\n\n\nUpdates the ensemble according to an Inversion process. \n\nInputs:\n\nekp :: The EnsembleKalmanProcess to update.\ng :: Model outputs, they need to be stored as a N_obs × N_ens array (i.e data are columms).\nprocess :: Type of the EKP.\nu_idx :: indices of u to update (see UpdateGroup)\ng_idx :: indices of g,y,Γ with which to update u (see UpdateGroup)\ndeterministic_forward_map :: Whether output g comes from a deterministic model.\nfailed_ens :: Indices of failed particles. If nothing, failures are computed as columns of g with NaN entries.\n\n\n\n\n\nupdate_ensemble!(\n    ekp::EnsembleKalmanProcess{FT, IT, TransformInversion},\n    g::AbstractMatrix{FT},\n    process::TransformInversion;\n    failed_ens = nothing,\n) where {FT, IT}\n\nUpdates the ensemble according to a TransformInversion process. \n\nInputs:\n\nekp :: The EnsembleKalmanProcess to update.\ng :: Model outputs, they need to be stored as a N_obs × N_ens array (i.e data are columms).\nprocess :: Type of the EKP.\nu_idx :: indices of u to update (see UpdateGroup)\ng_idx :: indices of g,y,Γ with which to update u (see UpdateGroup)\nfailed_ens :: Indices of failed particles. If nothing, failures are computed as columns of g with NaN entries.\n\n\n\n\n\nupdate_ensemble!(\n    ekp::EnsembleKalmanProcess{FT, IT, GaussNewtonInversion},\n    g::AbstractMatrix{FT},\n    process::GaussNewtonInversion;\n    deterministic_forward_map::Bool = true,\n    failed_ens = nothing,\n) where {FT, IT}\n\nUpdates the ensemble according to an GaussNewtonInversion process. The specific update is given by Algorithm 4.1 of Chada, Chen, Sanz-Alonso (2021), https://doi.org/10.3934/fods.2021011, a.k.a Iterated Ensemble Kalman Filter with Statistical Linearization (IEKF-SL)\n\nInputs:\n\nekp :: The EnsembleKalmanProcess to update.\ng :: Model outputs, they need to be stored as a N_obs × N_ens array (i.e data are columms).\nprocess :: Type of the EKP.\ndeterministicforwardmap :: Whether output g comes from a deterministic model.\nfailed_ens :: Indices of failed particles. If nothing, failures are computed as columns of g with NaN entries.\n\n\n\n\n\nupdate_ensemble!(\n    ekp::EnsembleKalmanProcess{FT, IT, SparseInversion{FT}},\n    g::AbstractMatrix{FT},\n    process::SparseInversion{FT};\n    deterministic_forward_map = true,\n    failed_ens = nothing,\n) where {FT, IT}\n\nUpdates the ensemble according to a SparseInversion process. \n\nInputs:\n\nekp :: The EnsembleKalmanProcess to update.\ng :: Model outputs, they need to be stored as a N_obs × N_ens array (i.e data are columms).\nprocess :: Type of the EKP.\nu_idx :: indices of u to update (see UpdateGroup)\ng_idx :: indices of g,y,Γ with which to update u (see UpdateGroup)\ndeterministic_forward_map :: Whether output g comes from a deterministic model.\nfailed_ens :: Indices of failed particles. If nothing, failures are computed as columns of g  with NaN entries.\n\n\n\n\n\nupdate_ensemble!(\n    ekp::EnsembleKalmanProcess{FT, IT, Sampler{FT,ST}},\n    g::AbstractMatrix{FT},\n    process::Sampler{FT, ST};\n    failed_ens = nothing,\n) where {FT, IT, ST}\n\nUpdates the ensemble according to a Sampler process. \n\nInputs:\n\nekp :: The EnsembleKalmanProcess to update.\ng :: Model outputs, they need to be stored as a N_obs × N_ens array (i.e data are columms).\nprocess :: Type of the EKP.\nu_idx :: indices of u to update (see UpdateGroup)\ng_idx :: indices of g,y,Γ with which to update u (see UpdateGroup)\nfailed_ens :: Indices of failed particles. If nothing, failures are computed as columns of g  with NaN entries.\n\n\n\n\n\nupdate_ensemble!(\n    uki::EnsembleKalmanProcess{FT<:AbstractFloat, IT<:Int64, TU<:TransformUnscented},\n    g::AbstractArray{FT<:AbstractFloat, 2},\n    process::TransformUnscented,\n    u_idx::Vector{Int64},\n    g_idx::Vector{Int64};\n    group_idx,\n    failed_ens,\n    kwargs...\n) -> Any\n\n\nUpdates the ensemble according to an TransformUnscented process. \n\nInputs:\n\nuki        :: The EnsembleKalmanProcess to update.\ng_in       :: Model outputs, they need to be stored as a N_obs × N_ens array (i.e data are columms).\nprocess :: Type of the EKP.\nu_idx :: indices of u to update (see UpdateGroup)\ng_idx :: indices of g,y,Γ with which to update u (see UpdateGroup)\ngroup_idx :: the label of the update group (1 is \"first update this iteration\")\nfailed_ens :: Indices of failed particles. If nothing, failures are computed as columns of g  with NaN entries.\n\n\n\n\n\nupdate_ensemble!(\n    uki::EnsembleKalmanProcess{FT, IT, Unscented},\n    g_in::AbstractMatrix{FT},\n    process::Unscented;\n    failed_ens = nothing,\n) where {FT <: AbstractFloat, IT <: Int}\n\nUpdates the ensemble according to an Unscented process. \n\nInputs:\n\nuki        :: The EnsembleKalmanProcess to update.\ng_in       :: Model outputs, they need to be stored as a N_obs × N_ens array (i.e data are columms).\nprocess :: Type of the EKP.\nu_idx :: indices of u to update (see UpdateGroup)\ng_idx :: indices of g,y,Γ with which to update u (see UpdateGroup)\ngroup_idx :: the label of the update group (1 is \"first update this iteration\")\nfailed_ens :: Indices of failed particles. If nothing, failures are computed as columns of g  with NaN entries.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_u","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_u","text":"get_u(ekp::EnsembleKalmanProcess, iteration::IT; return_array=true) where {IT <: Integer}\n\nReturns the unconstrained parameters at the given iteration. Returns a DataContainer object unless return_array is true.\n\n\n\n\n\nget_u(ekp::EnsembleKalmanProcess; return_array=true)\n\nReturns the unconstrained parameters from all iterations. The outer dimension is given by the number of iterations, and the inner objects are DataContainer objects unless return_array is true.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_g","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_g","text":"get_g(ekp::EnsembleKalmanProcess, iteration::IT; return_array=true) where {IT <: Integer}\n\nReturns the forward model evaluations at the given iteration. Returns a DataContainer object unless return_array is true.\n\n\n\n\n\nget_g(ekp::EnsembleKalmanProcess; return_array=true)\n\nReturns the forward model evaluations from all iterations. The outer dimension is given by the number of iterations, and the inner objects are DataContainer objects unless return_array is true.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_ϕ","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_ϕ","text":"get_ϕ(prior::ParameterDistribution, ekp::EnsembleKalmanProcess, iteration::IT; return_array=true)\n\nReturns the constrained parameters at the given iteration.\n\n\n\n\n\nget_ϕ(prior::ParameterDistribution, ekp::EnsembleKalmanProcess; return_array=true)\n\nReturns the constrained parameters from all iterations. The outer dimension is given by the number of iterations, and the inner objects are DataContainer objects unless return_array is true.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_obs-Tuple{EnsembleKalmanProcess}","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_obs","text":"get_obs(ekp::EnsembleKalmanProcess; build) -> Any\n\n\nGet the observation from the current batch in ObservationSeries build=false: returns a vector of vectors, build=true: returns a concatenated vector,\n\n\n\n\n\n","category":"method"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_obs-Tuple{EnsembleKalmanProcess, Any}","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_obs","text":"get_obs(ekp::EnsembleKalmanProcess, iteration; build) -> Any\n\n\nGet the observation for iteration from the ObservationSeries build=false: returns a vector of vectors, build=true: returns a concatenated vector,\n\n\n\n\n\n","category":"method"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_obs_noise_cov-Tuple{EnsembleKalmanProcess}","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_obs_noise_cov","text":"get_obs_noise_cov(ekp::EnsembleKalmanProcess; build) -> Any\n\n\nGet the obs_noise_cov from the current batch in ObservationSeries build=false:, returns a vector of blocks, build=true: returns a block matrix,\n\n\n\n\n\n","category":"method"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_obs_noise_cov-Tuple{EnsembleKalmanProcess, Any}","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_obs_noise_cov","text":"get_obs_noise_cov(\n    ekp::EnsembleKalmanProcess,\n    iteration;\n    build\n) -> Any\n\n\nGet the obs_noise_cov for iteration from the ObservationSeries build=false:, returns a vector of blocks, build=true: returns a block matrix,\n\n\n\n\n\n","category":"method"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_obs_noise_cov_inv-Tuple{EnsembleKalmanProcess}","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_obs_noise_cov_inv","text":"get_obs_noise_cov_inv(\n    ekp::EnsembleKalmanProcess;\n    build\n) -> Any\n\n\nGet the obs_noise_cov inverse from the current batch in ObservationSeries build=false:, returns a vector of blocks, build=true: returns a block matrix,\n\n\n\n\n\n","category":"method"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_obs_noise_cov_inv-Tuple{EnsembleKalmanProcess, Any}","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_obs_noise_cov_inv","text":"get_obs_noise_cov_inv(\n    ekp::EnsembleKalmanProcess,\n    iteration;\n    build\n) -> Any\n\n\nGet the obsnoisecov inverse for iteration from the ObservationSeries build=false:, returns a vector of blocks, build=true: returns a block matrix,\n\n\n\n\n\n","category":"method"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_u_final","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_u_final","text":"get_u_final(ekp::EnsembleKalmanProcess; return_array=true)\n\nGet the unconstrained parameters at the last iteration, returning a DataContainer Object if return_array is false.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_g_final","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_g_final","text":"get_g_final(ekp::EnsembleKalmanProcess; return_array=true)\n\nGet forward model outputs at the last iteration, returns a DataContainer Object if return_array is false.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_ϕ_final","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_ϕ_final","text":"get_ϕ_final(ekp::EnsembleKalmanProcess; return_array=true)\n\nGet the constrained parameters at the last iteration.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_u_mean","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_u_mean","text":"get_u_mean(ekp::EnsembleKalmanProcess, iteration::IT) where {IT <: Integer}\n\nReturns the mean unconstrained parameter at the given iteration.\n\n\n\n\n\nget_u_mean(uki::EnsembleKalmanProcess{FT, IT, Unscented}, iteration::IT)\n\nReturns the mean unconstrained parameter at the requested iteration.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_u_cov","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_u_cov","text":"get_u_cov(ekp::EnsembleKalmanProcess, iteration::IT) where {IT <: Integer}\n\nReturns the unconstrained parameter sample covariance at the given iteration.\n\n\n\n\n\nget_u_cov(uki::EnsembleKalmanProcess{FT, IT, Unscented}, iteration::IT)\n\nReturns the unconstrained parameter covariance at the requested iteration.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_g_mean","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_g_mean","text":"get_g_mean(ekp::EnsembleKalmanProcess, iteration::IT) where {IT <: Integer}\n\nReturns the mean forward map evaluation at the given iteration.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_ϕ_mean","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_ϕ_mean","text":"get_ϕ_mean(prior::ParameterDistribution, ekp::EnsembleKalmanProcess, iteration::IT)\n\nReturns the constrained transform of the mean unconstrained parameter at the given iteration.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_u_mean_final","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_u_mean_final","text":"get_u_mean_final(ekp::EnsembleKalmanProcess)\n\nGet the mean unconstrained parameter at the last iteration.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_u_cov_final","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_u_cov_final","text":"get_u_cov_final(ekp::EnsembleKalmanProcess)\n\nGet the mean unconstrained parameter covariance at the last iteration.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_g_mean_final","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_g_mean_final","text":"get_g_mean_final(ekp::EnsembleKalmanProcess)\n\nGet the mean forward model evaluation at the last iteration.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_ϕ_mean_final","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_ϕ_mean_final","text":"get_ϕ_mean_final(prior::ParameterDistribution, ekp::EnsembleKalmanProcess)\n\nGet the constrained transform of the mean unconstrained parameter at the last iteration.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_N_iterations","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_N_iterations","text":"get_N_iterations(ekp::EnsembleKalmanProcess)\n\nGet number of times update has been called (equals size(g), or size(u)-1).\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_N_ens","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_N_ens","text":"get_N_ens(ekp::EnsembleKalmanProcess)\n\nReturn N_ens field of EnsembleKalmanProcess.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_accelerator","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_accelerator","text":"get_accelerator(ekp::EnsembleKalmanProcess)\n\nReturn accelerator field of EnsembleKalmanProcess.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_scheduler","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_scheduler","text":"get_scheduler(ekp::EnsembleKalmanProcess)\n\nReturn scheduler field of EnsembleKalmanProcess.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_process","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_process","text":"get_process(ekp::EnsembleKalmanProcess)\n\nReturn process field of EnsembleKalmanProcess.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_rng","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_rng","text":"get_rng(m::FixedMinibatcher) -> Random.AbstractRNG\n\n\ngets the rng field from the FixedMinibatcher object\n\n\n\n\n\nget_rng(m::RandomFixedSizeMinibatcher) -> Random.AbstractRNG\n\n\ngets the rng field from the RandomFixesSizeMinibatcher object\n\n\n\n\n\nget_rng(ekp::EnsembleKalmanProcess)\n\nReturn rng field of EnsembleKalmanProcess.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_Δt","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_Δt","text":"get_Δt(ekp::EnsembleKalmanProcess)\n\nReturn Δt field of EnsembleKalmanProcess. \n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_failure_handler","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_failure_handler","text":"get_failure_handler(ekp::EnsembleKalmanProcess)\n\nReturn failure_handler field of EnsembleKalmanProcess.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_localizer","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_localizer","text":"get_localizer(ekp::EnsembleKalmanProcess)\n\nReturn localizer field of EnsembleKalmanProcess.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_localizer_type","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_localizer_type","text":"get_localizer_type(ekp::EnsembleKalmanProcess)\n\nReturn first parametric type of the localizer field of EnsembleKalmanProcess.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_nan_tolerance","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_nan_tolerance","text":"get_nan_tolerance(ekp::EnsembleKalmanProcess)\n\nReturn nan_tolerance field of EnsembleKalmanProcess.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_nan_row_values","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_nan_row_values","text":"get_nan_row_values(ekp::EnsembleKalmanProcess)\n\nReturn nan_row_values field of EnsembleKalmanProcess.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.list_update_groups_over_minibatch","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.list_update_groups_over_minibatch","text":"list_update_groups_over_minibatch(ekp::EnsembleKalmanProcess)\n\nReturn ugroups and ggroups for the current minibatch, i.e. the subset of \n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.compute_average_rmse","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.compute_average_rmse","text":"compute_average_rmse(ekp::EnsembleKalmanProcess) -> Any\n\n\nComputes the average covariance-weighted error over the forward model ensemble, normalized by the dimension: mean(sqrt( 1/dim(y) * tr((g_i - y)' *  Γ⁻¹ *  (g_i - y)))). The error is retrievable as get_error_metrics(ekp)[\"avg_rmse\"]\n\n\n\n\n\nFor Unscented processes it doesn't make sense to average RMSE at sigma points, so it is evaluated at the mean only, where it is exactly equal to sqrt(compute_loss_at_mean(uki))\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.compute_loss_at_mean","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.compute_loss_at_mean","text":"compute_loss_at_mean(ekp::EnsembleKalmanProcess) -> Any\n\n\nComputes the covariance-weighted error of the mean of the forward model output, normalized by the dimension 1/dim(y) * (ḡ - y)' * Γ⁻¹ * (ḡ - y). The error is retrievable as get_error_metrics(ekp)[\"loss\"] or returned from get_error(ekp) if a prior is not provided to the process\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.compute_average_unweighted_rmse","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.compute_average_unweighted_rmse","text":"compute_average_unweighted_rmse(\n    ekp::EnsembleKalmanProcess\n) -> Any\n\n\nComputes the average unweighted error over the forward model ensemble, normalized by the dimension: mean(sqrt( 1/dim(y) * tr((g_i - y)' * (g_i - y)))). The error is retrievable as get_error_metrics(ekp)[\"unweighted_avg_rmse\"]\n\n\n\n\n\nFor Unscented processes it doesn't make sense to average unweighted RMSE at sigma points, so it is evaluated at the mean only, where it is exactly equal to sqrt(compute_unweighted_loss_at_mean(uki))\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.compute_unweighted_loss_at_mean","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.compute_unweighted_loss_at_mean","text":"compute_unweighted_loss_at_mean(\n    ekp::EnsembleKalmanProcess\n) -> Any\n\n\nComputes the unweighted error of the mean of the forward model output, normalized by the dimension 1/dim(y) * (ḡ - y)' * (ḡ - y). The error is retrievable as get_error_metrics(ekp)[\"unweighted_loss\"]\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.compute_bayes_loss_at_mean","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.compute_bayes_loss_at_mean","text":"compute_bayes_loss_at_mean(\n    ekp::EnsembleKalmanProcess\n) -> Any\n\n\nComputes the bayes loss of the mean of the forward model output, normalized by dimensions (1/dim(y)*dim(u)) * [(ḡ - y)' * Γ⁻¹ * (ḡ - y) + (̄u - m)' * C⁻¹ * (̄u - m)]. If the prior is not provided to the process on creation of EKP, then m and C are estimated from the initial ensemble.\n\nThe error is retrievable as get_error_metrics(ekp)[\"bayes_loss\"] or returned from get_error(ekp) if a prior is provided to the process\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.compute_crps","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.compute_crps","text":"compute_crps(ekp::EnsembleKalmanProcess) -> Any\n\n\nComputes a Gaussian approximation of CRPS (continuous rank probability score) of the ensemble with the observation (performing through a whitening by C^GG, see e.g., Zheng, Sun, 2025, https://arxiv.org/abs/2410.09133).\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.compute_error!","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.compute_error!","text":"compute_error!(ekp::EnsembleKalmanProcess) -> Any\n\n\nComputes a variety of error metrics and stores this in EnsembleKalmanProcess. (retrievable with get_error_metrics(ekp)) currently available:\n\navg_rmse           computed with compute_average_rmse(ekp)\nloss               computed with compute_loss_at_mean(ekp)\nunweighed_avg_rmse computed with compute_average_unweighted_rmse(ekp)\nunweighted_loss    computed with compute_unweighted_loss_at_mean(ekp)\nbayes_loss         computed with compute_bayes_loss_at_mean(ekp)\ncrps               computed with compute_crps(ekp)\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_error_metrics","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_error_metrics","text":"get_error_metrics(ekp::EnsembleKalmanProcess)\n\nReturns the stored error_metrics, created with compute_error!\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_error","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_error","text":"get_error(ekp::EnsembleKalmanProcess)\n\n[For back compatability] Returns the relevant loss function that is minimized over EKP iterations.\n\nIf prior provided to the process:     get_error_metrics(ekp)[\"bayes_loss\"], the loss computed with compute_bayes_loss_at_mean(ekp)\nIf prior not provided to the process: get_error_metrics(ekp)[\"loss\"], the loss computed with compute_loss_at_mean(ekp)\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.lmul_obs_noise_cov","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.lmul_obs_noise_cov","text":"lmul_obs_noise_cov(\n    ekp::EnsembleKalmanProcess,\n    X::AbstractVecOrMat\n) -> Any\n\n\nConvenience function to multiply X by obs_noise_cov on the left without building the full matrix.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.lmul_obs_noise_cov_inv","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.lmul_obs_noise_cov_inv","text":"lmul_obs_noise_cov_inv(\n    ekp::EnsembleKalmanProcess,\n    X::AbstractVecOrMat\n) -> Any\n\n\nConvenience function to multiply X by obs_noise_cov_inv on the left without building the full matrix.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.lmul_obs_noise_cov!","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.lmul_obs_noise_cov!","text":"lmul_obs_noise_cov!(\n    out::AbstractMatrix,\n    ekp::EnsembleKalmanProcess,\n    X::AbstractVecOrMat,\n    idx_set::AbstractVector\n)\n\n\nConvenience function to multiply X by obs_noise_cov on the left without building the full matrix, and storing the result in the first argument out, and at a set of indices idx_triple = [(block_idx, local_idx, global_idx), ...]. Here, for each triple the multiplication will: \n\nselect block block_idx from the unbuilt obs_noise_cov_inv\nselect local indices [:,local_idx] of this block\nmultiply with the corresponding global indices X[global_idx,:]\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.lmul_obs_noise_cov_inv!","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.lmul_obs_noise_cov_inv!","text":"lmul_obs_noise_cov_inv!(\n    out::AbstractMatrix,\n    ekp::EnsembleKalmanProcess,\n    X::AbstractVecOrMat,\n    idx_set::AbstractVector\n)\n\n\nConvenience function to multiply X by obs_noise_cov_inv on the left without building the full matrix, and storing the result in the first argument out, and at a set of indices idx_triple = [(block_idx, local_idx, global_idx), ...]. Here, for each triple the multiplication will: \n\nselect block block_idx from the unbuilt obs_noise_cov_inv\nselect local indices [:,local_idx] of this block\nmultiply with the corresponding global indices X[global_idx,:]\n\nThe primary use case is in update_ensemble! for TransformInversion()\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.DefaultScheduler","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.DefaultScheduler","text":"struct DefaultScheduler{FT} <: EnsembleKalmanProcesses.LearningRateScheduler\n\nScheduler containing a default constant step size, users can override this temporarily within update_ensemble!.\n\nΔt_default::Any: step size\n\n\n\n\n\n","category":"type"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.MutableScheduler","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.MutableScheduler","text":"struct MutableScheduler{FT} <: EnsembleKalmanProcesses.LearningRateScheduler\n\nScheduler containing a mutable constant step size, users can override this permanently within update_ensemble!.\n\nΔt_mutable::Vector: mutable step size\n\n\n\n\n\n","category":"type"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.EKSStableScheduler","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.EKSStableScheduler","text":"struct EKSStableScheduler{FT} <: EnsembleKalmanProcesses.LearningRateScheduler\n\nScheduler known to be stable for EKS, In particular, Delta t = fracalphaU + varepsilon where U = (G(u) - barG(u))^TGamma^-1(G(u) - y).  Cannot be overriden.\n\nnumerator::Any: the numerator alpha\nnugget::Any: the nugget term varepsilon\n\n\n\n\n\n","category":"type"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.DataMisfitController","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.DataMisfitController","text":"struct DataMisfitController{FT, S} <: EnsembleKalmanProcesses.LearningRateScheduler\n\nScheduler from Iglesias, Yang, 2021, Based on Bayesian Tempering. Terminates at T=1 by default, and at this time, ensemble spread provides a (more) meaningful approximation of posterior uncertainty In particular, for parameters theta_j at step n, to calculate the next timestep Delta t_n = minleft(maxleft(fracJ2Phi sqrtfracJ2langle Phi Phi rangleright) 1-sum^n-1_i t_iright) where Phi_j = Gamma^-frac12(G(theta_j) - y)^2.  Cannot be overriden by user provided timesteps. By default termination returns true from update_ensemble! and \n\nif on_terminate == \"stop\", stops further iteration.\nif on_terminate == \"continue_fixed\", continues iteration with the final timestep fixed\nif on_terminate == \"continue\", continues the algorithm (though no longer compares to 1-sum^n-1_i t_i) \n\nThe user may also change the T with terminate_at keyword.\n\niteration::Vector{Int64}: the current iteration\nterminate_at::Any: the algorithm time for termination, default: 1.0\non_terminate::Any: the action on termination, default: \"stop\",\n\n\n\n\n\n","category":"type"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.calculate_timestep!","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.calculate_timestep!","text":"calculate_timestep!(\n    ekp::EnsembleKalmanProcess,\n    g::AbstractMatrix,\n    Δt_new::Union{Nothing, AbstractFloat}\n) -> Any\n\n\nCalculates next timestep by pushing to ekp.Δt,  !isnothing(return_value) implies termination condition has been met\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.FailureHandler","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.FailureHandler","text":"FailureHandler{P <: Process, FM <: FailureHandlingMethod}\n\nStructure defining the failure handler method used in the EnsembleKalmanProcess.\n\nFields\n\nfailsafe_update::Function: Failsafe algorithmic update equation\n\nConstructors\n\nFailureHandler(process, method)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/EnsembleKalmanInversion.jl:82.\n\nFailureHandler(process, method)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/EnsembleKalmanInversion.jl:95.\n\nFailureHandler(process, method)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/EnsembleKalmanSampler.jl:62.\n\nFailureHandler(process, method)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/EnsembleTransformKalmanInversion.jl:95.\n\nFailureHandler(process, method)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/EnsembleTransformKalmanInversion.jl:107.\n\nFailureHandler(process, method)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/GaussNewtonKalmanInversion.jl:28.\n\nFailureHandler(process, method)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/GaussNewtonKalmanInversion.jl:41.\n\nFailureHandler(process, method)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/SparseEnsembleKalmanInversion.jl:44.\n\nFailureHandler(process, method)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/SparseEnsembleKalmanInversion.jl:56.\n\nFailureHandler(process, method)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/UnscentedKalmanInversion.jl:316.\n\nFailureHandler(process, method)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/UnscentedKalmanInversion.jl:335.\n\nFailureHandler(process, method)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/UnscentedTransformKalmanInversion.jl:102.\n\nFailureHandler(process, method)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/UnscentedTransformKalmanInversion.jl:121.\n\n\n\n\n\n","category":"type"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.SampleSuccGauss","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.SampleSuccGauss","text":"SampleSuccGauss <: FailureHandlingMethod\n\nFailure handling method that substitutes failed ensemble members by new samples from the empirical Gaussian distribution defined by the updated successful ensemble.\n\n\n\n\n\n","category":"type"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.IgnoreFailures","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.IgnoreFailures","text":"Failure handling method that ignores forward model failures\n\n\n\n\n\n","category":"type"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.sample_empirical_gaussian","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.sample_empirical_gaussian","text":"sample_empirical_gaussian(\n    rng::AbstractRNG,\n    u::AbstractMatrix{FT},\n    n::IT;\n    inflation::Union{FT, Nothing} = nothing,\n) where {FT <: Real, IT <: Int}\n\nReturns n samples from an empirical Gaussian based on point estimates u, adding inflation if the covariance is singular.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.split_indices_by_success","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.split_indices_by_success","text":" split_indices_by_success(g::AbstractMatrix{FT}) where {FT <: Real}\n\nReturns the successful/failed particle indices given a matrix with output vectors stored as columns. Failures are defined for particles containing at least one NaN output element.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.impute_over_nans","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.impute_over_nans","text":"impute_over_nans(\n    g::AbstractMatrix,\n    nan_tolerance,\n    nan_row_values::AbstractVector;\n    verbose\n) -> Any\n\n\nImputation of \"reasonable values\" over NaNs in the following manner\n\nDetect failures: check if any column contains NaNs exceeding the fraction nan_tolerance, such members are flagged as failures\nImpute values in rows with few NaNs: Of the admissible columns, any NaNs are replaced by finite values of the ensemble-mean (without NaNs) over the row. \nImpute a value for row with all NaNs: Of the admissible columns, the value of the observation itself y is imputed\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_prior_mean","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_prior_mean","text":"get_prior_mean(\n    process::Inversion\n) -> Union{Nothing, AbstractVector}\n\n\nReturns the stored prior_mean from the Inversion process \n\n\n\n\n\nget_prior_mean(\n    process::TransformInversion\n) -> Union{Nothing, AbstractVector}\n\n\nReturns the stored prior_mean from the TransformInversion process \n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_prior_cov","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_prior_cov","text":"get_prior_cov(\n    process::Inversion\n) -> Union{Nothing, LinearAlgebra.UniformScaling, AbstractMatrix}\n\n\nReturns the stored prior_cov from the Inversion process \n\n\n\n\n\nget_prior_cov(\n    process::TransformInversion\n) -> Union{Nothing, LinearAlgebra.UniformScaling, AbstractMatrix}\n\n\nReturns the stored prior_cov from the TransformInversion process \n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_impose_prior","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_impose_prior","text":"get_impose_prior(process::Inversion) -> Bool\n\n\nReturns the stored impose_prior from the Inversion process \n\n\n\n\n\nget_impose_prior(process::TransformInversion) -> Bool\n\n\nReturns the stored impose_prior from the TransformInversion process \n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_buffer","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_buffer","text":"get_buffer(p::TransformInversion) -> AbstractVector\n\n\nReturns the stored buffer from the TransformInversion process \n\n\n\n\n\nget_buffer(p::TransformUnscented) -> AbstractVector\n\n\nReturns the stored buffer from the TransformUnscented process \n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_default_multiplicative_inflation","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_default_multiplicative_inflation","text":"get_default_multiplicative_inflation(\n    process::Inversion\n) -> AbstractFloat\n\n\nReturns the stored default_multiplicative_inflation from the Inversion process \n\n\n\n\n\nget_default_multiplicative_inflation(\n    p::TransformInversion\n) -> AbstractFloat\n\n\nReturns the stored default_multiplicative_inflation from the TransformInversion process \n\n\n\n\n\n","category":"function"},{"location":"defaults/#defaults","page":"List of default configurations","title":"Default Configurations","text":"","category":"section"},{"location":"defaults/#ens-size","page":"List of default configurations","title":"Recommended Ensemble Size","text":"The ensemble size is generally not determinable in advance. However there are several rules of thumb for calibrating a parameter vector theta that can be used as a starting point.\n\nParameter dimension Ensemble size\nmathrmdim(theta)leq 10 N_mathrmens geq 10 cdot mathrmdim(theta)\n10 leq mathrmdim(theta)leq 100 N_mathrmens = 100\n100leq mathrmdim(theta) N_mathrmens = 100 and SEC\n\nnote: for the `Unscented` process\nUKI and UTKI always create an ensemble size proportional to mathrmdim(theta). It is not configurable by the user, and is retrievable from an EnsembleKalmanProcess object ekp using get_N_ens(ekp).","category":"section"},{"location":"defaults/#Quick-links!","page":"List of default configurations","title":"Quick links!","text":"What does scheduler = ... do? See here.\nWhat does localization_method = ... do? See here. and our example\nWhat does failure_handler_method = ... do? See here\nWhat does accelerator = ... do? See here, and our examples","category":"section"},{"location":"defaults/#Prebuilt-defaults","page":"List of default configurations","title":"Prebuilt defaults","text":"Defaults have been chosen for the methodology based on prior experience. The configurations can be revealed by adding the keyword\n\nEnsembleKalmanProcess(..., verbose = true)\n\nTo use the defaults, one constructs an Ensemble Kalman Process with\n\nEnsembleKalmanProcess(initial_parameters, observation, process)\n\nand the following configurations (listed below) will be automatically created depending on the process type chosen, they are listed as keyword arguments that will be automatically added into EnsembleKalmanProcess() on creation. \n\ninfo: Recommended process\nFor the simplest and most flexible update we recommend the Inversion() process. EnsembleKalmanProcess(initial_parameters, observation, Inversion())\n\nPlease see the relevant documentation pages for each configurable if you wish to modify them.","category":"section"},{"location":"defaults/#process-:-Inversion","page":"List of default configurations","title":"process <: Inversion","text":"Process documentation here\n\nscheduler = DataMisfitController(terminate_at = 1)\nlocalization_method = Localizers.SECNice()\nfailure_handler_method = SampleSuccGauss()\naccelerator = NesterovAccelerator()","category":"section"},{"location":"defaults/#process-:-TransformInversion","page":"List of default configurations","title":"process <: TransformInversion","text":"Process documentation here\n\nscheduler = DataMisfitController(terminate_at = 1)\nlocalization_method = Localizers.NoLocalization()\nfailure_handler_method = SampleSuccGauss()\naccelerator = DefaultAccelerator()","category":"section"},{"location":"defaults/#process-:-SparseInversion","page":"List of default configurations","title":"process <: SparseInversion","text":"Process documentation here\n\nscheduler = DefaultScheduler()\nlocalization_method = Localizers.SECNice()\nfailure_handler_method = SampleSuccGauss()\naccelerator = DefaultAccelerator()","category":"section"},{"location":"defaults/#process-:-GaussNewtonInversion","page":"List of default configurations","title":"process <: GaussNewtonInversion","text":"Process documentation here\n\nscheduler = DataMisfitController(terminate_at = 1)\nlocalization_method = Localizers.SECNice()\nfailure_handler_method = SampleSuccGauss()\naccelerator = NesterovAccelerator()","category":"section"},{"location":"defaults/#process-:-Sampler","page":"List of default configurations","title":"process <: Sampler","text":"Process documentation here\n\nscheduler = EKSStableScheduler(1.0, eps())\nlocalization_method = Localizers.NoLocalization()\nfailure_handler_method = IgnoreFailures()\naccelerator = DefaultAccelerator()","category":"section"},{"location":"defaults/#process-:-Unscented","page":"List of default configurations","title":"process <: Unscented","text":"Process documentation here\n\nscheduler = DataMisfitController(terminate_at = 1)\nlocalization_method = Localizers.NoLocalization()\nfailure_handler_method = SampleSuccGauss()\naccelerator = DefaultAccelerator()","category":"section"},{"location":"defaults/#process-:-TransformUnscented","page":"List of default configurations","title":"process <: TransformUnscented","text":"Process documentation here\n\nscheduler = DataMisfitController(terminate_at = 1)\nlocalization_method = Localizers.NoLocalization()\nfailure_handler_method = SampleSuccGauss()\naccelerator = DefaultAccelerator()","category":"section"},{"location":"defaults/#\"Vanilla\"-settings:-How-to-turn-off-features","page":"List of default configurations","title":"\"Vanilla\" settings: How to turn off features","text":"As the defaults now implement recent features. The following snippet shows how one can use keyword arguments to construct an EKP with no additional features or variants.\n\nEnsembleKalmanProcess(\n    initial_parameters,\n    observation,\n    process,\n    scheduler = DefaultScheduler(1), # constant timestep size 1\n    localization_method = Localizers.NoLocalization(), # no localization\n    failure_handler_method = IgnoreFailures(), # no failure handling\n    accelerator = DefaultAccelerator(), # no acceleration\n)\n\nnote: Note\nYou will need to call the Localizers module viausing EnsembleKalmanProcesses.Localizersto get the localization_method structures","category":"section"},{"location":"accelerators/#accelerators","page":"Accelerators","title":"Accelerators","text":"We provide options to accelerate the convergence of some optimizer variants with accelerators. These accelerators have been adapted from gradient-based methods in order to accelerate the following ensemble Kalman processes:\n\nEnsemble Kalman inversion (EKI), Inversion()\nEnsemble transform Kalman inversion (ETKI), TransformInversion()\nUnscented Kalman inversion (UKI), Unscented() (experimental, may result in instability)\nEnsemble Kalman sampler (EKS), Sampler() (experimental, may result in instability)\n\nFurther theoretical details and experiments of the accelerators, please see (Vernon, Bach, Dunbar, 2025)","category":"section"},{"location":"accelerators/#Using-Accelerators","page":"Accelerators","title":"Using Accelerators","text":"An EKI struct can be created with acceleration as follows:\n\n# assume we have defined an initial ensemble, a vector of observations y, and observational noise covariance Γ\n\nekiobj_accelerated = EKP.EnsembleKalmanProcess(\n            initial_ensemble,\n            y,\n            Γ,\n            Inversion();\n            accelerator = NesterovAccelerator()\n        )\n\nThe rest of the process is the same as without the accelerator.\n\nVery similarly, one can create an ETKI struct with acceleration:\n\netkiobj_accelerated = EKP.EnsembleKalmanProcess(\n            initial_ensemble,\n            y,\n            Γ,\n            TransformInversion();\n            accelerator = NesterovAccelerator()\n        )\n\nAgain, the rest of the process is the same as without the accelerator.","category":"section"},{"location":"accelerators/#Experiments-on-EKI-and-ETKI","page":"Accelerators","title":"Experiments on EKI and ETKI","text":"Accelerators have been found to accelerate EKI convergence on a number of example inverse problems. In this \"exponential sine\" inverse problem, we look to solve for amplitude and vertical shift parameters of an underlying sine wave. The model is defined as expleft(theta_1 sin(phi+t) + theta_2right), with unknown parameters theta = theta_1 theta_2. Observations (model output) consist of the difference between maximum and minimum, and the mean, of the evaluated model. We define theta_texttrue = left1 08right.\n\nThe NesterovAccelerator() (shown in blue) has been found to produce the most consistent acceleration on this problem, as seen below. The FirstOrderNesterovAccelerator() (shown in red) uses a momentum coefficient very similar to that of the NesterovAccelerator(), and enjoys similar performance. The ConstantNesterovAccelerator(0.9) (shown in green) is effective in this test case, but can be very unstable. These methods differ only in their momentum coefficient values, which are plotted on the right. Vanilla EKI is shown in black. The experiment is repeated 50 times; ribbons denote one standard error from the mean.\n\n(Image: Momentum coefficients) (Image: Momentum coefficient values)\n\nBelow is an example of accelerated ETKI convergence on the same problem, using the NesterovAccelerator().\n\n(Image: etki momentum) (Image: uki momentum)","category":"section"},{"location":"accelerators/#Background-and-Implementation","page":"Accelerators","title":"Background & Implementation","text":"","category":"section"},{"location":"accelerators/#Nesterov-Acceleration-for-Gradient-Descent","page":"Accelerators","title":"Nesterov Acceleration for Gradient Descent","text":"In traditional gradient descent, one iteratively solves for x^*, the minimizer of a function f(x), by performing the update step \n\nx_k+1 = x_k + alpha  nabla f(x_k) \n\nwhere alpha is a step size parameter. In 1983, Nesterov acceleration (also known as Nesterov momentum) was introduced to accelerate gradient descent. In the modified algorithm, the update step becomes \n\nx_k+1 = x_k + beta (x_k - x_k-1) + alpha  nabla f(x_k + beta (x_k - x_k-1)) \n\nwhere beta is a momentum coefficient. Intuitively, the method mimics a ball gaining speed while rolling down a constantly-sloped hill.","category":"section"},{"location":"accelerators/#Implementation-in-EKI-Algorithm","page":"Accelerators","title":"Implementation in EKI Algorithm","text":"The exact implementation, theory, and experiments for this work are available at (Vernon, Bach, Dunbar, 2025).\n\nNesterov acceleration can be used to accelerate gradient flows, as shown by (Su et al 2016). EKI, a gradient-free method, can be understood as approximating a form of gradient flow (Calvello et al 2023)). Additionally, work by (Kovachki and Stuart 2021) demonstrated success when using a modified particle-based Nesterov acceleration method. This work inspired the following implementation of accelerators for a variety of EKP processes.\n\nThe traditional update step for EKI is as follows, with j = 1  J denoting the ensemble member and k denoting iteration number.\n\nu_k+1^j = u_k^j + C_k^umathcalG (frac1Delta tGamma + C^mathcalGmathcalG_k)^-1 left(y - mathcalG(u_k^j)right)\n\nWhen using accelerators, this update step is modified to include a term reminiscent of that in Nesterov's acceleration method for gradient descent.\n\nWe first compute intermediate values:\n\nv_k^j = u_k^j+ beta_k (u_k^j - u_k-1^j)\n\nWe then update the ensemble:\n\nu_k+1^j = v_k^j + C_k^umathcalG (frac1Delta tGamma + C^mathcalGmathcalG_k)^-1 left(y - mathcalG(v_k^j)right)\n\nThe momentum coefficient beta_k here differs for different accelerators.\n\nIn the NesterovAccelerator(), we recursively compute beta_k as beta_k = theta_k(theta_k-1^-1-1) in the  as derived in (Su et al). This implementation has generally been found to be the most effective in most test cases.\nIn the FirstOrderNesterovAccelerator(), we compute beta_k = 1-3k^-1. This is the coefficient originally used by Nesterov.\nThe ConstantNesterovAccelerator() uses a specified constant coefficient, with the default being beta_k = 09.","category":"section"},{"location":"failure_handling/#failures","page":"Failure handling","title":"Handling forward model failures","text":"In situations where the forward model mathcalG represents a diagnostic of a complex computational model, there might be cases where for some parameter combinations theta, attempting to evaluate mathcalG(theta) may result in model failure. Some examples could be\n\nA member is numerically unstable, or throws error exceptions\nA member exceeds a user-defined wall-clock time for computation\nA member's outputs produce (full or partial) values that are user-defined as \"failure\"\nPossible corruption of data or compute nodes during HPC model evaluation\n\nIn such cases, this package offers the option for users to replace entries in the evaluation with NaN values, and then updates will continue with the following procedures\n\nImputation: If only partial output of an ensemble member is NaN, then values may be redeemed with values derived from other ensemble members, or from user-defined values. Imputed members will be considered successful.\nFailure handling: The update equations are then modified to handle/replace/resample failed members. Crucially, this is done in a way that does not break parallelism (i.e., no re-running of failed members)\n\nEnsembleKalmanProcesses.jl implements such modifications through the FailureHandler structure, an input to the EnsembleKalmanProcess constructor. Currently, the only failsafe modification available is SampleSuccGauss(), described in Lopez-Gomez et al (2022).\n\nwarning: These are last-resort handlers\nThese modifications are not a magic bullet. If large fractions of ensemble members fail during an iteration, this will degenerate the span of the ensemble, as one may not be able to replace lost information.","category":"section"},{"location":"failure_handling/#Implementation","page":"Failure handling","title":"Implementation","text":"When available, the failure handling and imputation is active by default, see here for the different options. Alternatively, one can use the failure_handler_method keyword\n\nusing EnsembleKalmanProcesses\nusing EnsembleKalmanProcesses.ParameterDistributions\n\n\n# for e.g., EKI\nJ = 50  # number of ensemble members\ninitial_ensemble = construct_initial_ensemble(prior, J) # Initialize ensemble from prior\nekiobj = EnsembleKalmanProcess(\n    initial_ensemble,\n    y,\n    obs_noise_cov,\n    Inversion(),\n    failure_handler_method = SampleSuccGauss())\n\n# for e.g., UKI\nukiobj = EnsembleKalmanProcess(\n    y\n    obs_noise_cov,\n    Unscented(prior),\n    failure_handler_method = SampleSuccGauss())","category":"section"},{"location":"failure_handling/#When-has-a-particle-\"failed\"?","page":"Failure handling","title":"When has a particle \"failed\"?","text":"","category":"section"},{"location":"failure_handling/#Full-particle-failure","page":"Failure handling","title":"Full particle failure","text":"The user determines if a ensemble member has failed and replace the output (column) of mathcalG(theta) with NaNs. The FailureHandler takes care of the rest.","category":"section"},{"location":"failure_handling/#Partial-particle-failure-(a.k.a-Imputation)","page":"Failure handling","title":"Partial particle failure (a.k.a Imputation)","text":"If there is a partial failure within an output then the user sets only failed entries within a particle to NaN. The user can then use two keywords to determine the imputation approach, given to EnsembleKalmanProcess.\n\nnan_tolerance: If the number of NaNs in any observation is below a given nan_tolerance (default 10%) then the particle will still be considered successful (not handled by the FailureHandler) and the algorithm will impute NaNs using information from other ensemble members or user-defined values.\nnan_rows_value: if the same entry fails for all ensemble members, then imputing is not possible without additional information; here the user may supply a vector with the dimension of the output and this value will be imputed for all members in this row.","category":"section"},{"location":"failure_handling/#Example-of-Imputation-and-Failure-Handling","page":"Failure handling","title":"Example of Imputation and Failure Handling","text":"For example imagine we have a 7-dimensional output and 4 ensemble members. The latest run produces\n\ng = \n7×4 Matrix{Float64}:\nNaN           0.992373   NaN           -0.200025\n-1.37133     0.0527899  NaN           -1.15243\nNaN         NaN          NaN          NaN\nNaN         NaN          NaN          NaN\n-2.28799   NaN            0.614214     1.54217\n0.582582    0.0744582   -1.37219      0.253971\n0.620447    0.441834     0.0641471    0.490588\n\nNow, given the user-defined settings in EnsembleKalmanProcess(...)\n\nnan_tolerance = 0.5         # failure if > 50% NaN\nnan_rows_value = collect(1:7) # replace failed row k with value k\n\nThe imputation will produce:\n\n┌ Warning: In forward map ensemble g, detected 12 NaNs. \n│ Given nan_tolerance = 0.5 to determine failed members: \n│ - Ensemble members failed:       1 \n│ - NaNs in successful members:    8 \n│ - rows index set for imputation: [1, 3, 4, 5] \n│ - rows index entirely NaN:       [3, 4]\n[ Info: Imputed 8 NaNs\n7×4 Matrix{Float64}:\n0.396174   0.992373   NaN          -0.200025     # `g[1, 1]`        -> `mean(g[1, [2, 4] ])`\n-1.37133    0.0527899  NaN          -1.15243\n3.0        3.0        NaN           3.0          # `g[3, [1, 2, 4]]`-> `3.0`\n4.0        4.0        NaN           4.0          # `g[4, [1, 2, 4]]`-> `4.0`\n-2.28799   -0.0438687    0.614214    1.54217     # `g[5, 2]`        -> `mean(g[5, [1, 3, 4])`\n0.582582   0.0744582   -1.37219     0.253971\n0.620447   0.441834     0.0641471   0.490588\n\nHere,\n\nA warning is given to describe the context\nmember 3 was deemed a failure(>50% NaN) and will be replaced with the failure handler, SampleSuccGauss during EKP update\nmembers 1, 2, and 4 were deemed successful, and so are imputed with values for the EKP update.","category":"section"},{"location":"failure_handling/#[SampleSuccGauss()-for-EKI]-(@id-failure-eki)","page":"Failure handling","title":"[SampleSuccGauss() for EKI] (@id failure-eki)","text":"The SampleSuccGauss() modification is based on updating all ensemble members with a distribution given by only the successful parameter ensemble. Let Theta_sn= theta^(1)_sndotstheta^(J_s)_sn be the successful ensemble, for which each evaluation mathcalG(theta^(j)_sn) does not fail, and let theta_fn^(k) be the ensemble members for which the evaluation mathcalG(theta^(k)_fn) fails. The successful ensemble Theta_sn is updated to Theta_sn+1 using expression (2), and each failed ensemble member as\n\n    theta_fn+1^(k) sim mathcalN left(m_s n+1 Sigma_s n+1 right)\n\nwhere\n\n    m_s n+1 = dfrac1J_ssum_j=1^J_s theta_sn+1^(j) qquad Sigma_s n+1 = mathrmCov(theta_s n+1 theta_s n+1) + kappa_*^-1mu_s1I_p\n\nHere, kappa_* is a limiting condition number, mu_s1 is the largest eigenvalue of the sample covariance mathrmCov(theta_s n+1 theta_s n+1) and I_p is the identity matrix of size ptimes p.","category":"section"},{"location":"failure_handling/#[SampleSuccGauss()-for-UKI]-(@id-failure-uki)","page":"Failure handling","title":"[SampleSuccGauss() for UKI] (@id failure-uki)","text":"The SampleSuccGauss() modification is based on performing the UKI quadratures over the successful sigma points. Consider the set of off-center sigma points hattheta = hattheta_s cup hattheta_f where hattheta_s^(j),  j=1 dots J_s are successful members and hattheta_f^(k) are not. For ease of notation, consider an ordering of hattheta such that hattheta_s are its first J_s elements, and note that we deal with the central point hattheta^(0) separately. We estimate the covariances mathrmCov_q(mathcalG_n mathcalG_n) and mathrmCov_q(theta_n mathcalG_n) from the successful ensemble,\n\n   tag1 mathrmCov_q(theta_n mathcalG_n) approx sum_j=1^J_sw_sj (hattheta_s n^(j) - bartheta_sn)(mathcalG(hattheta_s n^(j)) - barmathcalG_sn)^T\n\n   tag2 mathrmCov_q(mathcalG_n mathcalG_n) approx sum_j=1^J_sw_sj (mathcalG(hattheta_s n^(j)) - barmathcalG_sn)(mathcalG(hattheta_s n^(j)) - barmathcalG_sn)^T\n\nwhere the weights at each successful sigma point are scaled up, to preserve the sum of weights,\n\n    w_sj = left(dfracsum_i=1^2p w_isum_k=1^J_s w_kright)w_j\n\nIn equations (1) and (2), the means bartheta_sn and barmathcalG_sn must be modified from the original formulation if the central point hattheta^(0)=m_n results in model failure. If this is the case, then an average is taken across the other (successful) ensemble members\n\n   bartheta_sn =\ndfrac1J_ssum_j=1^J_shattheta_s n^(j) qquad   barmathcalG_sn =\ndfrac1J_ssum_j=1^J_smathcalG(hattheta_s n^(j))","category":"section"},{"location":"contributing/#Contributing","page":"Contributing","title":"Contributing","text":"Thank you for considering contributing to EnsembleKalmanProcesses! We encourage opening issues and pull requests (PRs).","category":"section"},{"location":"contributing/#What-to-contribute?","page":"Contributing","title":"What to contribute?","text":"The easiest way to contribute is by using EnsembleKalmanProcesses, identifying problems and opening issues;\nYou can try to tackle an existing issue. It is best to outline your proposed solution in the issue thread before implementing it in a PR;\nWrite an example or tutorial. It is likely that other users may find your use of EnsembleKalmanProcesses insightful;\nImprove documentation or comments if you found something hard to use;\nImplement a new feature if you need it. We strongly encourage opening an issue to make sure the administrators are on board before opening a PR with an unsolicited feature addition.","category":"section"},{"location":"contributing/#Using-git","page":"Contributing","title":"Using git","text":"If you are unfamiliar with git and version control, the following guides will be helpful:\n\nAtlassian (bitbucket) git tutorials. A set of tips and tricks for getting started with git.\nGitHub's git tutorials. A set of resources from GitHub to learn git.","category":"section"},{"location":"contributing/#Forks-and-branches","page":"Contributing","title":"Forks and branches","text":"Create your own fork of EnsembleKalmanProcesses on GitHub and check out your copy:\n\n$ git clone https://github.com/<your-username>/EnsembleKalmanProcesses.jl.git\n$ cd EnsembleKalmanProcesses.jl\n\nNow you have access to your fork of EnsembleKalmanProcesses through origin. Create a branch for your feature; this will hold your contribution:\n\n$ git checkout -b <branchname>","category":"section"},{"location":"contributing/#Some-useful-tips","page":"Contributing","title":"Some useful tips","text":"When you start working on a new feature branch, make sure you start from main by running: git checkout main and git pull.\nCreate a new branch from main by using git checkout -b <branchname>.","category":"section"},{"location":"contributing/#Develop-your-feature","page":"Contributing","title":"Develop your feature","text":"Make sure you add tests for your code in test/ and appropriate documentation in the code and/or in docs/. Before committing your changes, you can verify their behavior by running the tests, the examples, and building the documentation locally. In addition, make sure your feature follows the formatting guidelines by running\n\njulia --project=.dev .dev/climaformat.jl .\n\nfrom the EnsembleKalmanProcesses.jl directory.","category":"section"},{"location":"contributing/#Squash-and-rebase","page":"Contributing","title":"Squash and rebase","text":"When your PR is ready for review, clean up your commit history by squashing and make sure your code is current with EnsembleKalmanProcesses.jl main by rebasing. The general rule is that a PR should contain a single commit with a descriptive message.\n\nTo make sure you are up to date with main, you can use the following workflow:\n\n$ git checkout main\n$ git pull\n$ git checkout <name_of_local_branch>\n$ git rebase main\n\nThis may create conflicts with the local branch. The conflicted files will be outlined by git. To resolve conflicts, we have to manually edit the files (e.g. with vim). The conflicts will appear between >>>>, ===== and <<<<<. We need to delete these lines and pick what version we want to keep.\n\nTo squash your commits, you can use the following command:\n\n$ git rebase -i HEAD~n\n\nwhere n is the number of commits you need to squash into one. Then, follow the instructions in the terminal. For example, to squash 4 commits:\n\n$ git rebase -i HEAD~4\n\nwill open the following file in (typically) vim:\n\n   pick 01d1124 <commit message 1>\n   pick 6340aaa <commit message 2>\n   pick ebfd367 <commit message 3>\n   pick 30e0ccb <commit message 4>\n\n   # Rebase 60709da..30e0ccb onto 60709da\n   #\n   # Commands:\n   #  p, pick = use commit\n   #  e, edit = use commit, but stop for amending\n   #  s, squash = use commit, but meld into previous commit\n   #\n   # If you remove a line here THAT COMMIT WILL BE LOST.\n   # However, if you remove everything, the rebase will be aborted.\n##\n\nWe want to keep the first commit and squash the last 3. We do so by changing the last three commits to squash and then do :wq on vim.\n\n   pick 01d1124 <commit message 1>\n   squash 6340aaa <commit message 2>\n   squash ebfd367 <commit message 3>\n   squash 30e0ccb <commit message 4>\n\n   # Rebase 60709da..30e0ccb onto 60709da\n   #\n   # Commands:\n   #  p, pick = use commit\n   #  e, edit = use commit, but stop for amending\n   #  s, squash = use commit, but meld into previous commit\n   #\n   # If you remove a line here THAT COMMIT WILL BE LOST.\n   # However, if you remove everything, the rebase will be aborted.\n\nThen in the next screen that appears, we can just delete all messages that we do not want to show in the commit. After this is done and we are back to  the console, we have to force push. We need to force push because we rewrote the local commit history.\n\n$ git push -uf origin <name_of_local_branch>\n\nYou can find more information about squashing here.","category":"section"},{"location":"contributing/#Unit-testing","page":"Contributing","title":"Unit testing","text":"Currently a number of checks are run per commit for a given PR.\n\nJuliaFormatter checks if the PR is formatted with .dev/climaformat.jl.\nDocumentation rebuilds the documentation for the PR and checks if the docs are consistent and generate valid output.\nUnit Tests run subsets of the unit tests defined in tests/, using Pkg.test(). The tests are run in parallel to ensure that they finish in a reasonable time. The tests only run the latest commit for a PR, branch and will kill any stale jobs on push. These tests are only run on linux (Ubuntu LTS).\n\nUnit tests are run against every new commit for a given PR, the status of the unit-tests are not checked during the merge process but act as a sanity check for developers and reviewers. Depending on the content changed in the PR, some CI checks that are not necessary will be skipped.  For example doc only changes do not require the unit tests to be run.","category":"section"},{"location":"contributing/#The-merge-process","page":"Contributing","title":"The merge process","text":"If you're a collaborator and have the necessary permissions, and if you have both approved code-review and the (necessary) integration tests passing, then you may merge the pull-request into main. Our preferred method is the to click the Squash and Merge button set as default on the pull request.","category":"section"},{"location":"observations/#observations","page":"Observations and Minibatching","title":"Observations","text":"The Observations object facilitates convenient storing, grouping and minibatching over observations.","category":"section"},{"location":"observations/#The-key-objects","page":"Observations and Minibatching","title":"The key objects","text":"The Observation is a container for an observed variables (\"samples\"), their noise covariances (\"covariances\"), and names (\"names\"). They are easily stackable to help build larger heterogenous observations\nThe Minibatcher facilitate data streaming (minibatching), where a user can submit large group of observations, that are then batched up and looped over in epochs.\nThe ObservationSeries contains the list of Observations and Minibatcher and the utilities to get the current batch etc.\n\nnote: I usually just pass in a vector of data and a covariance to EKP\nUsers can indeed set up an experiment with just one data sample and covariance matrix for the noise. However internally these are still stored as an ObservationSeries with a special minibatcher that does nothing (created by no_minibatcher(size)).\n\nnote: How should I provide the noise covariance?\nWe provide some utilities and API for providing other forms of covariance than AbstractMatrix. For example, in high-dimensional problems one may wish to provide compact low-rank representations. See the section below for more details.","category":"section"},{"location":"observations/#Recommended-constructor:-A-single-(stacked)-observation","page":"Observations and Minibatching","title":"Recommended constructor: A single (stacked) observation","text":"Here the user has data for two independent variables: the five-dimensional y and the eight-dimensional z. The observational noise of y is uncorrelated in all components, while the observations of z there is a known correlation.\n\nWe recommend users build an Observation using the Dict constructor and make use of the combine_observations() utility.\n\nusing EnsembleKalmanProcesses # for `Observation`\nusing LinearAlgebra # for `I`, `Tridiagonal`\n\n\n# specify an observation of y with diagonal noise covariance\nydim = 5\ny = ones(ydim)\ncov_y = 0.01*I\n\n# specify an observation of z with tridiagonal noise covariance\nzdim = 8\nz = zeros(zdim)\ncov_z = Tridiagonal(0.1*ones(zdim-1), ones(zdim), 0.1*ones(zdim-1))\n\ny_obs = Observation(\n    Dict(\n        \"samples\" => y,\n        \"covariances\" => cov_y,\n        \"names\" => \"y\",\n        \"metadata\" => \"optional metadata information in any format\",\n    ),\n)\n\nz_obs = Observation(\n    Dict(\n        \"samples\" => z,\n        \"covariances\" => cov_z,\n        \"names\" => \"z\",\n        \"metadata\" => \"optional metadata information in any format\",\n    ),\n)\n\nfull_obs = combine_observations([y_obs,z_obs]) # conveniently stacks the observations\n\n# getting information out\nget_obs(full_obs) # returns [y,z]\n\nget_obs_noise_cov(full_obs) # returns block-diagonal matrix with blocks [cov_y 0; 0 cov_z]\n\ngetters get_* can be used for all internals,\n\nget_names(full_obs) # returns [\"y\", \"z\"]\n\nThere are some other fields stored such as indices of the y and z components\n\nget_indices(full_obs) # returns [1:ydim, ydim+1:ydim+zdim]\n\nget_metadata(full_obs) # combined metadata","category":"section"},{"location":"observations/#Recommended-constructor:-Many-stacked-observations","page":"Observations and Minibatching","title":"Recommended constructor: Many stacked observations","text":"Imagine the user has 100 independent data samples for two independent variables above Rather than stacking all the data together at once (forming a full system of size 100*(8+5) to update at each step) instead the user wishes to stream the data and do updates with random batches of 5 observations at each iteration.\n\nnote: Why would I choose to minibatch?\nThe memory- and time-scaling of many EKP methods is worse-than-linear in the observation dimension, therefore there is often large computational benefit to minibatch EKP updates. Such costs must be weighed against the cost of additional forward map evaluations needed to minibatching over one full epoch. \n\n# given a vector of 100 `Observation`s called hundred_full_obs,\nusing EnsembleKalmanProcesses # for `RandomFixedSizeMinibatcher`, `ObservationSeries`, `Minibatcher`\n\nminibatcher = RandomFixedSizeMinibatcher(5) # batches the epoch of size 100, into batches of size 5\n\nobservation_series = ObservationSeries(\n    Dict(\n        \"observations\" => hundred_full_obs,\n        \"minibatcher\" => minibatcher,\n        \"metadata\" => \"optional metadata information in any format\",\n    ),\n)\n\nget_metadata(observation_series) # returns metadata\n\n# some example methods to get information out at the current minibatch:\nget_current_minibatch(observation_series) # returns [i₁, ..., i₅],  the current minibatch subset of indices 1:100\n\nget_obs(observation_series) # returns [yi₁, zi₁, ..., yi₅, zi₅], the data sample for the current minibatch\n\nget_obs_noise_cov(observation_series) # returns block-diagonal matrix with blocks [cov_yi₁  0 ... 0 ; 0 cov_zi₁ 0 ... 0; ... ; 0 ... 0 cov_yi₅ 0; 0 ... 0 cov_zi₅]\n\nminibatches are updated internally to the update_ensemble!(ekp,...) step via a call to\n\nupdate_minibatch!(observation_series)\nget_current_minibatch(observation_series)","category":"section"},{"location":"observations/#Minibatchers","page":"Observations and Minibatching","title":"Minibatchers","text":"Minibatchers are modular and must be derived from the Minibatcher abstract type. They contain a method create_new_epoch!(minibatcher,args...;kwargs) that creates a sampling of an epoch of data. For example, if we have 100 data observations, the epoch is 1:100, and one possible minibatching is a random partitioning of 1:100 into a batch-size (e.g., 5) leading to 20 minibatches.  \n\nSome of the implemented Minibatchers\n\nFixedMinibatcher(given_batches, \"order\"), (default method = \"order\"), minibatches are fixed and run through in order for all epochs\nFixedMinibatcher(given_batches, \"random\"), minibatches are fixed, but are run through in a randomly chosen order in each epoch\nno_minibatcher(size), creates a FixedMinibatcher with just one batch which is the epoch (effectively no minibatching)\nRandomFixedSizeMinibatcher(minibatch_size, \"trim\"), (default method = \"trim\") creates minibatches of size minibatch_size by randomly sampling the epoch, if the minibatch size does not divide into the number of samples it will ignore the remainder (and thus preserving a constant batch size)\nRandomFixedSizeMinibatcher(minibatch_size, \"extend\"), creates minibatches of size minibatch_size by randomly sampling the epoch, if the minibatch size does not divide into the number of samples it will include the remainder in the final batch (and thus will cover the entirety of the data, with a larger final batch)","category":"section"},{"location":"observations/#Identifiers","page":"Observations and Minibatching","title":"Identifiers","text":"One can additionally provide a vector of names to name each Observation in the ObservationSeries by giving using the Dict entry \"names\" => names.\n\nTo think about the differences between the identifiers for Observation and ObservationSeries consider an application of observing the average state of a dynamical system over 100 time windows. The time windows will be batched over during the calibration.\n\nThe compiled information is given in the object:\n\nyz_observation_series::ObservationSeries\n\nAs this contains many time windows, setting the names of the ObservationSeries objects to index the time window is a sensible identifier, for example,\n\nget_names(yz_observation_series)\n> [\"window_1\", \"window_2\", ..., \"window_100\"]\n\nThe individual Observations should refer only to the state being measured, so suitable identifiers might be, for example,\n\nobs = get_observations(yz_observation_series)[1] # get first observation in the series\nget_names(obs)\n> [\"y_window_average\", \"z_window_average\"]","category":"section"},{"location":"observations/#building-covariances","page":"Observations and Minibatching","title":"Building (scalable!) observational noise covariance","text":"For most low-dimensional problems (e.g. dim < 5000), the user can simply provide a UniformScaling, or an AbstractMatrix as they are most familiar, in conjunction with any EKP process.\n\nFor high-dimensional problems, the user must first select an output-scalable  process. For example, TransformInversion(...) (ETKI) or TransformUnscented(...) (UTKI)\n\nNext the user must select a scalable storage for the noise covariance matrix in observations as the operational cost of storing and updating very large (non-diagonal) AbstractMatrix objects is prohibitive.\n\nTherefore in high-dimensions we recommend the following scalable options:\n\nFor a diagonal covariance: Diagonal or UniformScaling\nFor a low-rank covariance: SVD\nFor a sum of low-rank and diagonal covariance: SVDplusD \n\nThe framework is extensible to new types as they arise (so long as one can define an efficient implementation of left multiplication and inverse of the struct)","category":"section"},{"location":"observations/#Example-of-building-scalable-high-dimensional-ObservationSeries","page":"Observations and Minibatching","title":"Example of building scalable high-dimensional ObservationSeries","text":"The following example demonstrates our utilities tsvd_cov_from_samples to quickly build such forms from available samples.\n\nImagine a problem where the observation dimension is size 10^6, and we have 30 noisy samples of such data from repeated runs of an experiment. We also believe that there may also be some additional 5% noise from model error when fitting our parameters to data, as our model is also not perfect. In case the model error and samples are zero anywhere, we also add a small regularization to the diagonal of size 10^-6.\n\nLet's build some observations!\n\nusing EnsembleKalmanProcesses \nusing LinearAlgebra\nusing Statistics\n\n# \"data\"\nn_trials = 30\noutput_dim = 1_000_000\nY = randn(output_dim, n_trials);\n\n# the noise estimated from the samples (will have rank n_trials-1)\ninternal_cov = tsvd_cov_from_samples(Y); # SVD object\n\n# the \"5%\" model error (diagonal)\nmodel_error_frac = 0.05\ndata_mean = vec(mean(Y,dims=2));\nmodel_error_cov = Diagonal((model_error_frac*data_mean).^2);\n\n# regularize the model error diagonal (in case of zero entries)\nmodel_error_cov += 1e-6*I\n\n# Combine...\ncovariance = SVDplusD(internal_cov, model_error_cov);\n\nY_obs_vec = [];\nfor k = 1:n_trials\n    push!(Y_obs_vec, Observation(\n            Dict(\n                \"samples\" => Y[:,k],\n                \"covariances\" => covariance,\n                \"names\" => \"experiment_$k\",\n            ),\n        ),  \n    )\nend \n\nLet's assume that we then wish to update this with specific batches of size 2, in order. Let's build an ObservationSeries!\n\nb_size = 2;\ngiven_batches = [collect(((i - 1) * b_size + 1):(i * b_size)) for i in 1:n_trials];\n\nminibatcher = FixedMinibatcher(given_batches);\nobservation_series = ObservationSeries(Y_obs_vec, minibatcher);\n\nThis can be passed into a scalable EKI (with some prior)\n\nusing EnsembleKalmanProcesses.ParameterDistributions\nprior = constrained_gaussian(\"example_params\", 3, 2, 0, Inf, repeats=3) ;\n\nutki = EnsembleKalmanProcess(observation_series, TransformUnscented(prior));\n\nwarning: Warning\nAlways extract the current observational noise from ekp with get_obs_noise_cov(ekp, build=false). Using build=true (default) will cause memory issues in this case as it will try to build the compactly-stored matrix.\n\nSome tips:\n\nWe always recommend adding some regularization (via adding a Diagonal or UniformScaling) to low-rank covariances.\nOne can further reduce the rank by providing it as a second example tsvd_cov_from_samples(samples, rank).\nDetails of the SVDplusD API, and other methods for creating tsvd objects can be found in the API docs.","category":"section"},{"location":"ensemble_kalman_sampler/#eks","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampling","text":"","category":"section"},{"location":"ensemble_kalman_sampler/#What-Is-It-and-What-Does-It-Do?","page":"Ensemble Kalman Sampler","title":"What Is It and What Does It Do?","text":"The Ensemble Kalman Sampler (EKS) (Garbuno-Inigo et al, 2020, Cleary et al, 2020, and it's variant Affine-invariant interacting Langevin Dynamics (ALDI) Garbuno-Inigo et al, 2020) are derivative-free tools for approximate Bayesian inference. They does so by approximately sampling from the posterior distribution. That is, EKS provides both point estimation (through the mean of the final ensemble) and uncertainty quantification (through the covariance of the final ensemble), this is in contrast to EKI, which only provides point estimation. \n\nThe EKS algorithm, viewed affine invariant system of interacting particles (Garbuno-Inigo et al, 2020) and ALDI differs from EKS by a finite-sample correction is introduced to overcome its computational finite-sample implementation. Both of these variants are provided through the Sampler process in our the toolbox - by default we construct this improved ALDI variant.\n\nWhile there are noisy variants of the standard EKI, EKS differs from them in its noise structure (as its noise is added in parameter space, not in  data space), and its update rule explicitly accounts for the prior (rather than having it enter through initialization).  The approximatiom of the posterior through EKS typically needs more iterations than EKI to converge to a suitable solution, and to help we provide an adaptive learning rate scheduler EKSStableScheduler() and a semi-implicit formulation to help maintain a stable interacting particle system. However, the posterior approximation through EKS is obtained with far less computational effort than a typical Markov Chain Monte Carlo (MCMC) like Metropolis-Hastings, though it will provide Gaussian-like uncertainty.","category":"section"},{"location":"ensemble_kalman_sampler/#Problem-Formulation","page":"Ensemble Kalman Sampler","title":"Problem Formulation","text":"The data y and parameter vector theta are assumed to be related according to:\n\n    y = mathcalG(theta) + eta \n\nwhere mathcalG  mathbbR^p rightarrow mathbbR^d denotes the forward map, y in mathbbR^d is the vector of observations, and eta is the observational noise, which is assumed to be drawn from a d-dimensional Gaussian with distribution mathcalN(0 Gamma_y). The objective of the inverse problem is to compute the unknown parameters theta given the observations y, the known forward map mathcalG, and noise characteristics eta of the process.\n\nnote: Note\nTo obtain Bayesian characterization for the posterior from EKS, the user must specify a Gaussian prior distribution. See Prior distributions to see how one can apply flexible constraints while maintaining Gaussian priors. ","category":"section"},{"location":"ensemble_kalman_sampler/#Ensemble-Kalman-Sampling-Algorithm","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampling Algorithm","text":"The EKS is based on the following update equation for the parameter vector theta^(j)_n of ensemble member j at the n-iteration:\n\nbeginaligned\ntheta_n+1^(* j) = theta_n^(j) - dfracDelta t_nJsum_k=1^Jlangle mathcalG(theta_n^(k)) - barmathcalG_n Gamma_y^-1(mathcalG(theta_n^(j)) - y) rangle theta_n^(k) + fracd+1J left(theta_n^(j) - bar theta_n right) - Delta t_n mathsfC(Theta_n) Gamma_theta^-1 theta_n + 1^(* j)  \ntheta_n + 1^j = theta_n+1^(* j) + sqrt2 Delta t_n mathsfC(Theta_n) xi_n^j \nendaligned\n\nwhere the subscript n=1 dots N_textit indicates the iteration, J is the ensemble size (i.e., the number of particles in the ensemble), Delta t_n is an internal adaptive time step (thus no need for the user to specify), Gamma_theta is the prior covariance, and xi_n^(j) sim mathcalN(0 mathrmI_p). barmathcalG_n is the ensemble mean of the forward map mathcalG(theta),\n\nbarmathcalG_n = dfrac1Jsum_k=1^JmathcalG(theta_n^(k))\n\nThe p times p matrix mathsfC(Theta_n), where Theta_n = lefttheta^(j)_nright_j=1^J is the set of all ensemble particles in the n-th iteration, denotes the empirical covariance between particles\n\nmathsfC(Theta_n) = frac1J sum_k=1^J (theta^(k)_n - bartheta_n) otimes (theta^(k)_n - bartheta_n)\n\nwhere bartheta_n is the ensemble mean of the particles,\n\nbartheta_n = dfrac1Jsum_k=1^Jtheta^(k)_n ","category":"section"},{"location":"ensemble_kalman_sampler/#Constructing-the-Forward-Map","page":"Ensemble Kalman Sampler","title":"Constructing the Forward Map","text":"At the core of the forward map mathcalG is the dynamical model PsimathbbR^p rightarrow mathbbR^o (running Psi is usually where the computational heavy-lifting is done), but the map mathcalG may include additional components such as a transformation of the (unbounded) parameters theta to a constrained domain the dynamical model can work with, or some post-processing of the output of Psi to generate the observations. For example, mathcalG may take the following form:\n\nmathcalG = mathcalH circ Psi circ mathcalT^-1\n\nwhere mathcalHmathbbR^o rightarrow mathbbR^d is the observation map and mathcalT is the transformation from the constrained to the unconstrained parameter space, such that mathcalT(phi)=theta. A family of standard transformations and their inverses are available in the ParameterDistributions module.","category":"section"},{"location":"ensemble_kalman_sampler/#How-to-Construct-an-Ensemble-Kalman-Sampler","page":"Ensemble Kalman Sampler","title":"How to Construct an Ensemble Kalman Sampler","text":"An EKS object can be created using the EnsembleKalmanProcess constructor by specifying the Sampler type. The constructor takes an argument, the prior. The following example shows how an EKS object is instantiated. An observation (y) and the covariance of the observational noise (obs_cov) are assumed to be defined previously in the code.\n\nusing EnsembleKalmanProcesses # for `construct_initial_ensemble`,`EnsembleKalmanProcess`\nusing EnsembleKalmanProcesses.ParameterDistributions  # for `ParameterDistribution`\n\n# Construct prior (see `ParameterDistributions.jl` docs)\nprior = ParameterDistribution(...)\n\n\n# Construct initial ensemble\nN_ens = 20  # ensemble size\ninitial_ensemble = construct_initial_ensemble(prior, N_ens)\n\n# Construct ensemble Kalman process\neksobj = EnsembleKalmanProcess(initial_ensemble, y, obs_noise_cov, Sampler(prior))\n\nOne can also build the original EKS variant (not ALDI), with the sampler_type keyword \n\nSampler(prior, sampler_type=\"eks\")","category":"section"},{"location":"ensemble_kalman_sampler/#Updating-the-ensemble","page":"Ensemble Kalman Sampler","title":"Updating the ensemble","text":"Once the EKS object eksobj has been initialized, the initial ensemble of particles is iteratively updated by the update_ensemble! function, which takes as arguments the eksobj and the evaluations of the forward model at each member of the current ensemble. In the following example, the forward map G maps a parameter to the corresponding data – this is done for each parameter in the ensemble, such that the resulting g_ens is of size d x N_ens. The update_ensemble! function then stores the updated ensemble as well as the evaluations of the forward map in eksobj.\n\nA typical use of the update_ensemble! function given the EKS object eksobj, the dynamical model Ψ, and the observation map H (the latter two are assumed to be defined elsewhere, e.g. in a separate module)  may look as follows:\n\n# Given:\n# Ψ (some black box simulator)\n# H (some observation of the simulator output)\n# prior (prior distribution and parameter constraints)\n\nN_iter = 100 # Number of iterations\n\nfor n in 1:N_iter\n    ϕ_n = get_ϕ_final(prior, eksobj) # Get current ensemble in constrained \"ϕ\"-space\n    G_n = [H(Ψ(ϕ_n[:, i])) for i in 1:J]  # Evaluate forward map\n    g_ens = hcat(G_n...)  # Reformat into `d x N_ens` matrix\n    update_ensemble!(eksobj, g_ens) # Update ensemble\nend","category":"section"},{"location":"ensemble_kalman_sampler/#Solution","page":"Ensemble Kalman Sampler","title":"Solution","text":"The solution of the EKS algorithm is an approximate Gaussian distribution whose mean (u_post) and covariance (Γ_post) can be extracted from the ''final ensemble'' (i.e., after the last iteration). The sample mean of the last ensemble is also the \"optimal\" parameter (u_optim) for the given calibration problem. These statistics can be accessed as follows:\n\n# mean of the Gaussian distribution, the optimal parameter in computational u-space\nu_post = get_u_mean_final(eksobj)\n# (empirical) covariance of the Gaussian distribution in computational u-space\nΓ_post = get_u_cov_final(eksobj)\n\n# constrained samples in physical space (prior contains the physical encoding)\nϕ = get_ϕ_final(prior, eksobj)\n\nTo obtain new samples of this approximate posterior in the constrained space, we first sample the distribution, then transform using the constraints contained within the prior \n\nusing Random, Distributions\n\nten_post_samples = rand(MvNormal(u_post,Γ_post), 10)\nten_post_samples_phys = transform_unconstrained_to_constrained(prior, ten_post_samples) # the optimal physical parameter value","category":"section"},{"location":"ensemble_kalman_sampler/#Quick-comparison-of-samplers","page":"Ensemble Kalman Sampler","title":"Quick comparison of samplers","text":"From examples/LossMinimization/loss_minimization_finite_vs_infinite_ekp.jl. Quick comparison between three samplers ALDI, EKS, and GNKI, taken attheir current defaults. We also plot of error vs spread over the iterations\n\n<img src=\"../assets/samplers/animated_sampler.gif\" width=\"300\"> <img src=\"../assets/samplers/animated_sampler-eks.gif\" width=\"300\"> <img src=\"../assets/samplers/animated_gauss-newton.gif\" width=\"300\">  <img src=\"../assets/samplers/mean_over_iteration.png\" width=\"300\"> \n\nIn black: Prior and posterior distribution contours\nerror (solid) is defined by frac1N_enssum^N_ens_i=1  theta_i - theta^* ^2 where theta_i are ensemble members and theta^* is the true value used to create the observed data.\nspread (dashed) is defined by frac1N_enssum^N_ens_i=1  theta_i - bartheta ^2 where theta_i are ensemble members and bartheta is the mean over these members.\n\nWe see the ensemble does not collapse and samples the posterior distribution. The statistics over long times of these methods are statistically the same, though the EKS/ALDI variant has faster convergence than GNKI.","category":"section"},{"location":"visualization/#visualization","page":"Visualization","title":"Visualization and error metrics","text":"We provide some simple plotting features to help users diagnose convergence in the algorithm. Through\n\nVisualizing priors\nVisualizing slices of parameter ensembles over iterations (or algorithm time) over input space\nVisualizing the loss function or other computed error metrics from compute_error!\n\nThe following documentation provides the overview of what is currently implemented for Plots or Makie backends, these will be expanded in due course.","category":"section"},{"location":"visualization/#Plots.jl","page":"Visualization","title":"Plots.jl","text":"note: Add Plots.jl to your Project.toml\nTo enable plotting by Plots.jl, use using Plots.\n\nPlotting using Plots.jl supports only plotting distributions. See the example below.\n\nusing EnsembleKalmanProcesses.ParameterDistributions\nprior_u1 = constrained_gaussian(\"positive_with_mean_2\", 2, 1, 0, Inf)\nprior_u2 = constrained_gaussian(\"four_with_spread_5\", 0, 5, -Inf, Inf, repeats=4)\nprior = combine_distributions([prior_u1, prior_u2])\n\nusing Plots\np = plot(prior)","category":"section"},{"location":"visualization/#Makie.jl","page":"Visualization","title":"Makie.jl","text":"note: Add a Makie-backend package to your Project.toml\nImport one of the Makie backends (GLMakie, CairoMakie, WGLMakie, RPRMakie, etc.) to enable these functions!\n\nPlotting functionality is provided by Makie.jl through a package extension. See the documentation for a list of all the available plotting functions. The plotting functions have the same signature that one would expect from Makie plotting functions. See the plot methods section in Makie documentation for more information.","category":"section"},{"location":"visualization/#Plot-priors","page":"Visualization","title":"Plot priors","text":"The function plot_parameter_distribution will plot the marginal histograms for all dimensions of the parameter distribution.\n\nusing CairoMakie # load a Makie backend\nimport EnsembleKalmanProcesses.Visualize as viz\n\nfig_priors = CairoMakie.Figure(size = (500, 400))\nviz.plot_parameter_distribution(fig_priors[1, 1], prior)\n\nfig_priors","category":"section"},{"location":"visualization/#Plot-errors","page":"Visualization","title":"Plot errors","text":"The functions plot_error_over_iters and plot_error_over_time and the mutating versions can be used to plot the errors over the number of iterations or time respectively. All keyword arguments supported by Makie.lines are supported by these functions too.\n\nAny of the stored error metrics, computed by EnsembleKalmanProcess can be plotted by the error_metric=\"metric-name\" keyword argument, . See compute_error! for a list of the computed error metrics and their names.\n\n# Assume that ekp is the EnsembleKalmanProcess object\nusing CairoMakie # load a Makie backend\nimport EnsembleKalmanProcesses.Visualize as viz\n\nfig_errors = CairoMakie.Figure(size = (300 * 2, 300 * 1))\nviz.plot_error_over_iters(fig_errors[1, 1], ekp, color = :tomato)\n# Error plotting functions support plotting different errors through the\n# error_metric keyword argument\nax1 = CairoMakie.Axis(fig_errors[1, 2], title = \"Average RMSE over iterations\", yscale = Makie.pseudolog10)\nviz.plot_error_over_time!(\n    ax1,\n    ekp,\n    linestyle = :dashdotdot,\n    error_metric = \"avg_rmse\",\n)\nfig_errors","category":"section"},{"location":"visualization/#Plot-constrained-parameters","page":"Visualization","title":"Plot constrained parameters","text":"","category":"section"},{"location":"visualization/#Plot-by-a-slice-of-the-parameter","page":"Visualization","title":"Plot by a slice of the parameter","text":"The functions plot_ϕ_over_iters and plot_ϕ_over_time and the mutating versions can be used to plot a scatter plot of an individual parameter (dimension) over the number of iterations or time respectively. All keyword arguments supported by Makie.Scatter are supported by these functions too.\n\nFurthermore, there are also plot_ϕ_mean_over_iters and plot_ϕ_mean_over_time and the mutating versions can be used to plot the mean parameter (dimension) over the number of iterations or time respectively. All keyword arguments supported by Makie.Scatter are supported by these functions too. Furthermore, there is the keyword argument plot_std = true which can be used to also plot the standard deviation as well. To support plotting both mean and standard deviation, there are also the keyword arguments line_kwargs and band_kwargs for adjusting how the mean and standard deviation are plotted. Any keyword arguments accepted by Makie.Lines can be passed to line_kwargs and any keyword arguments accepted by Makie.Band can be passed to band_kwargs.\n\nusing CairoMakie # load a Makie backend\nimport EnsembleKalmanProcesses.Visualize as viz\n\nfig_ϕ = CairoMakie.Figure(size = (300 * 2, 300 * 2))\nEnsembleKalmanProcesses.Visualize.plot_ϕ_over_time(fig_ϕ[1, 1], ekp, prior, 1, axis = (xscale = Makie.pseudolog10,))\nax1 = CairoMakie.Axis(fig_ϕ[1, 2])\nviz.plot_ϕ_mean_over_iters!(ax1, ekp, prior, 2)\nviz.plot_ϕ_mean_over_iters(\n            fig_ϕ[2, 1],\n            ekp,\n            prior,\n            3,\n            plot_std = true,\n            line_kwargs = (linestyle = :dash,),\n            band_kwargs = (alpha = 0.2,),\n            color = :purple,\n        )\nfig_ϕ","category":"section"},{"location":"visualization/#Plot-by-distribution-name","page":"Visualization","title":"Plot by distribution name","text":"It is also possible to plot the constrained (possibly-multidimensional) parameters by name. Note that a gridposition per parameter-dimension must be provided for plotting. See the example below.\n\nusing CairoMakie # load a Makie backend\nimport EnsembleKalmanProcesses.Visualize as viz\n\nfig_ϕ = CairoMakie.Figure(size = (300 * 2, 300 * 2))\nviz.plot_ϕ_over_time(\n    [fig_ϕ[1, 1], fig_ϕ[1, 2]],\n    ekp,\n    prior,\n    \"two_with_spread_2\",\n    linewidth = 1.5)\nviz.plot_ϕ_mean_over_time(\n    [fig_ϕ[2, 1], fig_ϕ[2, 2]],\n    ekp,\n    prior,\n    \"two_with_spread_2\",\n    linewidth = 3.0)\nfig_ϕ","category":"section"},{"location":"examples/sinusoid_example_toml/#TOML-interface-for-fitting-parameters-of-a-sinusoid","page":"TOML interface","title":"TOML interface for fitting parameters of a sinusoid","text":"info: How do I run this code?\nThe full code is found in the examples/ directory of the github repository\n\nHere we revisit the simple sinusoid example, with the purpose of demonstrating how the EKP tools can interact with models in a non-intrusive fashion. In particular, this example will be useful for users whose model is written in another language, requires HPC management, or requires additional data processing stages.\n\nWe can generally orchestrate the different stages of the calibration process using a script file. Here we employ a Linux bash script:\n\n> bash calibrate_script\n\nWhich executes the following sequence of processes (in this case, calls to julia --project)\n\n# generate data\njulia --project generate_data.jl \n\n# create initial ensemble and build EKI\njulia --project initialize_EKP.jl \n\n# the EKI loop\nfor i in $(seq 1 $N_iterations); do\n\n    # run the model at each ensemble member\n    for j in $(seq 1 $N_ensemble); do\n        julia --project run_computer_model.jl $i $j\n    done\n\n    # update the ensemble with EKI\n    julia --project update_EKP.jl $i\n    \ndone\n\nThe interaction between the calibration tools and the forward map are only through simple readable files stored in a nested file structure: for iteration i, ensemble member j\n\nParameter values are stored (with their priors) in output/iteration_i/member_j/parameters.toml\nComputer model outputs are stored in output/iteration_i/member_j/model_output.jld2","category":"section"},{"location":"examples/sinusoid_example_toml/#Inputs-and-Outputs","page":"TOML interface","title":"Inputs and Outputs","text":"The prior distributions are provided in priors.toml in TOML format.\n\n[amplitude]\nprior = \"Parameterized(Normal(0.5815754049028404, 0.47238072707743883))\"\nconstraint = \"bounded_below(0.0)\"\ndescription = \"\"\"\nThe amplitude of the sine curve.\nThis yields a physical prior that is log-normal with approximate (mean,sd) = (2,1)\n\"\"\"\n\n[vert_shift]\nprior = \"Parameterized(Normal(0,5))\"\nconstraint = \"no_constraint()\"\ndescription = \"\"\"\nThe vertical shift of the sine curve.\nThis yields a physical prior that is Normal with (mean,sd) = (0,5)\n\"\"\"\n\nMore information on the priors and constraints are given here. More examples defining priors in TOML format may be found in test/TOMLInterface/toml/.\n\nAfter running the example, (it takes several minutes to run), results are stored in output/eki.jld2. To view the results, one can interact with the stored objects by loading julia --project and proceeding as follows:\n\njulia> using EnsembleKalmanProcesses, JLD2\n\njulia> @load \"output/eki.jld2\"\n3-element Vector{Symbol}:\n :eki\n :param_dict\n :prior\n\nThen, for example, the final 6-member parameter ensemble is retrieved with:\n\njulia> get_ϕ_final(prior,eki)\n2×6 Matrix{Float64}:\n 1.38462  1.36124  1.32444  1.26686  1.33462  1.3636\n 6.51158  6.31867  6.68542  6.12809  6.44726  6.52448\n\nwhile the initial ensemble is retrieved with:\n\njulia> get_ϕ(prior, eki, 1)\n2×6 Matrix{Float64}:\n  1.05344   1.67949    6.29847  0.951586  2.07678    1.62284\n -7.00616  -0.931872  -6.11603  0.984338  0.274007  -1.39082\n\nnote: Why is it so slow?\nThe example is slow because the forward map is written in Julia, and so 99.99% of computation for each call to julia --project is precompilation. Ensembles in Julia can be accelerated by using methods discussed here, or by compiling system images with PackageCompiler.jl, for example. This example is for instructional purposes only, and so is not optimized.","category":"section"},{"location":"inflation/#inflation","page":"Inflation","title":"Inflation","text":"Inflation is an approach that slows down collapse in ensemble Kalman methods. Two distinct forms of inflation are implemented in this package. Both involve perturbing the ensemble members following the standard update rule of the chosen Kalman process. Multiplicative inflation expands ensemble members away from their mean in a deterministic manner, whereas additive inflation hinges on the addition of stochastic noise to ensemble members.\n\nFor both implementations, a scaling factor s is included to extend functionality to cases with mini-batching.  The scaling factor s multiplies the artificial time step Delta t in the inflation equations to account for sampling error. For mini-batching, the scaling factor should be:\n\n    s = fracBC\n\nwhere B is the mini-batch size and C is the full dataset size.","category":"section"},{"location":"inflation/#Multiplicative-Inflation","page":"Inflation","title":"Multiplicative Inflation","text":"Multiplicative inflation effectively scales parameter vectors in parameter space, such that the perturbed ensemble remains in the linear span of the original ensemble. The implemented update equation follows Huang et al, 2022 eqn. 41:\n\nbeginaligned\n    m_n+1 = m_n  qquad u^j_n + 1 = m_n+1 + sqrtfrac11 - s Deltat left(u^j_n - m_n right) qquad (1)\nendaligned\n\nwhere m is the ensemble average. In this way, the parameter covariance is inflated by a factor of frac11 - s Deltat, while the ensemble mean remains fixed.\n\n     C_n + 1 = frac11 - s Deltat C_n qquad (2)\n\nMultiplicative inflation can be used by flagging the update_ensemble! method as follows:\n\n    EKP.update_ensemble!(ekiobj, g_ens; multiplicative_inflation = true, s = 1.0)","category":"section"},{"location":"inflation/#Additive-Inflation","page":"Inflation","title":"Additive Inflation","text":"Additive inflation is implemented by systematically adding stochastic perturbations to the parameter ensemble in the form of Gaussian noise. Additive inflation is capable of breaking the linear subspace property, meaning the parameter ensemble can evolve outside of the span of the current ensemble. In additive inflation, the ensemble is perturbed in the following manner after the standard Kalman update:\n\n     u_n+1 = u_n + zeta_n qquad (3) \n    zeta_n sim N(0 fracs Deltat 1 - s Deltat Sigma) qquad (4)\n\nThis can be seen as a stochastic modification of the ensemble covariance, while the mean remains fixed\n\n     C_n + 1 = C_n + fracs Deltat 1 - s Deltat Sigma qquad (5)\n\nFor example, if Sigma = C_n we see inflation that is statistically equivalent to scaling the parameter covariance by a factor of frac11 - s Deltat as in eqn. 2.\n\nAdditive inflation, by default takes Sigma = C_0 (the prior covariance), and can be used by flagging the update_ensemble! method as follows:\n\n    EKP.update_ensemble!(ekiobj, g_ens; additive_inflation = true, s = 1.0)\n\nAny positive semi-definite matrix (or uniform scaling) Sigma may be provided to generate additive noise to the ensemble by flagging the update_ensemble! method as follows:\n\n    Σ = 0.01*I # user defined inflation\n    EKP.update_ensemble!(ekiobj, g_ens; additive_inflation = true, additive_inflation_cov = Σ, s = 1.0)","category":"section"},{"location":"API/Observations/#Observations","page":"Observations","title":"Observations","text":"","category":"section"},{"location":"API/Observations/#Observation","page":"Observations","title":"Observation","text":"","category":"section"},{"location":"API/Observations/#Covariance-utilities","page":"Observations","title":"Covariance utilities","text":"","category":"section"},{"location":"API/Observations/#Minibatcher","page":"Observations","title":"Minibatcher","text":"","category":"section"},{"location":"API/Observations/#ObservationSeries","page":"Observations","title":"ObservationSeries","text":"","category":"section"},{"location":"API/Observations/#EnsembleKalmanProcesses.Observation","page":"Observations","title":"EnsembleKalmanProcesses.Observation","text":"struct Observation{AV1<:(AbstractVector), AV2<:(AbstractVector), AV3<:(AbstractVector), AV4<:(AbstractVector), AV5<:(AbstractVector), MD}\n\nStructure that contains a (possibly stacked) observation. Defined by sample(s), noise covariance(s), and name(s)\n\nTypical Constructors:\n\nObservation(\n    Dict(\n        \"samples\" => [1,2,3],\n        \"covariances\" => I(3),\n        \"names\" => \"one_two_three\",\n    ),\n)\n\nor\n\nObservation([1,2,3], I(3), \"one_two_three\")\n\nOne can stack up multiple observations with combine_observations, (recommended), or by providing vectors of samples, covariances and names to the dictionary.\n\nFields\n\nsamples::AbstractVector: A (vector of) observation vectors\ncovs::AbstractVector: A (vector of) observation covariance matrices\ninv_covs::AbstractVector: A (vector of) inverses of observation covariance matrices\nnames::AbstractVector: A (vector of) name strings\nindices::AbstractVector: A (vector of) indices of the contained observation blocks\nmetadata::Any: Metadata of any type that the user can group with the Observation\n\n\n\n\n\n","category":"type"},{"location":"API/Observations/#EnsembleKalmanProcesses.get_samples","page":"Observations","title":"EnsembleKalmanProcesses.get_samples","text":"get_samples(o::Observation) -> AbstractVector\n\n\ngets the samples field from the Observation object\n\n\n\n\n\n","category":"function"},{"location":"API/Observations/#EnsembleKalmanProcesses.get_covs","page":"Observations","title":"EnsembleKalmanProcesses.get_covs","text":"get_covs(o::Observation) -> AbstractVector\n\n\ngets the covs field from the Observation object\n\n\n\n\n\n","category":"function"},{"location":"API/Observations/#EnsembleKalmanProcesses.get_inv_covs","page":"Observations","title":"EnsembleKalmanProcesses.get_inv_covs","text":"get_inv_covs(o::Observation) -> AbstractVector\n\n\ngets the inv_covs field from the Observation object\n\n\n\n\n\n","category":"function"},{"location":"API/Observations/#EnsembleKalmanProcesses.get_names","page":"Observations","title":"EnsembleKalmanProcesses.get_names","text":"get_names(o::Observation) -> AbstractVector\n\n\ngets the names field from the Observation object\n\n\n\n\n\nget_names(os::ObservationSeries) -> AbstractVector\n\n\ngets the names field from the ObservationSeries object\n\n\n\n\n\n","category":"function"},{"location":"API/Observations/#EnsembleKalmanProcesses.get_indices","page":"Observations","title":"EnsembleKalmanProcesses.get_indices","text":"get_indices(o::Observation) -> AbstractVector\n\n\ngets the indices field from the Observation object\n\n\n\n\n\n","category":"function"},{"location":"API/Observations/#EnsembleKalmanProcesses.get_metadata-Tuple{OB} where OB<:Observation","page":"Observations","title":"EnsembleKalmanProcesses.get_metadata","text":"get_metadata(o::Observation) -> Any\n\n\ngets the metadata field from the Observation object\n\n\n\n\n\n","category":"method"},{"location":"API/Observations/#EnsembleKalmanProcesses.combine_observations","page":"Observations","title":"EnsembleKalmanProcesses.combine_observations","text":"combine_observations(\n    obs_vec::AbstractVector\n) -> Observation{AV1, AV2, AV3, AV4, AV5, Vector{Any}} where {AV1<:(Vector), AV2<:(Vector), AV3<:(Vector), AV4<:(Vector), AV5<:(Vector)}\n\n\ncombines a vector of Observation objects into a single Observation\n\n\n\n\n\n","category":"function"},{"location":"API/Observations/#EnsembleKalmanProcesses.get_obs-Tuple{OB} where OB<:Observation","page":"Observations","title":"EnsembleKalmanProcesses.get_obs","text":"get_obs(o::Observation; build) -> Any\n\n\nif build=true, returns the stacked vector of observed samples samples(default), otherwise it calls get_samples\n\n\n\n\n\n","category":"method"},{"location":"API/Observations/#EnsembleKalmanProcesses.get_obs_noise_cov-Tuple{OB} where OB<:Observation","page":"Observations","title":"EnsembleKalmanProcesses.get_obs_noise_cov","text":"get_obs_noise_cov(o::Observation; build) -> Any\n\n\nif build=true, returns the block matrix of observation covariances covs (default), otherwise it calls get_covs\n\n\n\n\n\n","category":"method"},{"location":"API/Observations/#EnsembleKalmanProcesses.get_obs_noise_cov_inv-Tuple{OB} where OB<:Observation","page":"Observations","title":"EnsembleKalmanProcesses.get_obs_noise_cov_inv","text":"get_obs_noise_cov_inv(o::Observation; build) -> Any\n\n\nif build=true, returns the block matrix of the inverses of the observation covariances inv_covs (default), otherwise it calls get_inv_covs\n\n\n\n\n\n","category":"method"},{"location":"API/Observations/#EnsembleKalmanProcesses.tsvd_mat","page":"Observations","title":"EnsembleKalmanProcesses.tsvd_mat","text":"tsvd_mat(\n    X,\n    r::Int64;\n    return_inverse,\n    quiet,\n    tsvd_kwargs...\n) -> Any\n\n\nFor a given matrix X and rank r, return the truncated SVD for X as a LinearAlgebra.jl SVD object. Setting return_inverse=true also return it's psuedoinverse X⁺.\n\n\n\n\n\n","category":"function"},{"location":"API/Observations/#EnsembleKalmanProcesses.tsvd_cov_from_samples","page":"Observations","title":"EnsembleKalmanProcesses.tsvd_cov_from_samples","text":"tsvd_cov_from_samples(\n    sample_mat::AbstractMatrix;\n    data_are_columns,\n    return_inverse,\n    quiet,\n    tsvd_kwargs...\n) -> Union{Tuple{LinearAlgebra.SVD, LinearAlgebra.SVD}, LinearAlgebra.SVD}\n\n\nFor a given sample_mat, (with data_are_columns = true), rank \"r\" is optionally provided. Returns the SVD objects corresponding to the matrix cov(sample_mat; dims=2). Efficient representation when size(sample_mat,1) << size(sample_mat,2). Setting return_inverse=true also returns its psuedoinverse.\n\nExample usage to make a low-rank covariance:\n\n# \"data\"\nn_trials = 30\noutput_dim = 1_000_000\nY = randn(output_dim, n_trials);\n\n# the noise estimated from the samples (will have rank n_trials-1)\ninternal_cov = tsvd_cov_from_samples(Y)\ninternal_cov_lower_rank = tsvd_cov_from_samples(Y, n_trials-5)\n\nIf one also wishes to add a Diagonal matrix to internal cov to increase the rank in a compact fashion, use the SVDplusD object type\n\ndiag_cov = 1e-6*Diagonal(1:output_dim)\nfull_cov = SVDplusD(internal_cov, diag_cov)\n\nEither can be passed in the covariances entry of an Observation\n\n\n\n\n\n","category":"function"},{"location":"API/Observations/#EnsembleKalmanProcesses.SVDplusD","page":"Observations","title":"EnsembleKalmanProcesses.SVDplusD","text":"struct SVDplusD <: EnsembleKalmanProcesses.SumOfCovariances\n\nStorage for a covariance matrix of the form D + USV' for Diagonal D, and SVD decomposition USV'. Note the inverse of this type (as computed through inv_cov(...)) will be stored compactly as a DminusTall type.\n\nsvd_cov::LinearAlgebra.SVD: summand of covariance matrix stored with SVD decomposition\ndiag_cov::LinearAlgebra.Diagonal: summand of covariance matrix stored as a diagonal matrix\n\n\n\n\n\n","category":"type"},{"location":"API/Observations/#EnsembleKalmanProcesses.DminusTall","page":"Observations","title":"EnsembleKalmanProcesses.DminusTall","text":"struct DminusTall{D<:LinearAlgebra.Diagonal, AM<:(AbstractMatrix)} <: EnsembleKalmanProcesses.SumOfCovariances\n\nStorage for a covariance matrix of the form D - RR' for Diagonal D, and (tall) matrix R. Primary use case for this matrix is to compactly store the inverse of the SVDplusD type.\n\ndiag_cov::LinearAlgebra.Diagonal: summand of covariance matrix stored as a diagonal matrix\ntall_cov::AbstractMatrix: summand of covariance matrix stored as an abstract matrix\n\n\n\n\n\n","category":"type"},{"location":"API/Observations/#EnsembleKalmanProcesses.FixedMinibatcher","page":"Observations","title":"EnsembleKalmanProcesses.FixedMinibatcher","text":"struct FixedMinibatcher{AV1<:(AbstractVector), SS<:AbstractString, ARNG<:Random.AbstractRNG} <: Minibatcher\n\nA Minibatcher that takes in a given epoch of batches. It creates a new epoch by either copying-in-order, or by shuffling, the provided batches.\n\nFields\n\nminibatches::AbstractVector: explicit indices of the minibatched epoch\nmethod::AbstractString: method of selecting minibatches from the list for each epoch (\"order\" select in order, \"random\" generate a random selector)\nrng::Random.AbstractRNG: rng for sampling, if \"random\" method is selected\n\nExample epochs\n\ngiven_batches = [[1,2,3], [4,5,6], [7,8,9]]\nmb = FixedMinibatcher(given_batches)\n# create_new_epoch(mb) = [[1,2,3],[4,5,6],[7,8,9]]\n\nmb2 = FixedMinibatcher(given_batches, \"random\")\n# create_new_epoch(mb2) = [[4,5,6],[1,2,3],[7,8,9]]\n\n\n\n\n\n","category":"type"},{"location":"API/Observations/#EnsembleKalmanProcesses.no_minibatcher","page":"Observations","title":"EnsembleKalmanProcesses.no_minibatcher","text":"no_minibatcher(\n\n) -> FixedMinibatcher{Vector{T}, String, Random.TaskLocalRNG} where T<:(AbstractVector)\nno_minibatcher(\n    epoch_size::Int64\n) -> FixedMinibatcher{Vector{T}, String, Random.TaskLocalRNG} where T<:(AbstractVector)\n\n\nconstructs a FixedMinibatcher of given epoch_size, that generates an epoch of 1:epoch_size and one minibatch that constitutes the whole epoch\n\n\n\n\n\n","category":"function"},{"location":"API/Observations/#EnsembleKalmanProcesses.create_new_epoch!-Tuple{FM} where FM<:FixedMinibatcher","page":"Observations","title":"EnsembleKalmanProcesses.create_new_epoch!","text":"create_new_epoch!(\n    m::FixedMinibatcher,\n    args...;\n    kwargs...\n) -> Any\n\n\nupdates the epoch by either copying (\"order\") the initialization minibatches, or by randomizing (\"random\") their order\n\n\n\n\n\n","category":"method"},{"location":"API/Observations/#EnsembleKalmanProcesses.get_minibatches-Tuple{FM} where FM<:FixedMinibatcher","page":"Observations","title":"EnsembleKalmanProcesses.get_minibatches","text":"get_minibatches(m::FixedMinibatcher) -> AbstractVector\n\n\ngets the minibatches field from the FixedMinibatcher object\n\n\n\n\n\n","category":"method"},{"location":"API/Observations/#EnsembleKalmanProcesses.get_method-Tuple{FM} where FM<:FixedMinibatcher","page":"Observations","title":"EnsembleKalmanProcesses.get_method","text":"get_method(m::FixedMinibatcher) -> AbstractString\n\n\ngets the method field from the FixedMinibatcher object\n\n\n\n\n\n","category":"method"},{"location":"API/Observations/#EnsembleKalmanProcesses.get_rng-Tuple{FM} where FM<:FixedMinibatcher","page":"Observations","title":"EnsembleKalmanProcesses.get_rng","text":"get_rng(m::FixedMinibatcher) -> Random.AbstractRNG\n\n\ngets the rng field from the FixedMinibatcher object\n\n\n\n\n\n","category":"method"},{"location":"API/Observations/#EnsembleKalmanProcesses.RandomFixedSizeMinibatcher","page":"Observations","title":"EnsembleKalmanProcesses.RandomFixedSizeMinibatcher","text":"struct RandomFixedSizeMinibatcher{SS<:AbstractString, ARNG<:Random.AbstractRNG, AV2<:(AbstractVector)} <: Minibatcher\n\nA Minibatcher that takes in a given epoch of batches. It creates a new epoch by either copying-in-order, or by shuffling, the provided batches.\n\nFields\n\nminibatch_size::Int64: fixed size of minibatches\nmethod::AbstractString: how to deal with remainder if minibatch-size doesn't divide the epoch size (\"trim\" - ignore trailing samples, \"extend\" - have a larger final minibatch)\nrng::Random.AbstractRNG: rng for sampling\nminibatches::AbstractVector: explicit indices of the minibatched epoch\n\nExample epochs\n\nfor data = 1:10\nbatch_size = 3\nmb = RandomFixedSizeMinibatcher(batch_size)\n# create_new_epoch(mb) = [[6,7,5],[4,3,10],[9,2,8]] #  1 is trimmed\n\nmb2 = RandomFixedSizeMinibatcher(batch_size, \"extend\")\n# create_new_epoch(mb2) = [[2,9,1],[3,4,7],[10,5,1,6]] # last batch larger\n\n\n\n\n\n","category":"type"},{"location":"API/Observations/#EnsembleKalmanProcesses.get_minibatch_size-Tuple{RFSM} where RFSM<:RandomFixedSizeMinibatcher","page":"Observations","title":"EnsembleKalmanProcesses.get_minibatch_size","text":"get_minibatch_size(m::RandomFixedSizeMinibatcher) -> Int64\n\n\ngets the minibatch_size field from the RandomFixesSizeMinibatcher object\n\n\n\n\n\n","category":"method"},{"location":"API/Observations/#EnsembleKalmanProcesses.get_method-Tuple{RFSM} where RFSM<:RandomFixedSizeMinibatcher","page":"Observations","title":"EnsembleKalmanProcesses.get_method","text":"get_method(m::RandomFixedSizeMinibatcher) -> AbstractString\n\n\ngets the method field from the RandomFixesSizeMinibatcher object\n\n\n\n\n\n","category":"method"},{"location":"API/Observations/#EnsembleKalmanProcesses.get_rng-Tuple{RFSM} where RFSM<:RandomFixedSizeMinibatcher","page":"Observations","title":"EnsembleKalmanProcesses.get_rng","text":"get_rng(m::RandomFixedSizeMinibatcher) -> Random.AbstractRNG\n\n\ngets the rng field from the RandomFixesSizeMinibatcher object\n\n\n\n\n\n","category":"method"},{"location":"API/Observations/#EnsembleKalmanProcesses.get_minibatches-Tuple{RFSM} where RFSM<:RandomFixedSizeMinibatcher","page":"Observations","title":"EnsembleKalmanProcesses.get_minibatches","text":"get_minibatches(\n    m::RandomFixedSizeMinibatcher\n) -> AbstractVector\n\n\ngets the minibatches field from the RandomFixesSizeMinibatcher object\n\n\n\n\n\n","category":"method"},{"location":"API/Observations/#EnsembleKalmanProcesses.ObservationSeries","page":"Observations","title":"EnsembleKalmanProcesses.ObservationSeries","text":"struct ObservationSeries{AV1<:(AbstractVector), MM<:Minibatcher, AV2<:(AbstractVector), AV3<:(AbstractVector), MD}\n\nStructure that contains multiple Observations along with an optional Minibatcher. Stores all observations in EnsembleKalmanProcess, as well as defining the behavior of the get_obs, get_obs_noise_cov, and get_obs_noise_cov_inv methods\n\nTypical Constructor\n\nObservationSeries(\n    Dict(\n        \"observations\" => vec_of_observations,\n        \"names\" => names_of_observations,\n        \"minibatcher\" => minibatcher,\n    ),\n)\n\nFields\n\nobservations::AbstractVector: A vector of Observations to be used in the experiment\nminibatcher::Minibatcher: A Minibatcher object used to define the minibatching\nnames::AbstractVector: A vector of string identifiers for the observations\ncurrent_minibatch_index::Dict: The current index (epoch #, minibatch #) of the current minibatch, stored as a Dict\nminibatches::AbstractVector: The batch history (grouped by minibatch and epoch)\nmetadata::Any: Metadata of any type that the user can group with the ObservationSeries\n\n\n\n\n\n","category":"type"},{"location":"API/Observations/#EnsembleKalmanProcesses.get_observations-Tuple{OS} where OS<:ObservationSeries","page":"Observations","title":"EnsembleKalmanProcesses.get_observations","text":"get_observations(os::ObservationSeries) -> AbstractVector\n\n\ngets the observations field from the ObservationSeries object\n\n\n\n\n\n","category":"method"},{"location":"API/Observations/#EnsembleKalmanProcesses.get_length_epoch-Tuple{OS} where OS<:ObservationSeries","page":"Observations","title":"EnsembleKalmanProcesses.get_length_epoch","text":"get_length_epoch(os::ObservationSeries) -> Any\n\n\ngets the number of minibatches in an epoch\n\n\n\n\n\n","category":"method"},{"location":"API/Observations/#EnsembleKalmanProcesses.get_minibatches-Tuple{OS} where OS<:ObservationSeries","page":"Observations","title":"EnsembleKalmanProcesses.get_minibatches","text":"get_minibatches(os::ObservationSeries) -> AbstractVector\n\n\ngets the minibatches field from the ObservationSeries object\n\n\n\n\n\n","category":"method"},{"location":"API/Observations/#EnsembleKalmanProcesses.get_minibatch_index","page":"Observations","title":"EnsembleKalmanProcesses.get_minibatch_index","text":"get_minibatch_index(\n    os::ObservationSeries,\n    iteration::Int64\n) -> Dict\n\n\nreturns the minibatch_index Dict(\"epoch\"=> x, \"minibatch\" => y), for a given iteration \n\n\n\n\n\n","category":"function"},{"location":"API/Observations/#EnsembleKalmanProcesses.get_current_minibatch_index-Tuple{OS} where OS<:ObservationSeries","page":"Observations","title":"EnsembleKalmanProcesses.get_current_minibatch_index","text":"get_current_minibatch_index(os::ObservationSeries) -> Dict\n\n\ngets the current_minibatch_index field from the ObservationSeries object\n\n\n\n\n\n","category":"method"},{"location":"API/Observations/#EnsembleKalmanProcesses.get_minibatcher-Tuple{OS} where OS<:ObservationSeries","page":"Observations","title":"EnsembleKalmanProcesses.get_minibatcher","text":"get_minibatcher(os::ObservationSeries) -> Minibatcher\n\n\ngets the minibatcher field from the ObservationSeries object\n\n\n\n\n\n","category":"method"},{"location":"API/Observations/#EnsembleKalmanProcesses.get_metadata-Tuple{OS} where OS<:ObservationSeries","page":"Observations","title":"EnsembleKalmanProcesses.get_metadata","text":"get_metadata(os::ObservationSeries) -> Any\n\n\ngets the metadata field from the ObservationSeries object\n\n\n\n\n\n","category":"method"},{"location":"API/Observations/#EnsembleKalmanProcesses.update_minibatch!-Tuple{OS} where OS<:ObservationSeries","page":"Observations","title":"EnsembleKalmanProcesses.update_minibatch!","text":"update_minibatch!(os::ObservationSeries) -> Any\n\n\nWithin an epoch: iterates the current minibatch index by one. At the end of an epoch: obtains a new epoch of minibatches from the Minibatcher updates the epoch index by one, and minibatch index to one.\n\n\n\n\n\n","category":"method"},{"location":"API/Observations/#EnsembleKalmanProcesses.get_minibatch","page":"Observations","title":"EnsembleKalmanProcesses.get_minibatch","text":"get_minibatch(\n    os::ObservationSeries,\n    it_or_mbi::Union{Nothing, Int64, Dict}\n) -> Any\n\n\nget the minibatch for a given minibatch index (Dict(\"epoch\"=> x, \"minibatch\" => y)), or iteration Int. If nothing is provided as an iteration then the current minibatch is returned\n\n\n\n\n\n","category":"function"},{"location":"API/Observations/#EnsembleKalmanProcesses.get_current_minibatch-Tuple{OS} where OS<:ObservationSeries","page":"Observations","title":"EnsembleKalmanProcesses.get_current_minibatch","text":"get_current_minibatch(os::ObservationSeries) -> Any\n\n\nget the current minibatch that is pointed to by the current_minibatch_indices field\n\n\n\n\n\n","category":"method"},{"location":"API/Observations/#EnsembleKalmanProcesses.get_obs-Tuple{OS} where OS<:ObservationSeries","page":"Observations","title":"EnsembleKalmanProcesses.get_obs","text":"get_obs(os::ObservationSeries; kwargs...) -> Any\n\n\nif build=true then gets the observed sample, stacked over the current minibatch. build=false lists the samples for all observations. \n\n\n\n\n\n","category":"method"},{"location":"API/Observations/#EnsembleKalmanProcesses.get_obs-Union{Tuple{IorN}, Tuple{OS}, Tuple{OS, IorN}} where {OS<:ObservationSeries, IorN<:Union{Nothing, Int64}}","page":"Observations","title":"EnsembleKalmanProcesses.get_obs","text":"get_obs(os::Union{Nothing, Int64}; kwargs...)\n\n\nif build=true then gets the observed sample, stacked over the minibatch at iteration. build=false lists the samples for all observations. If isnothing(iteration) or not defined then the current iteration is used.\n\n\n\n\n\n","category":"method"},{"location":"API/Observations/#EnsembleKalmanProcesses.get_obs_noise_cov-Tuple{OS} where OS<:ObservationSeries","page":"Observations","title":"EnsembleKalmanProcesses.get_obs_noise_cov","text":"get_obs_noise_cov(os::ObservationSeries; kwargs...) -> Any\n\n\nif build=true then gets the observation covariance matrix, blocked over the current minibatch. build=false lists the covs for all observations \n\n\n\n\n\n","category":"method"},{"location":"API/Observations/#EnsembleKalmanProcesses.get_obs_noise_cov-Union{Tuple{IorN}, Tuple{OS}, Tuple{OS, IorN}} where {OS<:ObservationSeries, IorN<:Union{Nothing, Int64}}","page":"Observations","title":"EnsembleKalmanProcesses.get_obs_noise_cov","text":"get_obs_noise_cov(os::Union{Nothing, Int64}; kwargs...)\n\n\nif build=true then gets the observation covariance matrix, blocked over the minibatch at iteration. build=false lists the covs for all observations. If isnothing(iteration) or not defined then the current iteration is used.\n\n\n\n\n\n","category":"method"},{"location":"API/Observations/#EnsembleKalmanProcesses.get_obs_noise_cov_inv-Tuple{OS} where OS<:ObservationSeries","page":"Observations","title":"EnsembleKalmanProcesses.get_obs_noise_cov_inv","text":"get_obs_noise_cov_inv(\n    os::ObservationSeries;\n    kwargs...\n) -> Any\n\n\nif build=true then gets the inverse of the observation covariance matrix, blocked over the current minibatch. build=false lists the inv_covs for all observations. \n\n\n\n\n\n","category":"method"},{"location":"API/Observations/#EnsembleKalmanProcesses.get_obs_noise_cov_inv-Union{Tuple{IorN}, Tuple{OS}, Tuple{OS, IorN}} where {OS<:ObservationSeries, IorN<:Union{Nothing, Int64}}","page":"Observations","title":"EnsembleKalmanProcesses.get_obs_noise_cov_inv","text":"get_obs_noise_cov_inv(os::Union{Nothing, Int64}; kwargs...)\n\n\nif build=true then gets the inverse of the observation covariance matrix, blocked over minibatch at iteration. build=false lists the inv_covs for all observations. If isnothing(iteration) or not defined then the current iteration is used.\n\n\n\n\n\n","category":"method"},{"location":"installation_instructions/#Installation","page":"Installation instructions","title":"Installation","text":"EnsembleKalmanProcesses.jl is a registered Julia package. You can install the latest version of EnsembleKalmanProcesses.jl through the built-in package manager. Press ] in the Julia REPL command prompt and\n\njulia> ]\npkg> add EnsembleKalmanProcesses\npkg> instantiate\n\nThis will install the latest tagged release of the package.\n\ninfo: But I wanna be on the bleeding edge...\nIf you want the most recent developer's version of the package thenjulia> ]\npkg> add EnsembleKalmanProcesses#main\npkg> instantiate\n\nYou can run the tests via the package manager by:\n\njulia> ]\npkg> test EnsembleKalmanProcesses","category":"section"},{"location":"installation_instructions/#Cloning-the-repository","page":"Installation instructions","title":"Cloning the repository","text":"If you are interested in getting your hands dirty and modifying the code then, you can also clone the repository and then instantiate, e.g.,\n\n> cd EnsembleKalmanProcesses.jl\n> julia --project -e 'using Pkg; Pkg.instantiate()'\n\ninfo: Do I need to clone the repository?\nMost times, cloning the repository in not necessary. If you only want to use the package's functionality, adding the packages as a dependency on your project is enough.","category":"section"},{"location":"installation_instructions/#Running-the-test-suite","page":"Installation instructions","title":"Running the test suite","text":"You can run the package's tests:\n\n> julia --project -e 'using Pkg; Pkg.test()'\n\nAlternatively, you can do this from within the repository:\n\n> julia --project\njulia> ]\n(EnsembleKalmanProcesses) pkg> test","category":"section"},{"location":"installation_instructions/#Building-the-documentation-locally","page":"Installation instructions","title":"Building the documentation locally","text":"Once the project is built, you can build the project documentation under the docs/ sub-project:\n\n> julia --project=docs/ -e 'using Pkg; Pkg.develop(PackageSpec(path=pwd())); Pkg.instantiate()'\n> julia --project=docs/ docs/make.jl\n\nThe locally rendered HTML documentation can be viewed at docs/build/index.html","category":"section"},{"location":"installation_instructions/#Running-repository-examples","page":"Installation instructions","title":"Running repository examples","text":"We have a selection of examples, found within the examples/ directory to demonstrate different use of our toolbox. Each example directory contains a Project.toml\n\nTo build with the latest EnsembleKalmanProcesses.jl release:\n\n> cd examples/example-name/\n> julia --project -e 'using Pkg; Pkg.instantiate()'\n> julia --project example-file-name.jl\n\nIf you wish to run a local modified version of EnsembleKalmanProcesses.jl then try the following (starting from the EnsembleKalmanProcesses.jl package root)\n\n> cd examples/example-name/\n> julia --project \n> julia> ]\n> (example-name)> rm EnsembleKalmanProcesses.jl\n> (example-name)> dev ../..\n> (example-name)> instantiate\n\nfollowed by\n\n> julia --project example-file-name.jl","category":"section"},{"location":"literated/loss_minimization/#Minimization-of-simple-loss-functions","page":"Minimization Loss","title":"Minimization of simple loss functions","text":"info: How do I run this code?\nThe full code is found in the examples/ directory of the github repository\n\nFirst we load the required packages.\n\nusing Distributions, LinearAlgebra, Random, Plots\n\nusing EnsembleKalmanProcesses\nusing EnsembleKalmanProcesses.ParameterDistributions\n\nconst EKP = EnsembleKalmanProcesses","category":"section"},{"location":"literated/loss_minimization/#Loss-function-with-single-minimum","page":"Minimization Loss","title":"Loss function with single minimum","text":"Here, we minimize the loss function\n\nG₁(u) = u - u_* \n\nwhere u is a 2-vector of parameters and u_* is given; here u_* = (-1 1).\n\nustar = [1, -1]\nG₁(u) = [norm(u - ustar)]\nnothing # hide\n\nWe set the seed for pseudo-random number generator for reproducibility.\n\nrng_seed = 41\nrng = Random.seed!(Random.GLOBAL_RNG, rng_seed)\nnothing # hide\n\nWe set a stabilization level, which can aid the algorithm convergence\n\ndim_output = 1\nstabilization_level = 1e-3\nΓ_stabilization = stabilization_level * Matrix(I, dim_output, dim_output)\n\nThe functional is positive so to minimize it we may set the target to be 0,\n\nG_target = [0]\nnothing # hide","category":"section"},{"location":"literated/loss_minimization/#Prior-distributions","page":"Minimization Loss","title":"Prior distributions","text":"As we work with a Bayesian method, we define a prior. This will behave like an \"initial guess\" for the likely region of parameter space we expect the solution to live in. Here we define Normal(01) distributions with no constraints\n\nprior_u1 = constrained_gaussian(\"u1\", 0, 1, -Inf, Inf)\nprior_u2 = constrained_gaussian(\"u1\", 0, 1, -Inf, Inf)\nprior = combine_distributions([prior_u1, prior_u2])\nnothing # hide\n\nnote: Note\nIn this example there are no constraints, therefore no parameter transformations.","category":"section"},{"location":"literated/loss_minimization/#Calibration","page":"Minimization Loss","title":"Calibration","text":"We choose the number of ensemble members and the number of iterations of the algorithm\n\nN_ensemble = 20\nN_iterations = 10\nnothing # hide\n\nThe initial ensemble is constructed by sampling the prior\n\ninitial_ensemble = EKP.construct_initial_ensemble(rng, prior, N_ensemble)\n\nWe then initialize the Ensemble Kalman Process algorithm, with the initial ensemble, the target, the stabilization and the process type (for EKI this is Inversion, initialized with Inversion()). We also remove the cutting-edge defaults and instead use the vanilla options.\n\nensemble_kalman_process = EKP.EnsembleKalmanProcess(\n    initial_ensemble,\n    G_target,\n    Γ_stabilization,\n    Inversion(),\n    scheduler = DefaultScheduler(1),\n    accelerator = DefaultAccelerator(),\n    localization_method = EnsembleKalmanProcesses.Localizers.NoLocalization(),\n)\nnothing # hide\n\nThen we calibrate by (i) obtaining the parameters, (ii) calculate the loss function on the parameters (and concatenate), and last (iii) generate a new set of parameters using the model outputs:\n\nfor i in 1:N_iterations\n    params_i = get_u_final(ensemble_kalman_process)\n\n    g_ens = hcat([G₁(params_i[:, i]) for i in 1:N_ensemble]...)\n\n    EKP.update_ensemble!(ensemble_kalman_process, g_ens)\nend\n\nand visualize the results:\n\nu_init = get_u_prior(ensemble_kalman_process)\n\nanim_unique_minimum = @animate for i in 1:N_iterations\n    u_i = get_u(ensemble_kalman_process, i)\n\n    plot(\n        [ustar[1]],\n        [ustar[2]],\n        seriestype = :scatter,\n        markershape = :star5,\n        markersize = 11,\n        markercolor = :red,\n        label = \"optimum u⋆\",\n    )\n\n    plot!(\n        u_i[1, :],\n        u_i[2, :],\n        seriestype = :scatter,\n        xlims = extrema(u_init[1, :]),\n        ylims = extrema(u_init[2, :]),\n        xlabel = \"u₁\",\n        ylabel = \"u₂\",\n        markersize = 5,\n        markeralpha = 0.6,\n        markercolor = :blue,\n        label = \"particles\",\n        title = \"EKI iteration = \" * string(i),\n    )\nend\nnothing # hide\n\nThe results show that the minimizer of G_1 is u=u_*.\n\ngif(anim_unique_minimum, \"unique_minimum.gif\", fps = 1) # hide","category":"section"},{"location":"literated/loss_minimization/#Loss-function-with-two-minima","page":"Minimization Loss","title":"Loss function with two minima","text":"Now let's do an example in which the loss function has two minima. We minimize the loss function\n\nG₂(u) = u - v_* u - w_* \n\nwhere again u is a 2-vector, and v_* and w_* are given 2-vectors. Here, we take v_* = (1 -1) and w_* = (-1 -1).\n\nvstar = [1, -1]\nwstar = [-1, -1]\nG₂(u) = [norm(u - vstar) * norm(u - wstar)]\nnothing # hide\n\nThe procedure is same as the single-minimum example above.\n\nWe set the seed for pseudo-random number generator for reproducibility,\n\nrng_seed = 10\nrng = Random.seed!(Random.GLOBAL_RNG, rng_seed)\nnothing # hide\n\nA positive function can be minimized with a target of 0,\n\nG_target = [0]\n\nWe choose the stabilization as in the single-minimum example","category":"section"},{"location":"literated/loss_minimization/#Prior-distributions-2","page":"Minimization Loss","title":"Prior distributions","text":"We define the prior. We can place prior information on e.g., u₁, demonstrating a belief that u₁ is more likely to be negative. This can be implemented by setting a bias in the mean of its prior distribution to e.g., -05:\n\nprior_u1 = constrained_gaussian(\"u1\", -0.5, sqrt(2), -Inf, Inf)\nprior_u2 = constrained_gaussian(\"u1\", 0, sqrt(2), -Inf, Inf)\nprior = combine_distributions([prior_u1, prior_u2])\n\nnote: Note\nIn this example there are no constraints, therefore no parameter transformations.","category":"section"},{"location":"literated/loss_minimization/#Calibration-2","page":"Minimization Loss","title":"Calibration","text":"We choose the number of ensemble members, the number of EKI iterations, construct our initial ensemble and the EKI with the Inversion() constructor (exactly as in the single-minimum example):\n\nN_ensemble = 20\nN_iterations = 20\n\ninitial_ensemble = EKP.construct_initial_ensemble(rng, prior, N_ensemble)\n\nensemble_kalman_process = EKP.EnsembleKalmanProcess(\n    initial_ensemble,\n    G_target,\n    Γ_stabilization,\n    Inversion(),\n    scheduler = DefaultScheduler(1),\n    accelerator = DefaultAccelerator(),\n    localization_method = EnsembleKalmanProcesses.Localizers.NoLocalization(),\n)\n\nWe calibrate by (i) obtaining the parameters, (ii) calculating the loss function on the parameters (and concatenate), and last (iii) generate a new set of parameters using the model outputs:\n\nfor i in 1:N_iterations\n    params_i = get_u_final(ensemble_kalman_process)\n\n    g_ens = hcat([G₂(params_i[:, i]) for i in 1:N_ensemble]...)\n\n    EKP.update_ensemble!(ensemble_kalman_process, g_ens)\nend\n\nand visualize the results:\n\nu_init = get_u_prior(ensemble_kalman_process)\n\nanim_two_minima = @animate for i in 1:N_iterations\n    u_i = get_u(ensemble_kalman_process, i)\n\n    plot(\n        [vstar[1]],\n        [vstar[2]],\n        seriestype = :scatter,\n        markershape = :star5,\n        markersize = 11,\n        markercolor = :red,\n        label = \"optimum v⋆\",\n    )\n\n    plot!(\n        [wstar[1]],\n        [wstar[2]],\n        seriestype = :scatter,\n        markershape = :star5,\n        markersize = 11,\n        markercolor = :green,\n        label = \"optimum w⋆\",\n    )\n\n    plot!(\n        u_i[1, :],\n        u_i[2, :],\n        seriestype = :scatter,\n        xlims = extrema(u_init[1, :]),\n        ylims = extrema(u_init[2, :]),\n        xlabel = \"u₁\",\n        ylabel = \"u₂\",\n        markersize = 5,\n        markeralpha = 0.6,\n        markercolor = :blue,\n        label = \"particles\",\n        title = \"EKI iteration = \" * string(i),\n    )\nend\nnothing # hide\n\nOur bias in the prior shifts the initial ensemble into the negative u_1 direction, and thus increases the likelihood (over different instances of the random number generator) of finding the minimizer u=w_*.\n\ngif(anim_two_minima, \"two_minima.gif\", fps = 1) # hide\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"gauss_newton_kalman_inversion/#gnki","page":"Gauss Newton Kalman Inversion","title":"Gauss Newton Kalman Inversion","text":"","category":"section"},{"location":"gauss_newton_kalman_inversion/#What-Is-It-and-What-Does-It-Do?","page":"Gauss Newton Kalman Inversion","title":"What Is It and What Does It Do?","text":"Gauss Netwon Kalman Inversion (GNKI) (Chada et al., 2021, Chen & Oliver, 2013), also known as the Iterative Ensemble Kalman Filter with Statistical Linearization, is a derivative-free ensemble optimizaton method based on the Gauss Newton optimization update and the Iterative Extended Kalman Filter (IExKF) (Jazwinski, 1970).  In the linear case and continuous limit, GNKI recovers the true posterior mean and covariance.  Empirically, GNKI performs well as an optimization algorithm in the nonlinear case.  ","category":"section"},{"location":"gauss_newton_kalman_inversion/#Problem-Formulation","page":"Gauss Newton Kalman Inversion","title":"Problem Formulation","text":"The data y and parameter vector theta are assumed to be related according to:\n\ntag1 y = mathcalG(theta) + eta \n\nwhere mathcalG  mathbbR^p rightarrow mathbbR^d denotes the forward map, y in mathbbR^d is the vector of observations, and eta is the observational noise, which is assumed to be drawn from a d-dimensional Gaussian with distribution mathcalN(0 Gamma_y). The objective of the inverse problem is to compute the unknown parameters theta given the observations y, the known forward map mathcalG, and noise characteristics eta of the process.\n\nnote: Note\nGNKI relies on minimizing a loss function that includes regularization.  The user must specify a Gaussian prior with distribution mathcalN(m Gamma_theta). See Prior distributions to see how one can apply flexible constraints while maintaining Gaussian priors. \n\nThe optimal parameters theta^* given relation (1) minimize the loss \n\nmathcalL(theta y) = langle mathcalG(theta) - y    Gamma_y^-1 left ( mathcalG(theta) - y right ) rangle + langle m - theta    Gamma_theta^-1 left ( m - theta  right ) rangle\n\nwhere m is the prior mean and Gamma_theta is the prior covariance. ","category":"section"},{"location":"gauss_newton_kalman_inversion/#Algorithm","page":"Gauss Newton Kalman Inversion","title":"Algorithm","text":"GNKI updates the j-th ensemble member at the n-th iteration by directly approximating the Jacobian with statistics from the ensemble.\n\nFirst, the ensemble covariance matrices are computed: \n\nbeginaligned\n        mathcalG_n^(j)  = mathcalG(theta_n^(j)) qquad \n        barmathcalG_n = dfrac1Jsum_k=1^JmathcalG_n^(k) \n         C^theta mathcalG_n = dfrac1J - 1sum_k=1^J\n        (theta_n^(k) - bartheta_n )(mathcalG_n^(k) - barmathcalG_n)^T \n         C^theta theta_n = dfrac1J - 1 sum_k=1^J \n        (theta_n^(k) - bartheta_n )(theta_n^(k) - bartheta_n )^T\n\nendaligned\n\nUsing the ensemble covariance matrices, the update equation from n to n+1 under GNKI is\n\nbeginaligned\n         theta_n+1^(j) = theta_n^(j) + alpha left K_nleft(y_n^(j) - mathcalG(theta_n^(j))right) + left(I - K_n G_nright)left(m_n^(j) - theta_n^(j)right) right \n        \n         \n\n         K_n = Gamma_theta G_n^T left(G_n Gamma_theta G_n^T + Gamma_yright)^-1 \n        \n         G_n = left(C^theta mathcalG_nright)^T left(C^theta theta_nright)^-1 \n\n\nendaligned\n\nwhere y_n^(j) sim mathcalN(y 2alpha^-1Gamma_y) and m_n^(j) sim mathcalN(m 2alpha^-1Gamma_theta).","category":"section"},{"location":"gauss_newton_kalman_inversion/#Creating-the-EKI-Object","page":"Gauss Newton Kalman Inversion","title":"Creating the EKI Object","text":"We first build a prior distribution (for details of the construction see here).  Then we build our EKP object with \n\nusing EnsembleKalmanProcesses\n\ngnkiobj = EnsembleKalmanProcess(args..., GaussNewtonInversion(prior); kwargs...)\n\nFor general EKP object creation requirements see Creating the EKI object.  To make updates using the inversion algorithm see Updating the Ensemble.  ","category":"section"},{"location":"parameter_distributions/#parameter-distributions","page":"Prior distributions","title":"Defining prior distributions","text":"Bayesian inference begins with an explicit prior distribution. This page describes the interface EnsembleKalmanProcesses provides for specifying priors on parameters, via the ParameterDistributions module (src/ParameterDistributions.jl).","category":"section"},{"location":"parameter_distributions/#Summary","page":"Prior distributions","title":"Summary","text":"","category":"section"},{"location":"parameter_distributions/#ParameterDistribution-objects","page":"Prior distributions","title":"ParameterDistribution objects","text":"A prior is specified by a ParameterDistribution object, which has three components:\n\nThe distribution itself, given as a ParameterDistributionType object. This includes standard Julia Distributions, GaussianRandomFields as well as empirical/sample-based distributions, and thus can be univariate, multivariate or functional. To clarify, despite our use of the term \"Kalman processes,\" the prior distribution is not required to be Gaussian.\nA constraint (or array of constraints) on the domain of the distribution, given as a ConstraintType or Array{ConstraintType} object (the latter case builds a multivariate constraint as the Cartesian product of one-dimensional Constraints). This is used to enforce physical parameter values during inference: the model is never evaluated at parameter values outside the constrained region, and the posterior distribution will only be supported there.\nThe parameter name, given as a String.\n\nIn multiparameter settings, one should define one ParameterDistribution per parameter, and then concatenate these either in the constructor or with combine_distributions. This is illustrated below and in the Example combining several distributions.\n\nnote: What's up with the notation u, ϕ, and θ?\nParameters in unconstrained spaces are often denoted u or theta in the literature. In the code, method names featuring _u imply the return of a computational, unconstrained parameter.Parameters in physical/constrained spaces are often denoted mathcalT^-1(u), mathcalT^-1(theta), or phi in the literature (for some bijection mathcalT mapping to the unbounded space). In the code, method names featuring _ϕ imply the return of a physical, constrained parameter, and will always require a prior as input to perform the transformations internally.For more notations see our Glossary.","category":"section"},{"location":"parameter_distributions/#constrained-gaussian","page":"Prior distributions","title":"Recommended constructor","text":"constrained_gaussian() is a streamlined constructor for ParameterDistributions which addresses the most common use case; more general forms of the constructor are documented below, but we highly recommend that users begin here when it comes to specifying priors, only using the general constructor when necessary.\n\nUsage:\n\nusing EnsembleKalmanProcesses.ParameterDistributions # for `constrained_gaussian`, `combine_distributions`\nprior_1 = constrained_gaussian(\"param_1\", μ_1, σ_1, lower_bound, upper_bound)\nprior_2 = constrained_gaussian(\"param_2\", μ_2, σ_2, 0.0, Inf, repeats=3)\nprior = combine_distributions([prior_1, prior_2])\n\nprior_1 is a ParameterDistribution describing a prior distribution for a parameter named \"param_1\" taking values on the interval [lower_bound, upper_bound]; the prior distribution has approximate mean μ_1 and standard deviation σ_1.\n\nprior_2 is a ParameterDistribution describing a 3-dimensional prior distribution for a parameter named \"param_2\" with each dimensions taking independent values on the half-open interval [0.0, Inf); the marginals of this prior distribution have approximate mean μ_2 and standard deviation σ_2.\n\nThe use case constrained_gaussian() addresses is when prior information is qualitative, and exact distributions of the priors are unknown: i.e., the user is only able to specify the physical and likely ranges of prior parameter values at a rough, qualitative level. constrained_gaussian() does this by constructing a ParameterDistribution corresponding to a Gaussian \"squashed\" to fit in the given constraint interval, such that the \"squashed\" distribution has the specified mean and standard deviation (e.g. prior_2 above is a log-normal for each dimension).\n\nThe parameters of the Gaussian are chosen automatically (depending on the constraint) to reproduce the desired μ and σ — per the use case, other details of the form of the prior distribution shouldn't be important for downstream inference!                \n\nnote: Slow/Failed construction?\nThe most common case of slow or failed construction is when requested parameters place too much mass at the hard boundary. A typical case is when the requested variance satisfies sigma approx mathrmdist(mumathrmboundary) Such priors can be defined, but not with our convenience constructor. If this is not the case but you still get failures please let us know!","category":"section"},{"location":"parameter_distributions/#Plotting","page":"Prior distributions","title":"Plotting","text":"For quick visualization we have a plot recipe for ParameterDistribution types. This will plot marginal histograms for all dimensions of the parameter distribution. For example, \n\n# with values:\n# e.g. lower_bound = 0.0, upper_bound = 1.0\n# μ_1 = 0.5, σ_1 = 0.25\n# μ_2 = 0.5, σ_2 = 0.25\n\nusing Plots\nplot(prior) \n\nOne can also access the underlying Gaussian distributions in the unconstrained space with\n\nusing Plots\nplot(prior, constrained=false) ","category":"section"},{"location":"parameter_distributions/#Recommended-constructor-Simple-example","page":"Prior distributions","title":"Recommended constructor - Simple example","text":"Task: We wish to create a prior for a one-dimensional parameter. Our problem dictates that this parameter is bounded between 0 and 1; domain knowledge leads us to expect it should be around 0.7. The parameter is called point_seven.\n\nWe're told that the prior mean is 0.7; we choose a prior standard deviation of 0.15 to be sufficiently wide without putting too much mass at the upper bound. The constructor is then\n\nusing EnsembleKalmanProcesses.ParameterDistributions # for `constrained_gaussian`\nprior = constrained_gaussian(\"point_seven\", 0.7, 0.15, 0.0, 1.0)\nnothing # hide\n\nThe pdf of the constructed prior distribution (in the physical, constrained space) looks like:\n\np = plot(p2, legend=false, size = (450, 450)) #hide\n\nIn Simple example revisited below, we repeat this example \"manually\" with the general constructor.\n\nnote: What if I want to impose the same prior on many parameters?\nThe recommended constructor can be called as constrained_gaussian(...; repeats = n) to return a combined prior formed by n identical priors.","category":"section"},{"location":"parameter_distributions/#ParameterDistribution-struct","page":"Prior distributions","title":"ParameterDistribution struct","text":"This section provides more details on the components of a ParameterDistribution object.","category":"section"},{"location":"parameter_distributions/#ParameterDistributionType","page":"Prior distributions","title":"ParameterDistributionType","text":"The ParameterDistributionType struct wraps four types for specifying different types of prior distributions:\n\nThe Parameterized type is initialized using a Julia Distributions.jl object. Samples are drawn randomly from the distribution object.\nThe VectorOfParameterized type is initialized with a vector of distributions.\nThe Samples type is initialized using a two dimensional array. Samples are drawn randomly (with replacement) from the columns of the provided array.\nThe FunctionParameterDistributionType struct defines parameters specified as fields over a domain. More detail can be found here.\n\nwarning: Warning\nWe recommend that the distributions be unbounded (see next section), as the filtering algorithms in EnsembleKalmanProcesses are not guaranteed to preserve constraints unless defined through the ConstraintType mechanism.","category":"section"},{"location":"parameter_distributions/#ConstraintType","page":"Prior distributions","title":"ConstraintType","text":"The inference algorithms implemented in EnsembleKalmanProcesses assume unbounded parameter domains. To be able to handle constrained parameter values consistently, the ConstraintType  defines a bijection between the physical, constrained parameter domain and an unphysical, unconstrained domain in which the filtering takes place. This bijection is specified by the functions transform_constrained_to_unconstrained and transform_unconstrained_to_constrained, which are built from either predefined constructors or user-defined constraint functions given as arguments to the ConstraintType constructor. \n\nWe provide the following predefined constructors which implement mappings that handle the most common constraints:\n\nno_constraint(): The parameter is unconstrained and takes values in (-∞, ∞) (mapping is the identity).\nbounded_below(lower_bound): The parameter takes values in [lower_bound, ∞).\nbounded_above(upper_bound): The parameter takes values in (-∞, upper_bound].\nbounded(lower_bound,upper_bound): The parameter takes values on the interval [lower_bound, upper_bound].\n\nThese are demonstrated in ConstraintType Examples.\n\nCurrently we only support multivariate constraints which are the Cartesian product of the one-dimensional ConstraintTypes. Every component of a multidimensional parameter must have an associated constraint, so, e.g. for a multivariate ParameterDistributionType of dimension p the user must provide a p-dimensional Array{ConstraintType}. A VectorOfParameterized distribution built with distributions of dimension p and q has dimension p+q.\n\nnote: Note\nWhen a nontrivial ConstraintType is given, the general constructor assumes the ParameterDistributionType is specified in the unconstrained space; the actual prior pdf is then the composition of the ParameterDistributionType's pdf with the transform_unconstrained_to_constrained transformation. We provide constrained_gaussian to define priors directly in the physical, constrained space.\n\nwarning: Warning\nIt is up to the user to ensure any custom mappings transform_constrained_to_unconstrained and transform_unconstrained_to_constrained are inverses of each other.","category":"section"},{"location":"parameter_distributions/#The-name","page":"Prior distributions","title":"The name","text":"This is simply a String used to identify different parameters in multi-parameter situations, as in the methods below.","category":"section"},{"location":"parameter_distributions/#function-parameter-type","page":"Prior distributions","title":"FunctionParameterDistributionType","text":"Learning a function distribution is useful when one wishes to obtain a parametric representation of a function that is (relatively) agnostic of the underlying grid discretization. Most practical implementations involve posing a restrictive class of functions by truncation of a spectral decomposition. The function is then represented as a set of coefficients of these modes (known as degrees of freedom), rather than directly through the values at evaluation points.\n\nAs a subtype of ParameterDistributionType, we currently support one option for specifying prior distributions over functions:\n\nThe GaussianRandomFieldInterface type is initialized with a Gaussian Random Field object and the GRF package. Currently we support objects from GaussianRandomFields.jl with package GRFJL(). Gaussian random fields allow the definition of scalar function distributions defined over a uniform mesh on interval, rectangular, and hyper-rectangular domains.\n\nAs with other ParameterDistributions, a function distribution, is built from a name, a FunctionPameterDistributionType struct and a constraint, here only one, placed on the scalar output space of the function using a Constraint().\n\nnote: constraints\nThe transformation transform_unconstrained_to_constrained, will map from (unconstrained) degrees of freedom, to (constrained) evaluations of the function on a numerical grid. In particular, the transform_constrained_to_unconstrained is no longer the inverse of this map, it now simply maps from constrained evaluations to unconstrained evaluations on the grid.\n\nWe provide an example construction here.","category":"section"},{"location":"parameter_distributions/#ParameterDistribution-constructor","page":"Prior distributions","title":"ParameterDistribution constructor","text":"The Recommended constructor, constrained_gaussian(), is described above. For more general cases in which the prior needs to be specified in more detail, a ParameterDistribution may be constructed \"manually\" from its component objects:\n\nusing EnsembleKalmanProcesses.ParameterDistributions # for `ParameterDistribution`, `combine_distributions`\nprior_1 = ParameterDistribution(distribution_1, constraint_1, name_1)\nprior_2 = ParameterDistribution(distribution_2, constraint_2, name_2)\nprior = combine_distributions( [prior_1, prior_2])\nnothing # hide\n\nArguments may also be provided as a Dict:\n\nusing EnsembleKalmanProcesses.ParameterDistributions # for `ParameterDistribution`\ndict_1 = Dict(\"distribution\" => distribution_1, \"constraint\" => constraint_1, \"name\" => name_1)\ndict_2 = Dict(\"distribution\" => distribution_2, \"constraint\" => constraint_2, \"name\" => name_2)\nprior = ParameterDistribution( [dict_1, dict_2] )\nnothing # hide\n\nWe provide Additional Examples below; see also examples in the package examples/ and unit tests found in test/ParameterDistributions/runtests.jl.","category":"section"},{"location":"parameter_distributions/#ParameterDistribution-methods","page":"Prior distributions","title":"ParameterDistribution methods","text":"These functions typically return a Dict with ParameterDistribution.name as a keys, or an Array if requested:\n\nget_name: returns the name(s) of parameters in the ParameterDistribution.\nget_distribution: returns the distributions (ParameterDistributionType objects) in the ParameterDistribution. Note that this is not the prior pdf used for inference if nontrivial constraints have been applied.\nmean, var, cov, sample, logpdf: mean, variance, covariance, logpdf or samples the Julia Distribution if Parameterized, or draws from the list of samples if Samples. Extends the StatsBase definitions. Note that these do not correspond to the prior pdf used for inference if nontrivial constraints have been applied.\ntransform_unconstrained_to_constrained: Applies the constraint mappings.\ntransform_constrained_to_unconstrained: Applies the inverse constraint mappings.","category":"section"},{"location":"parameter_distributions/#Additional-Examples","page":"Prior distributions","title":"Additional Examples","text":"","category":"section"},{"location":"parameter_distributions/#Simple-example-revisited","page":"Prior distributions","title":"Simple example revisited","text":"To illustrate what the constrained_gaussian constructor is doing, in this section we repeat the Recommended constructor - Simple example given above, using the \"manual,\" general-purpose constructor. Let's bring in the packages we will require\n\nusing EnsembleKalmanProcesses.ParameterDistributions # for `bounded`, `Parameterized`, and `ParameterDistribution` \nusing Distributions # for `Normal`\nnothing # hide\n\nThen we initialize the constraint first,\n\nconstraint = bounded(0, 1)\nnothing # hide\n\nThis defines the following transformation to the  constrained space (and also its inverse)\n\ntransform_unconstrained_to_constrained(x) = exp(x) / (exp(x) + 1)\nnothing # hide\n\nThe prior mean should be around 0.7 (in the constrained space), and one can find that the push-forward of a particular normal distribution, namely, transform_unconstrained_to_constrained(Normal(mean = 1, sd = 0.5)) gives a prior pdf with 95% of its mass between [0.5, 0.88]. \n\nThis is the main difference from the use of the constrained_gaussian constructor: in that example, the constructor numerically solved for the parameters of the Normal() which would reproduce the requested μ, σ for the physical, constrained quantity (since no closed-form transformation for the moments exists.)\n\ndistribution = Parameterized(Normal(1, 0.5))\nnothing # hide\n\nFinally we attach the name\n\nname = \"point_seven\"\nnothing # hide\n\nand the distribution is created by either:\n\nprior = ParameterDistribution(distribution, constraint, name)\nnothing # hide\n\nor\n\nprior_dict = Dict(\"distribution\" => distribution, \"constraint\" => constraint, \"name\" => name)\nprior = ParameterDistribution(prior_dict)\nnothing # hide\n\nThe pdf of the Normal distribution and its transform to the physical, constrained space are:\n\np = plot(p1, p2, legend=false, size = (900, 450)) #hide","category":"section"},{"location":"parameter_distributions/#samples-example","page":"Prior distributions","title":"Sample-based distribution","text":"We repeat the work of Simple example revisited, but now assuming that to create our prior, we only have samples given by the histogram:\n\np = plot(p3, legend=false, size = (450, 450)) #hide\n\nImagine we do not know this distribution is bounded. To create a ParameterDistribution one can take a matrix constrained_samples whose columns are this data:\n\nusing EnsembleKalmanProcesses.ParameterDistributions # for `Samples`, `no_constraint`, `ParameterDistribution`, `bounded`\nconstrained_samples = [0.1 0.2 0.3 0.4] # hide\ndistribution = Samples(constrained_samples)\nconstraint = no_constraint()\nname = \"point_seven\"\nprior = ParameterDistribution(distribution, constraint, name)\nnothing # hide\n\nnote: Note\nThis naive implementation will not enforce any boundaries during the algorithm implementation.\n\nImagine that we know about the boundedness of this distribution, then, as in Simple example revisited, we define the constraint\n\nconstraint = bounded(0, 1)\nnothing # hide\n\nwhich stores the transformation:\n\nunconstrained_samples = constraint.constrained_to_unconstrained.(constrained_samples)\nnothing # hide\n\nThis maps the samples into an unbounded space, giving the following histogram:\n\np = plot(p3, legend=false, size = (450, 450)) #hide\n\nAs before we define a Samples distribution from matrix whose columns are the (now unconstrained) samples, along with a name to create the ParameterDistribution.\n\ndistribution = Samples(unconstrained_samples)\nname = \"point_seven\"\nprior = ParameterDistribution(distribution, constraint, name)\nnothing # hide","category":"section"},{"location":"parameter_distributions/#Example-combining-several-distributions","page":"Prior distributions","title":"Example combining several distributions","text":"To show how to combine priors in a more complex setting (e.g. for an entire parametrized process), we create a 25-dimensional parameter distribution from three dictionaries.\n\nBring in the packages!\n\nusing EnsembleKalmanProcesses.ParameterDistributions\n# for `bounded_below`, `bounded`, `Constraint`, `no_constraint`,\n#     `Parameterized`, `Samples`,`VectorOfParameterized`,\n#     `ParameterDistribution`, `combine_distributions`\nusing LinearAlgebra  # for `SymTridiagonal`, `Matrix`\nusing Distributions # for `MvNormal`, `Beta`\nnothing # hide\n\nThe first parameter is a 3-dimensional distribution, with the following bound constraints on parameters in physical space:\n\nc1 = repeat([bounded_below(0)], 3)\nnothing # hide\n\nWe know that a multivariate normal represents its distribution in the transformed (unbounded) space. Here we take a tridiagonal covariance matrix.\n\ndiagonal_val = 0.5 * ones(3)\nudiag_val = 0.25 * ones(2)\nmean = ones(3)\ncovariance = Matrix(SymTridiagonal(diagonal_val, udiag_val))\nd1 = Parameterized(MvNormal(mean, covariance)) # 3D multivariate normal\nnothing # hide\n\nWe also provide a name\n\nname1 = \"constrained_mvnormal\"\nnothing # hide\n\nThe second parameter is a 2-dimensional one. It is only given by 4 samples in the transformed space - (where one will typically generate samples). It is bounded in the first dimension by the constraint shown, there is a user provided transform for the second dimension - using the default constructor.\n\nd2 = Samples([1.0 5.0 9.0 13.0; 3.0 7.0 11.0 15.0]) # 4 samples of 2D parameter space\n\ntransform = (x -> 3 * x + 14)\njac_transform = (x -> 3)\ninverse_transform = (x -> (x - 14) / 3)\nabstract type Affine <: ConstraintType end\n\nc2 = [bounded(10, 15),\n      Constraint{Affine}(transform, jac_transform, inverse_transform, nothing)]\nname2 = \"constrained_sampled\"\nnothing # hide\n\nThe final parameter is 4-dimensional, defined as a list of i.i.d univariate distributions we make use of the VectorOfParameterized type\n\nd3 = VectorOfParameterized(repeat([Beta(2,2)],4))\nc3 = repeat([no_constraint()],4)\nname3 = \"Beta\"\nnothing # hide\n\nThe full prior distribution for this setting is created either through building simple distributions and combining\n\nu1 = ParameterDistribution(d1, c1, name1)\nu2 = ParameterDistribution(d2, c2, name2)\nu3 = ParameterDistribution(d3, c3, name3)\nu = combine_distributions( [u1, u2, u3])\nnothing # hide\n\nor an array of the parameter specifications as dictionaries.\n\nparam_dict1 = Dict(\"distribution\" => d1, \"constraint\" => c1, \"name\" => name1)\nparam_dict2 = Dict(\"distribution\" => d2, \"constraint\" => c2, \"name\" => name2)\nparam_dict3 = Dict(\"distribution\" => d3, \"constraint\" => c3, \"name\" => name3)\nu = ParameterDistribution([param_dict1, param_dict2, param_dict3])\nnothing # hide\n\nWe can visualize the marginals of the constrained distributions,\n\nusing Plots\nplot(u)\n\nand the unconstrained distributions similarly,\n\nusing Plots\nplot(u, constrained = false)","category":"section"},{"location":"parameter_distributions/#function-example","page":"Prior distributions","title":"Function Distribution Example","text":"Here, define a function parameter distribution on 01 times 12 , bounded by -5-3 and with correlation lengthscales 0.05. First, we get the packages:\n\nusing EnsembleKalmanProcesses.ParameterDistributions # For `ParameterDistribution`\nusing Random, Distributions # for `rand` and `Normal`\nusing Plots\n# We must `import` the GRF package, rather than call a `using` statement here\nimport GaussianRandomFields # for `GaussianRandomFields`\n\nthen, we use the  GaussianRandomFields.jl package to define the distribution of choice. This distribution is unbounded. Here we take a Matern kernel, and define our evaluation grid on the domain. We choose 30 degrees of freedom (dofs), so this function distribution is specified through the value of 30 learnable coefficients. \n\nconst GRF = GaussianRandomFields\n# Define a `GaussianRandomFields` object\ninput_dim = 2 # Define a 2D -> 1D function\ndofs = 30 # the number of modes defining the distribution\npoints = [collect(0:0.01:1), collect(1:0.02:2)] # the 2D domain grid (uniform in each dimension)\n\ngrfjl_obj = GRF.GaussianRandomField(\n   GRF.CovarianceFunction(input_dim, GRF.Matern(0.05, 2)),\n   GRF.KarhunenLoeve(dofs),\n   points...,\n) # the Gaussian Random Field object from the package\nnothing # hide\n\nWe define our parameter distribution wrapper, where GRFJL() indicates the GRF package used. We also impose bounds into an interval -5-3 (here applied to the output space).\n\ngrf = GaussianRandomFieldInterface(grfjl_obj, GRFJL()) # our wrapper\npd = ParameterDistribution(\n    Dict(\n        \"distribution\" => grf,\n        \"constraint\" => bounded(-5, -3), \n        \"name\" => \"func_in_min5_min3\",\n    )\n) # The ParameterDistribution with constraint in the output space\nnothing # hide\n\nWe plot 4 samples of this distribution. Samples are taken over the (30-dimensional) degrees of freedom, and then we apply the transform_unconstrained_to_costrained map to (i) build the function distribution, (ii) evaluate it on the numerical grid, and (iii) constrain the output with our prescribed bounds.\n\nshape = [length(pp) for pp in points]\nsamples_constrained_flat = [transform_unconstrained_to_constrained(pd, rand(Normal(0,1), dofs)) for i = 1:4] \nplts = [contour(points..., reshape(samples_constrained_flat[i], shape...)', fill = true,) for i =1:4]\nplot(plts..., legend=false, size=(800,800)) ","category":"section"},{"location":"parameter_distributions/#ConstraintType-Examples","page":"Prior distributions","title":"ConstraintType Examples","text":"For each for the predefined ConstraintTypes, we present animations of the resulting constrained prior distribution for\n\nusing EnsembleKalmanProcesses.ParameterDistributions, Distributions # hide\nμ = 0 # hide\nσ = 1 # hide\ndistribution = Parameterized(Normal(μ, σ))\nnothing # hide\n\nwhere we vary μ and σ respectively. As noted above, in the presence of a nontrivial constraint, μ and σ will no longer correspond to the mean and standard deviation of the prior distribution (which is taken in the physical, constrained space).","category":"section"},{"location":"parameter_distributions/#Without-constraints:-\"constraint\"-no_constraints()","page":"Prior distributions","title":"Without constraints: \"constraint\" => no_constraints()","text":"The following specifies a prior based on an unconstrained Normal(0.5, 1) distribution:\n\nusing EnsembleKalmanProcesses.ParameterDistributions # for `Parameterized`, `no_constraint`, `ParameterDistribution`\nusing Distributions # for `Normal`\n\nparam_dict = Dict(\n\"distribution\" => Parameterized(Normal(0.5, 1)),\n\"constraint\" => no_constraint(),\n\"name\" => \"unbounded_parameter\",\n)\n\nprior = ParameterDistribution(param_dict)\nnothing # hide\n\nwhere no_constraint() automatically defines the identity constraint map\n\ntransform_unconstrained_to_constrained(x) = x\nnothing # hide\n\nThe following plots show the effect of varying μ and σ in the constrained space (which is trivial here):\n\ngif(anim_unbounded, \"anim_unbounded.gif\", fps = 5) # hide","category":"section"},{"location":"parameter_distributions/#Bounded-below-by-0:-\"constraint\"-bounded_below(0)","page":"Prior distributions","title":"Bounded below by 0: \"constraint\" => bounded_below(0)","text":"The following specifies a prior for a parameter which is bounded below by 0 (i.e. its only physical values are positive), and which has a Normal(0.5, 1) distribution in the unconstrained space:\n\nusing EnsembleKalmanProcesses.ParameterDistributions # for `Parameterized`, `bounded_below`, `ParameterDistribution`\nusing Distributions # for `Normal`\n\nparam_dict = Dict(\n\"distribution\" => Parameterized(Normal(0.5, 1)),\n\"constraint\" => bounded_below(0),\n\"name\" => \"bounded_below_parameter\",\n)\n\nprior = ParameterDistribution(param_dict)\nnothing # hide\n\nwhere bounded_below(0) automatically defines the constraint map\n\ntransform_unconstrained_to_constrained(x) = exp(x)\nnothing # hide\n\nThe following plots show the effect of varying μ and σ in the physical, constrained space:\n\ngif(anim_bounded_below, \"anim_bounded_below.gif\", fps = 5) # hide","category":"section"},{"location":"parameter_distributions/#Bounded-above-by-10.0:-\"constraint\"-bounded_above(10)","page":"Prior distributions","title":"Bounded above by 10.0: \"constraint\" => bounded_above(10)","text":"The following specifies a prior for a parameter which is bounded above by ten, and which has a Normal(0.5, 1) distribution in the unconstrained space:\n\nusing EnsembleKalmanProcesses.ParameterDistributions # for `Parameterized`, `bounded_above`, `ParameterDistribution`\nusing Distributions\n\nparam_dict = Dict(\n\"distribution\" => Parameterized(Normal(0.5, 1)),\n\"constraint\" => bounded_above(10),\n\"name\" => \"bounded_above_parameter\",\n)\nprior = ParameterDistribution(param_dict)\nnothing # hide\n\nwhere bounded_above(10) automatically defines the constraint map\n\ntransform_unconstrained_to_constrained(x) = 10 - exp(-x)\nnothing # hide\n\nThe following plots show the effect of varying μ and σ in the physical, constrained space:\n\ngif(anim_bounded_above, \"anim_bounded_above.gif\", fps = 5) # hide","category":"section"},{"location":"parameter_distributions/#Bounded-between-5-and-10:-\"constraint\"-bounded(5,-10)","page":"Prior distributions","title":"Bounded between 5 and 10: \"constraint\" => bounded(5, 10)","text":"The following specifies a prior for a parameter whose physical values lie in the range between 5 and 10, and which has a Normal(0.5, 1) distribution in the unconstrained space:\n\nusing EnsembleKalmanProcesses.ParameterDistributions# for `Parameterized`, `bounded`, `ParameterDistribution`\nusing Distributions # for `Normal`\n\nparam_dict = Dict(\n\"distribution\" => Parameterized(Normal(0.5, 1)),\n\"constraint\" => bounded(5, 10),\n\"name\" => \"bounded_parameter\",\n)\n\nprior = ParameterDistribution(param_dict)\nnothing # hide\n\nwhere bounded(-1, 5) automatically defines the constraint map\n\ntransform_unconstrained_to_constrained(x) = 10 - 5 / (exp(x) + 1)\nnothing # hide\n\nThe following plots show the effect of varying μ and σ in the physical, constrained space:\n\ngif(anim_bounded, \"anim_bounded.gif\", fps = 10) # hide","category":"section"},{"location":"internal_data_representation/#Wrapping-up-data","page":"Internal data representation","title":"Wrapping up data","text":"To provide a consistent form for data (such as observations, parameter ensembles, model evaluations) across the package, we store the data in simple wrappers internally.\n\nData is always stored as columns of AbstractMatrix. That is, we obey the format\n\n[ data dimension x number of data samples ]","category":"section"},{"location":"internal_data_representation/#The-DataContainer","page":"Internal data representation","title":"The DataContainer","text":"A DataContainer is constructed initially by copying and perhaps transposing matrix data\n\ndc = DataContainer(abstract_matrix; data_are_columns = true)\n\nnote: Note\nProviding an n-vector will be interpreted as a [1xn] matrix\n\nThe flag data_are_columns indicates whether the provided data is stored column- or row-wise. The data is retrieved with\n\nget_data(dc)","category":"section"},{"location":"internal_data_representation/#The-PairedDataContainer","page":"Internal data representation","title":"The PairedDataContainer","text":"A PairedDataContainer stores pairs of inputs and outputs in the form of DataContainers. It is constructed from two data matrices, or from two DataContainers.\n\npdc = PairedDataContainer(input_matrix, output_matrix; data_are_columns = true)\npdc = PairedDataContainer(input_data_container, output_data_container)\n\nnote: Note\nProviding inputs and outputs of different eltypes will lead to conversion of all data into the eltype: T = promote_type(eltype(in_mat), eltype(out_mat))\n\nData is retrieved with\n\nget_data(pdc) # returns both inputs and outputs\nget_inputs(pdc)\nget_outputs(pdc)","category":"section"},{"location":"learning_rate_scheduler/#learning-rate-schedulers","page":"Learning rate schedulers","title":"Learning Rate Schedulers (a.k.a) Timestepping","text":"","category":"section"},{"location":"learning_rate_scheduler/#Overview","page":"Learning rate schedulers","title":"Overview","text":"We demonstrate the behaviour of different learning rate schedulers through solution of a nonlinear inverse problem.\n\nIn this example we have a model that produces the exponential of a sinusoid f(A v) = exp(A sin(t) + v) forall t in 02pi. Given an initial guess of the parameters as A^* sim mathcalN(21) and v^* sim mathcalN(025), the inverse problem is to estimate the parameters from a noisy observation of only the maximum and mean value of the true model output.\n\nWe shall compare the following configurations of implemented schedulers. \n\nFixed, \"long\" step DefaultScheduler(0.5) - orange\nFixed, \"short\" step DefaultScheduler(0.02) - green\nAdaptive timestep (designed originally to ensure EKS remains stable) EKSStableScheduler() Kovachki & Stuart 2018 - red\nAdaptive misfit-controlling step (for finite-time algorithms, terminating at T=1) DataMisfitController(terminate_at=1) Iglesias & Yang 2021 - purple\nAdaptive misfit-controlling step (continuation beyond terminate condition) DataMisfitController(on_terminate=\"continue\") - brown\n\ninfo: Recommended Scheduler\nFor typical problems we provide a default scheduler depending on the process. For example, when constructing an Inversion()-type EnsembleKalmanProcess, by default this effectively adds the schedulerscheduler = DataMisfitController(terminate_at = 1) # adaptive step-sizestop at algorithm time \"T=1\"\n\nTo modify the scheduler, use the keyword argument\n\nekpobj = EKP.EnsembleKalmanProcess(args...; scheduler = scheduler, kwargs...)\n\nSeveral other choices are available:\n\nscheduler = MutableScheduler(2) # modifiable stepsize at each iteration with default \"2\"\nscheduler = EKSStableScheduler(numerator=10.0, nugget = 0.01) # Stable for EKS\nscheduler = DataMisfitController(terminate_at = 1000) # stop at algorithm time \"T=1000\"\n\nPlease see the learning rate schedulers API for defaults and other details","category":"section"},{"location":"learning_rate_scheduler/#early-terminate","page":"Learning rate schedulers","title":"Early termination (with adaptive learning rate)","text":"When using an adaptive learning rate, early termination may be triggered when a scheduler-specific condition is satisfied prior to the final user prescribed N_iter. See how to set the termination condition for such schedulers in the API documentation. When triggered, early termination is returns a not-nothing value from update_ensembe!( and can be integrated into the calibration loop as follows\n\nusing EnsembleKalmanProcesses # for get_ϕ_final, update_ensemble!\n# given\n# * the number of iterations `N_iter`\n# * a prior `prior`\n# * a forward map `G`\n# * the EKP object `ekpobj`\n\nfor i in 1:N_iter\n    params_i = get_ϕ_final(prior, ekpobj)\n    g_ens = G(params_i)\n    terminated = update_ensemble!(ekpobj, g_ens) # check for termination\n    if !isnothing(terminated) # if termination is flagged, break the loop\n       break\n    end\nend \n\nNote - in this loop the final iteration may be less than N_iter.","category":"section"},{"location":"learning_rate_scheduler/#Timestep-and-termination-time","page":"Learning rate schedulers","title":"Timestep and termination time","text":"Recall, for example for EKI, we perform updates of our ensemble of parameters j=1dotsJ at step n = 1dotsN_mathrmit using\n\ntheta_n+1^(j) = theta_n^(j) - dfracDelta t_nJsum_k=1^J left langle mathcalG(theta_n^(k)) - barmathcalG_n    Gamma_y^-1 left ( mathcalG(theta_n^(j)) - y right ) right rangle theta_n^(k)\n\nwhere barmathcalG_n is the mean value of mathcalG(theta_n) across ensemble members. We denote the current time t_n = sum_i=1^nDelta t_i, and the termination time as T = t_N_mathrmit.\n\nnote: Note\nAdaptive Schedulers typically try to make the biggest update that controls some measure of this update. For example, EKSStableScheduler() controls the frobenius norm of the update, while DataMisfitController() controls the Jeffrey divergence between the two steps. Largely they follow a pattern of scheduling very small initial timesteps, leading to much larger steps at later times.\n\nThere are two termination times that the theory indicates are useful\n\nT=1: In the linear Gaussian case, the theta_N_mathrmit will represent the posterior distribution. In nonlinear case it should still provide an approximation to the posterior distribution. Note that as the posterior does not necessarily optimize the data-misfit we find bartheta_N_mathrmit (the ensemble mean) provides a conservative estimate of the true parameters, while retaining spread. It is noted in Iglesias & Yang 2021 that with small enough (or well chosen) step-sizes this estimate at T=1 satisfies a discrepancy principle with respect to the observational noise.\nTto infty: Though theoretical concerns have been made with respect to continuation beyond T=1 for inversion methods such as EKI, in practice we commonly see better optimization of the data-misfit, and thus better representation bartheta_N_mathrmit to the true parameters. As expected this procedure leads to ensemble collapse, and so no meaningful information can be taken from the posterior spread, and the optimizer is not likely to be the posterior mode.","category":"section"},{"location":"learning_rate_scheduler/#The-experiment-with-EKI-and-UKI","page":"Learning rate schedulers","title":"The experiment with EKI & UKI","text":"We assess the schedulers by solving the inverse problem with EKI and UKI (we average results over 100 initial ensembles in the case of EKI). We will not draw comparisons between EKI and UKI here, rather we use them to observe consistent behavior in the schedulers. Shown below are the solution plots of one solve with each timestepper, for both methods. \n\n(Image: Solution EKI) (Image: Solution UKI)\n\nTop: EKI, Bottom: UKI. Left: The true model over 02pi (black), and solution schedulers (colors). Right: The noisy observation (black) of mean and max of the model; the distribution it was sampled from (gray-ribbon), and the corresponding ensemble-mean approximation given from each scheduler (colors).\n\nTo assess the timestepping we show the convergence plot against the algorithm iteration we measure two quantities.\n\nerror (solid) is defined by frac1N_enssum^N_ens_i=1  theta_i - theta^* ^2 where theta_i are ensemble members and theta^* is the true value used to create the observed data.\nspread (dashed) is defined by frac1N_enssum^N_ens_i=1  theta_i - bartheta ^2 where theta_i are ensemble members and bartheta is the mean over these members.\n\n(Image: Error vs spread EKI) (Image: Error vs spread UKI)\n\nTop: EKI. Bottom: UKI. Left: the error and spread of the different timesteppers at over iterations of the algorithm for a single run. Right: the error and spread of the different timesteppers at their final iterations, (for EKI, averaged from 100 initial conditions).\n\nFinding the Posterior (terminating at T=1):\n\nDMC with termination (purple), closely mimics a small-fixed timestep (green) that finishes stepping at T=1. Both retain more spread than other approaches, and DMC is far more efficient, typically terminating after around 10-20 steps, where fixed-stepping takes 50. We see that (for this experiment) this is a conservative estimate, as continuing to solve (e.g. brown) until later times often leads to a better error while still retaining similar \"error vs spread\" curves (before inevitable collapse). This is consistent with the concept of approximating the posterior, over seeking an optimizer.\nThe behavior observed in UKI is similar to EKI\n\nOptimizing the objective function (continuing T to infty):\n\nLarge fixed step (orange). This is very efficient, but can get stuck when drawn too large, (perhaps unintuitive from a gradient-descent perspective). It typically also collapses the ensemble. On average it gives lower error to the true parameters than DMC. \nBoth EKSStable and DMC with continuation schedulers, perform very similarly. Both retain good ensemble spread during convergence, and collapse after finding a local optimum. This optimum on average has the best error to the true parameters in this experiment. They appear to consistently find the same optimum as Ttoinfty but DMC finds this in fewer iterations.\nThe UKI behavior is largely similar to EKI here, except that ensemble spread is retained in the Ttoinfty limit in all cases, from inflation of the parameter covariance (Sigma_omega) within our implementation.\n\nwarning: EnsembleKalmanSampler\nWe observe blow-up in EKS, when not using the EKSStableScheduler.","category":"section"},{"location":"examples/darcy/#Learning-the-permeability-field-in-a-Darcy-flow","page":"Darcy flow","title":"Learning the permeability field in a Darcy flow","text":"info: How do I run this code?\nThe full code is found in the examples/ directory of the github repository\n\nIn this example, we illustrate a simple function learning problem. We are presented with an unknown field that is discretized with a finite-dimensional approximation (e.g. spatial discretization). When learning this field, if one represents each pointwise value at a gridpoint as a parameter, increasing the spatial resolution leads to increasingly high dimensional learning problems, thus giving poor computational scaling and increasingly ill-posed inverse problems from fixed data. If instead, we treat the approximation as a discretized function living in a function space, then one can learn coefficients of a basis of this function space. Since it is commonly the case that functions have relatively low effective dimension in this space, the dependence on the spatial discretization only arises in discretization error, which vanishes as resolution is increased.\n\nWe will solve for an unknown permeability field kappa governing the pressure field of a Darcy flow on a square 2D domain. To learn about the permeability we shall take few pointwise measurements of the solved pressure field within the domain. The forward solver is a simple finite difference scheme taken and modified from code here.","category":"section"},{"location":"examples/darcy/#Walkthrough-of-the-code","page":"Darcy flow","title":"Walkthrough of the code","text":"First we load standard packages,\n\nusing LinearAlgebra\nusing Distributions\nusing Random\nusing JLD2\n\nthe package to define the function distributions,\n\nimport GaussianRandomFields # we wrap this so we don't want to use \"using\"\nconst GRF = GaussianRandomFields\n\nand finally the EKP packages.\n\nusing EnsembleKalmanProcesses\nusing EnsembleKalmanProcesses.ParameterDistributions\nconst EKP = EnsembleKalmanProcesses\n\nWe include the forward solver here.\n\ninclude(\"GModel.jl\")\n\nWe define the spatial domain and discretization,\n\nrng = Random.MersenneTwister(seed)\ndim = 2\nN, L = 80, 1.0\npts_per_dim = LinRange(0, L, N)\n\nTo provide a simple test case, we assume that the true function parameter is a particular sample from the function space we set up to define our prior. We choose a value of the truth that doesnt have a vanishingly small probability under the prior defined by a probability distribution over functions; taken to be a family of Gaussian Random Fields (GRF). This function distribution is characterized by a covariance function (Matern) and an appropriate representation (Karhunen-Loeve expansion). The representation is truncated to a finite number of coefficients, the degrees of freedom (dofs), which define the effective dimension of the learning problem that is decoupled from the spatial discretization. Larger dofs may be required to represent multiscale functions, but come at an increased dimension of the parameter space and therefore a typical increase in cost and difficulty of the learning problem. For more details see GaussianRandomFields.jl\n\nsmoothness = 2.0\ncorr_length = 0.5\ndofs = 50\n\ngrf = GRF.GaussianRandomField(\n    GRF.CovarianceFunction(dim, GRF.Matern(smoothness, corr_length)),\n    GRF.KarhunenLoeve(dofs),\n    pts_per_dim,\n    pts_per_dim,\n)\n\nWe define a wrapper around the GRF, and as the permeability field must be positive we introduce a domain constraint into the function distribution. \n\npkg = GRFJL()\ndistribution = GaussianRandomFieldInterface(grf, pkg) # our wrapper from EKP\ndomain_constraint = bounded_below(0) # make κ positive\npd = ParameterDistribution(\n    Dict(\"distribution\" => distribution, \"name\" => \"kappa\", \"constraint\" => domain_constraint),\n) # the fully constrained parameter distribution\n\nHenceforth, the GRF is interfaced in the same manner as any other parameter distribution with regards to interface. We choose the true value by setting all degrees of freedom u_mathrmtrue = -15; this choice is arbitrary, upto not having a vanishingly small mass under the prior. We then use the EKP transform function to build the corresponding instance of the kappa_mathrmtrue.\n\nu_true = -1.5 * ones(dofs,1) # the truth parameter\nκ_true = transform_unconstrained_to_constrained(pd, u_true) # builds and constrains the function.\nκ_true = reshape(κ_true, N, N)\n\nWe generate the data sample for the truth in a perfect model setting by evaluating the the model here, and observing the pressure field at a few subsampled points in each dimension (here obs_ΔN, samples every 10 points in each dimension, leading to a 7 times 7 observation grid), and we assume 5% additive observational noise on the measurements.\n\nobs_ΔN = 10 \ndarcy = Setup_Param(pts_per_dim, obs_ΔN, κ_true) \nh_2d = solve_Darcy_2D(darcy, κ_true)\ny_noiseless = compute_obs(darcy, h_2d)\nobs_noise_cov = 0.05^2 * I(length(y_noiseless)) * (maximum(y_noiseless) - minimum(y_noiseless))\ntruth_sample = vec(y_noiseless + rand(rng, MvNormal(zeros(length(y_noiseless)), obs_noise_cov)))\n\nNow we set up the Bayesian inversion algorithm. The prior we have already defined to construct our truth\n\nprior = pd\n\nWe define some algorithm parameters, here we take ensemble members larger than the dimension of the parameter space to ensure a full rank ensemble covariance.\n\nN_ens = dofs + 2 # number of ensemble members\nN_iter = 20 # number of EKI iterations\n\nWe sample the initial ensemble from the prior, and create the EKP object as an EKI algorithm using the Inversion() keyword, we also use the DataMisfitController() learning rate scheduler\n\ninitial_params = construct_initial_ensemble(rng, prior, N_ens) \nekiobj = EKP.EnsembleKalmanProcess(initial_params, truth_sample, obs_noise_cov, Inversion(), scheduler=DataMisfitController())\n\nWe perform the inversion loop. Remember that within calls to get_ϕ_final the EKP transformations are applied, thus the ensemble that is returned will be the positively-bounded permeability field evaluated at all the discretization points. Each ensemble member is stored as a column and therefore for uses such as plotting one needs to reshape to the desired dimension. We allow for early termination when using the DataMisfitController scheudler.\n\nerr = zeros(N_iter)\nfor i in 1:N_iter\n    params_i = get_ϕ_final(prior, ekiobj)\n    g_ens = run_G_ensemble(darcy, params_i)\n    terminate = EKP.update_ensemble!(ekiobj, g_ens)\n    if !isnothing(terminate)\n        break\n    end\nend","category":"section"},{"location":"examples/darcy/#Inversion-results","page":"Darcy flow","title":"Inversion results","text":"We plot first the prior ensemble mean and pointwise variance of the permeability field, and also the pressure field solved with the ensemble mean. \n\n(Image: Darcy prior)\n\nNow we plot the final ensemble mean and pointwise variance of the permeability field, and also the pressure field solved with the ensemble mean.\n\n(Image: Darcy final)\n\nWe can compare this with the true permeability and pressure field:\n\n(Image: Darcy truth)","category":"section"},{"location":"glossary/#Glossary","page":"Glossary","title":"Glossary","text":"The following list includes the names and symbols of recurring concepts in EnsembleKalmanProcesses.jl. Some of these variables do not appear in the codebase, which relies on array programming for performance.  Contributions to the codebase require following this notational convention. Similarly, if you find inconsistencies in the documentation or codebase, please report an issue on GitHub.\n\nName Symbol (Theory/Docs) Symbol (Code)\nParameter vector, Parameters (unconstrained space) theta, u, mathcalT(phi) θ,u\nParameter vector, Parameters (physical / constrained space) phi, mathcalT^-1(theta) ϕ\nParameter vector size, Number of parameters p N_par\nEnsemble size J N_ens\nEnsemble particles, members theta^(j) \nNumber of iterations N_rm it N_iter\nObservation vector, Observations, Data vector y y\nObservation vector size, Data vector size d N_obs\nObservational noise eta obs_noise\nObservational noise covariance Gamma_y obs_noise_cov\nHilbert space inner product langle phi  Gamma^-1 psi rangle \nForward map mathcalG G\nDynamical model Psi Ψ\nTransform map (constrained to unconstrained) mathcalT T\nObservation map mathcalH H\nPrior covariance (unconstrained space) Gamma_theta prior_cov\nPrior mean (unconstrained space) m_theta prior_mean\n\nnote: On batching\nWhen observations or parameters are being batched, then their size (e.g., N_obs or N_par) will refer to the size of elements of each batch, summed over the batch. For example, when calibrating 100 observations of a 15-dimensional output space, but using a minibatching procedure with batch size 5, then N_obs = 75.","category":"section"},{"location":"API/Inversion/#Ensemble-Kalman-Inversion","page":"Inversion","title":"Ensemble Kalman Inversion","text":"","category":"section"},{"location":"API/Inversion/#EnsembleKalmanProcesses.Inversion","page":"Inversion","title":"EnsembleKalmanProcesses.Inversion","text":"Inversion <: Process\n\nAn ensemble Kalman Inversion process\n\n\n\n\n\n","category":"type"},{"location":"API/Inversion/#EnsembleKalmanProcesses.eki_update","page":"Inversion","title":"EnsembleKalmanProcesses.eki_update","text":" eki_update(\n    ekp::EnsembleKalmanProcess{FT, IT, Inversion},\n    u::AbstractMatrix{FT},\n    g::AbstractMatrix{FT},\n    y::AbstractMatrix{FT},\n    obs_noise_cov::Union{AbstractMatrix{CT}, UniformScaling{CT}},\n) where {FT <: Real, IT, CT <: Real}\n\nReturns the updated parameter vectors given their current values and the corresponding forward model evaluations, using the inversion algorithm from eqns. (4) and (5) of Schillings and Stuart (2017).\n\nLocalization is implemented following the ekp.localizer.\n\n\n\n\n\n","category":"function"},{"location":"examples/lorenz_example/#Lorenz-example","page":"Lorenz","title":"Lorenz 96 example","text":"info: How do I run this code?\nThe full code is found in the examples/ directory of the github repository","category":"section"},{"location":"examples/lorenz_example/#Overview","page":"Lorenz","title":"Overview","text":"The Lorenz 96 (hereafter L96) example is a toy-problem for the application of the EnsembleKalmanProcesses.jl optimization and approximate uncertainty quantification methodologies. Here is L96 with additional periodic-in-time forcing, we try to determine parameters (sinusoidal amplitude and stationary component of the forcing) from some output statistics. The standard L96 equations are implemented with an additional forcing term with time dependence. The output statistics which are used for learning are the finite time-averaged variances.","category":"section"},{"location":"examples/lorenz_example/#Lorenz-96-equations","page":"Lorenz","title":"Lorenz 96 equations","text":"The standard single-scale L96 equations are implemented. The Lorenz 96 system (Lorenz, 1996) is given by \n\nfracd x_id t = (x_i+1 - x_i-2) x_i-1 - x_i + F\n\nwith i indicating the index of the given longitude. The number of longitudes is given by N. The boundary conditions are given by\n\nx_-1 = x_N-1  x_0 = x_N  x_N+1 = x_1\n\nThe time scaling is such that the characteristic time is 5 days (Lorenz, 1996).  For very small values of F, the solutions x_i decay to F after the initial transient feature. For moderate values of F, the solutions are periodic, and for larger values of F, the system is chaotic. The solution variance is a function of the forcing magnitude. Variations in the base state as a function of time can be imposed through a time-dependent forcing term F(t).\n\nA temporal forcing term is defined\n\nF = F_s + A sin(omega t)\n\nwith steady-state forcing F_s, transient forcing amplitude A, and transient forcing frequency omega. The total forcing F must be within the chaotic regime of L96 for all time given the prescribed N.\n\nThe L96 dynamics are solved with RK4 integration.","category":"section"},{"location":"examples/lorenz_example/#Structure","page":"Lorenz","title":"Structure","text":"The main code is located in Lorenz_example.jl which provides the functionality to run the L96 dynamical system, extract time-averaged statistics from the L96 states, and use the time-average statistics for optimization and uncertainty quantification.\n\nThe L96 system is solved in GModel.jl according to the time integration settings specified in LSettings and the L96 parameters specified in LParams. The types of statistics to be collected are detailed in GModel.jl.","category":"section"},{"location":"examples/lorenz_example/#Lorenz-dynamics-inputs","page":"Lorenz","title":"Lorenz dynamics inputs","text":"","category":"section"},{"location":"examples/lorenz_example/#Dynamics-settings","page":"Lorenz","title":"Dynamics settings","text":"The use of the transient forcing term is with the flag, dynamics. Stationary forcing is dynamics=1 (A=0) and transient forcing is used with dynamics=2 (Aneq0). The default parameters are specified in Lorenz_example.jl and can be modified as necessary. The system is solved over time horizon 0 to tend at fixed time step dt.\n\nN = 36\ndt = 1/64\nt_start = 800","category":"section"},{"location":"examples/lorenz_example/#Inverse-problem-settings","page":"Lorenz","title":"Inverse problem settings","text":"The states are integrated over time Ts_days to construct the time averaged statistics for use by the optimization. The specification of the statistics to be gathered from the states are provided by stats_type. The Ensemble Kalman Process (EKP) settings are\n\nN_ens = 20 # number of ensemble members\nN_iter = 5 # number of EKI iterations","category":"section"},{"location":"examples/lorenz_example/#Setting-up-the-Inverse-Problem","page":"Lorenz","title":"Setting up the Inverse Problem","text":"The goal is to learn F_s and A based on the time averaged statistics in a perfect model setting. The true parameters are\n\nF_true = 8. # Mean F\nA_true = 2.5 # Transient F amplitude\nω_true = 2. * π / (360. / τc) # Frequency of the transient F\nparams_true = [F_true, A_true]\nparam_names = [\"F\", \"A\"]","category":"section"},{"location":"examples/lorenz_example/#Priors","page":"Lorenz","title":"Priors","text":"We implement (biased) priors as follows\n\nprior_means = [F_true + 1.0, A_true + 0.5]\nprior_stds = [2.0, 0.5 * A_true]\n# constrained_gaussian(\"name\", desired_mean, desired_std, lower_bd, upper_bd)\nprior_F = constrained_gaussian(param_names[1], prior_means[1], prior_stds[1], 0, Inf)\nprior_A = constrained_gaussian(param_names[2], prior_means[2], prior_stds[2], 0, Inf)\npriors = combine_distributions([prior_F, prior_A])\n\nWe use the recommended constrained_gaussian to add the desired scale and bounds to the prior distribution, in particular we place lower bounds to preserve positivity. ","category":"section"},{"location":"examples/lorenz_example/#Observational-Noise","page":"Lorenz","title":"Observational Noise","text":"The observational noise can be generated using the L96 system or prescribed, as specified by var_prescribe. \n\nvar_prescribe==false The observational noise is constructed by generating independent instantiations of the L96 statistics of interest at the true parameters for different initial conditions. The empirical covariance matrix is constructed.\n\nvar_prescribe==true The observational noise is prescribed as a Gaussian distribution with prescribed mean and variance.","category":"section"},{"location":"examples/lorenz_example/#Running-the-Example","page":"Lorenz","title":"Running the Example","text":"The L96 parameter estimation can be run using julia --project Lorenz_example.jl","category":"section"},{"location":"examples/lorenz_example/#Solution-and-Output","page":"Lorenz","title":"Solution and Output","text":"The output will provide the estimated parameters in the constrained ϕ-space. The priors are required in the get-method to apply these constraints.","category":"section"},{"location":"examples/lorenz_example/#Printed-output","page":"Lorenz","title":"Printed output","text":"# EKI results: Has the ensemble collapsed toward the truth?\nprintln(\"True parameters: \")\nprintln(params_true)\nprintln(\"\\nEKI results:\")\nprintln(get_ϕ_mean_final(priors, ekiobj))","category":"section"},{"location":"examples/lorenz_example/#Saved-output","page":"Lorenz","title":"Saved output","text":"The parameters and forward model outputs will be saved in parameter_storage.jld2 and data_storage.jld2, respectively. The data will be saved in the directory output.","category":"section"},{"location":"examples/lorenz_example/#Plots","page":"Lorenz","title":"Plots","text":"A scatter plot animation of the ensemble convergence to the true parameters is saved in the directory output.","category":"section"},{"location":"literated/sinusoid_example/#sinusoid-example","page":"Simple example","title":"Fitting parameters of a sinusoid","text":"info: How do I run this code?\nThe full code is found in the examples/ directory of the github repository\n\nIn this example we have a model that produces a sinusoid f(A v) = A sin(phi + t) + v forall t in 02pi, with a random phase phi. Given an initial guess of the parameters as A^* sim mathcalN(21) and v^* sim mathcalN(025), our goal is to estimate the parameters from a noisy observation of the maximum, minimum, and mean of the true model output.\n\nFirst, we load the packages we need:\n\nusing LinearAlgebra, Random\n\nusing Distributions, Plots\n\nusing EnsembleKalmanProcesses\nusing EnsembleKalmanProcesses.ParameterDistributions\nconst EKP = EnsembleKalmanProcesses\nnothing # hide\n\n# Setting up the model and data for our inverse problem\n\nNow, we define a model which generates a sinusoid given parameters theta: an amplitude and a vertical shift. We will estimate these parameters from data. The model adds a random phase shift upon evaluation.\n\ndt = 0.01\ntrange = 0:dt:(2 * pi + dt)\nfunction model(amplitude, vert_shift)\n    phi = 2 * pi * rand(rng)\n    return amplitude * sin.(trange .+ phi) .+ vert_shift\nend\nnothing # hide\n\nSeed for pseudo-random number generator.\n\nrng_seed = 41\nrng = Random.MersenneTwister(rng_seed)\nnothing # hide\n\nWe then define G(theta), which returns the observables of the sinusoid given a parameter vector. These observables should be defined such that they are informative about the parameters we wish to estimate. Here, the two observables are the y range of the curve (which is informative about its amplitude), as well as its mean (which is informative about its vertical shift).\n\nfunction G(u)\n    theta, vert_shift = u\n    sincurve = model(theta, vert_shift)\n    return [maximum(sincurve) - minimum(sincurve), mean(sincurve)]\nend\nnothing # hide\n\nSuppose we have a noisy observation of the true system. Here, we create a pseudo-observation y by running our model with the correct parameters and adding Gaussian noise to the output.\n\ndim_output = 2\n\nΓ = 0.1 * I\nnoise_dist = MvNormal(zeros(dim_output), Γ)\n\ntheta_true = [1.0, 7.0]\ny = G(theta_true) .+ rand(noise_dist)\nnothing # hide\n\n# Solving the inverse problem\n\nWe now define prior distributions on the two parameters. For the amplitude, we define a prior with mean 2 and standard deviation 1. It is additionally constrained to be nonnegative. For the vertical shift we define a Gaussian prior with mean 0 and standard deviation 5.\n\nprior_u1 = constrained_gaussian(\"amplitude\", 2, 1, 0, Inf)\nprior_u2 = constrained_gaussian(\"vert_shift\", 0, 5, -Inf, Inf)\nprior = combine_distributions([prior_u1, prior_u2])\nnothing # hide\n\nWe now generate the initial ensemble and set up the ensemble Kalman inversion. We define an ensemble size and we define a maximum iteration for this experiment (depending on the timestepper, early termination criteria can be used). We highlight that a key benefit of many variants of the Kalman approach over other particle methods, is that typically the ensemble size does not need to scale (e.g., linearly) with the number of parameters.\n\nN_ensemble = 20\nN_iterations = 10\n\ninitial_ensemble = EKP.construct_initial_ensemble(rng, prior, N_ensemble)\n\nensemble_kalman_process = EKP.EnsembleKalmanProcess(initial_ensemble, y, Γ, Inversion(); rng = rng)\nnothing # hide\n\nWe are now ready to carry out the inversion. At each iteration, we get the ensemble from the last iteration, apply G(theta) to each ensemble member, and apply the Kalman update to the ensemble. We also check if termination criteria has been exceeded, and break the loop if it has - on termination, the update is not performed.\n\nfor i in 1:N_iterations\n    params_i = get_ϕ_final(prior, ensemble_kalman_process)\n\n    G_ens = hcat([G(params_i[:, i]) for i in 1:N_ensemble]...)\n\n    terminate = EKP.update_ensemble!(ensemble_kalman_process, G_ens)\n    if !(isnothing(terminate))\n        @info \"Termination at iteration $(i-1)\"\n        break\n    end\nend\nnothing # hide\n\nFinally, we get the ensemble after the last iteration. This provides our estimate of the parameters.\n\nfinal_ensemble = get_ϕ_final(prior, ensemble_kalman_process)\n\nTo visualize the success of the inversion, we plot model with the true parameters, the initial ensemble, and the final ensemble.\n\np = plot(trange, model(theta_true...), c = :black, label = \"Truth\", legend = :bottomright, linewidth = 2)\n\nplot!(p, trange, [model(get_ϕ(prior, ensemble_kalman_process, 1)[:, 1]...)], c = :red, label = \"Initial ensemble\")\nplot!(p, trange, [model(final_ensemble[:, 1]...)], c = :blue, label = \"Final ensemble\")\nplot!(p, trange, [model(get_ϕ(prior, ensemble_kalman_process, 1)[:, i]...) for i in 2:N_ensemble], c = :red, label = \"\")\nplot!(p, trange, [model(final_ensemble[:, i]...) for i in 2:N_ensemble], c = :blue, label = \"\")\n\nxlabel!(\"Time\")\n\nWe see that the final ensemble is much closer to the truth. Note that the random phase shift is of no consequence.\n\nsavefig(p, \"output.png\")\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"localization/#localization","page":"Localization and SEC","title":"Localization and Sampling Error Correction (SEC)","text":"Ensemble Kalman inversion (EKI) seeks to find an optimal parameter vector theta in mathbbR^p by minimizing the mismatch between some data y in mathbbR^d and the forward model output mathcalG(theta) in mathbbR^d. Instead of relying on derivatives of the map mathcalG with respect to theta to find the optimum, EKI leverages sample covariances mathrmCov(theta mathcalG) and  mathrmCov(mathcalG mathcalG) diagnosed from an ensemble of J particles,\n\n   tag1\n   beginaligned\n         mathrmCov(theta mathcalG) = dfrac1Jsum_j=1^J\n        (theta^(j) - m)(mathcalG(theta^(j)) - barmathcalG)^T \n\n        mathrmCov(mathcalG mathcalG) = dfrac1Jsum_j=1^J\n        (mathcalG(theta^(j)) - barmathcalG)(mathcalG(theta^(j)) - barmathcalG)^T \n    endaligned\n\nwhere m and barmathcalG are the ensemble averages of theta and mathcalG(theta), respectively.\n\nFor models with just a few (p) parameters, we can typically afford to use J  p ensemble members, such that the sample covariance  mathrmCov(theta mathcalG) is full rank. Using more ensemble members than the number of model parameters, EKI can in theory probe all dimensions of parameter space to find the optimum parameter vector.\n\nFor models with a lot of parameters (e.g., a deep neural network), computational constraints limit the size of the ensemble to J  p or even J ll p members. Due to the characteristics of the EKI update equation, this means that the method can only find the minimum in the (J-1)-dimensional space spanned by the initial ensemble, leaving p-J+1 dimensions unexplored. This is known as the subspace property of EKI. As the dimensional gap p-J increases, we can expect the solution of the algorithm to deteriorate.","category":"section"},{"location":"localization/#Enter-Localization","page":"Localization and SEC","title":"Enter Localization","text":"In algebraic terms, the extent to which we can explore the dimensions of parameter space is roughly given by the rank of the matrices in equation (1). In the case J  p, the rank of mathrmCov(theta mathcalG) is limited to mathrmmin(d J-1). It has been shown that the performance of ensemble Kalman methods with J  p can be greatly improved by boosting this rank through an elementwise product with a suitable localization kernel Lambda,\n\ntag2 mathrmrank(mathrmCov(theta mathcalG) odot Lambda) geq mathrmrank(mathrmCov(theta mathcalG))\n\nSubstituting the covariance mathrmCov(theta mathcalG) by the boosted version defined in the left-hand side of equation (2), EKI is able to break the subspace property and explore additional dimensions in parameter space. Localization is an empirical way of correcting for the sampling error due to a small ensemble size, and so it can also be interpreted as a sampling error correction (SEC) method.","category":"section"},{"location":"localization/#Localization-in-EnsembleKalmanProcesses","page":"Localization and SEC","title":"Localization in EnsembleKalmanProcesses","text":"A wide variety of localization kernels are available in EnsembleKalmanProcesses.jl under the module Localizers. The optimal localization kernel will depend on the structure of the problem at hand, so the user is encouraged to try different kernels and review their references in the literature.\n\nIn practice, the localization method is chosen at EnsembleKalmanProcess construction time,\n\nusing Distributions\nusing LinearAlgebra\nusing EnsembleKalmanProcesses\nusing EnsembleKalmanProcesses.ParameterDistributions\nusing EnsembleKalmanProcesses.Localizers\nconst EKP = EnsembleKalmanProcesses\n\np = 10; d = 20; J = 6\n\n# Construct initial ensemble\npriors = ParameterDistribution[]\nfor i in 1:p\n   push!(priors, ParameterDistribution(Parameterized(Normal(0.0, 0.5)), no_constraint(), string(\"u\", i)))\nend\nprior = combine_distributions(priors)\ninitial_ensemble = EKP.construct_initial_ensemble(prior, J)\n\ny = 10.0 * rand(d)\nΓ = 1.0 * I\n\n# Construct EKP object with localization. Some examples of localization methods:\nlocs = [Delta(), RBF(1.0), RBF(0.1), BernoulliDropout(0.1), SEC(10.0), SECFisher(), SEC(1.0, 0.1), SECNice()]\nfor loc in locs\n   ekiobj = EKP.EnsembleKalmanProcess(initial_ensemble, y, Γ, Inversion(); localization_method = loc)\nend\n\nnote: Note\nCurrently Localization and SEC are implemented only for the Inversion() process, we are working on extensions to TransformInversion() ","category":"section"},{"location":"localization/#The-following-example-is-found-in-examples/Localization/localization_example_lorenz96.jl","page":"Localization and SEC","title":"The following example is found in examples/Localization/localization_example_lorenz96.jl","text":"This example was originally taken from Tong and Morzfeld (2022). Here, a single-scale lorenz 96 system state of dimension 200 is configured to be in a chaotic parameter regime, and integrated forward with timestep Delta t until time T. The goal is to perform ensemble inversion for the state at time T-20Delta t, given a noisy observation of the state at time T.\n\nTo perform this state estimation we use ensemble inversion with ensembles of size 20. This problem is severely rank-deficient, and we make up for this by imposing sampling-error correction methods to ensemble covariance matrices. Note that the SEC methods do not assume any spatial structure in the state, (differing from traditional state localization) and so are well suited for other types of inversions over parameter space.\n\n(Image: SEC_compared)\n\nnote: Our recommendation\nBased on these results, our recommendation is to use the SECNice() approach for the Inversion() process. Not only does it perform well, but additionally  it requires no tuning parameters, unlike for example SEC().","category":"section"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"This page documents ensemble Kalman inversion (EKI), as well as two variants, ensemble transform Kalman inversion (ETKI) and sparsity-inducing ensemble Kalman inversion (SEKI).","category":"section"},{"location":"ensemble_kalman_inversion/#eki","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"","category":"section"},{"location":"ensemble_kalman_inversion/#What-we-optimize,-and-types-of-solution","page":"Ensemble Kalman Inversion","title":"What we optimize, and types of solution","text":"One of the ensemble Kalman processes implemented in EnsembleKalmanProcesses.jl is ensemble Kalman inversion (Iglesias et al, 2013). Ensemble Kalman inversion (EKI) is a derivative-free ensemble optimization method that seeks to find the optimal parameters theta in mathbbR^p in the inverse problem defined by the data-model relation\n\ntag1 y = mathcalG(theta) + eta \n\nwhere mathcalG denotes the forward map, y in mathbbR^d is the vector of observations and eta  in mathbbR^d is additive noise. Note that p is the size of the parameter vector theta and d the size of the observation vector y. Here, we take eta sim mathcalN(0 Gamma_y) from a d-dimensional Gaussian with zero mean and covariance matrix Gamma_y.  This noise structure aims to represent the correlations between observations.\n\nThe optimal parameters theta^* in mathbbR^p (Maximum Likelihood Estimation) given relation (1) minimize the loss \n\nmathcalL(theta y) = frac12 left(y - mathcalG(theta)right)^top Gamma_y^-1 left(y - mathcalG(theta) right)\n\nwhich can be interpreted as the negative log-likelihood given a Gaussian likelihood.\n\nThis is acheived using process Inversion() and stepping to algorithm time T=infty. This form uses the prior like an initial condition.\n\nIf we using a prior to seek a Bayesian solution to our problem, (not just as initialization) then the optimal parameters theta^* in mathbbR^p (Maximum A Posteriori estimation) given relation (1) minimize the loss \n\nmathcalL(theta y) = frac12 left(y - mathcalG(theta)right)^top Gamma_y^-1 left(y - mathcalG(theta) right) + frac12(theta - m)^top C^-1(theta-m)\n\nwhich can be interpreted as the negative log-likelihood given a Gaussian likelihood and Gaussian prior N(mC). This is acheived in two ways:\n\nusing process Inversion() and terminating the iterations at algorithm time T=1 (default), \"finite-time variant\"\nusing process Inversion(prior) and stepping to T=infty.  \"infinite-time variant\"\n\nSee how these behave here","category":"section"},{"location":"ensemble_kalman_inversion/#The-EKI-update","page":"Ensemble Kalman Inversion","title":"The EKI update","text":"Denoting the parameter vector of the j-th ensemble member at the n-th iteration as theta^(j)_n, its update equation from n to n+1 under EKI is\n\ntag2 theta_j+1^(n) = theta_j^(n) + Delta t C^thetamathcalG_j(Gamma_y + Delta t C_j^mathcalGG)^-1(y - mathcalG(theta_j^(n)))\n\nWhere the notations for means and covariances are given as\n\nbeginaligned\n    C^thetamathcalG_j = frac1Nsum_n=1^N left (theta^(n)_j - overlinetheta_j)otimes(mathcalG(theta^(n)_j) - overline mathcalG(theta)_j) right\n    C^mathcalGG_j = frac1Nsum_n=1^N left(mathcalG(theta^(n)_j) - overline mathcalG(theta)_j)otimes(mathcalG(theta^(n)_j) - overline mathcalG(theta)_j) right\n    overlinetheta_j = frac1Nsum_n=1^N theta^(n)_jqquad overlinemathcalG(theta)_j = frac1N sum_n=1^NmathcalG(theta^(n)_j)\nendaligned\n\nThere is no difference between the Inversion() and Inversion(prior) updates, but the latter works with an augmented state (see here). The algorithmic timestep (a.k.a learning rate) Delta t is usually taken to be adaptive with a schedule, as described here.\n\nThe final estimate bartheta_N_rm it is taken to be the ensemble mean at the final iteration, \n\nbartheta_N_rm it = dfrac1Jsum_k=1^Jtheta_N_rm it^(k)\n\nFor typical applications, a near-optimal solution theta can be found after as few as 10 iterations of the algorithm, or 10cdot J evaluations of the forward model mathcalG. The rules of thumb of choosing J are see here, and to reduce errors when J ll p , we have sampling-error-correction (localization) approaches here. ","category":"section"},{"location":"ensemble_kalman_inversion/#Constructing-the-Forward-Map","page":"Ensemble Kalman Inversion","title":"Constructing the Forward Map","text":"The forward map mathcalG maps the space of unconstrained parameters theta in mathbbR^p to the space of outputs y in mathbbR^d. In practice, the user may not have access to such a map directly. Consider a situation where the goal is to learn a set of parameters phi of a dynamical model Psi mathbbR^p rightarrow mathbbR^o, given observations y in mathbbR^d and a set of constraints on the value of phi. Then, the forward map may be constructed as\n\nmathcalG = mathcalH circ Psi circ mathcalT^-1\n\nwhere mathcalH mathbbR^o rightarrow mathbbR^d is the observation map and mathcalT is the transformation map from constrained to unconstrained parameter spaces, such that mathcalT(phi) = theta. A family of standard transformation maps and their inverse are available in the ParameterDistributions module.","category":"section"},{"location":"ensemble_kalman_inversion/#Creating-the-EKI-Object","page":"Ensemble Kalman Inversion","title":"Creating the EKI Object","text":"An ensemble Kalman inversion object can be created using the EnsembleKalmanProcess constructor by specifying the Inversion() process type.\n\nThe EnsembleKalmanProcess then is built with and initial ensemble, observation and the process. The following utilities describe this\n\nusing EnsembleKalmanProcesses # for `construct_initial_ensemble`, `Inversion`, `Observation`\nusing EnsembleKalmanProcesses.ParameterDistributions # for `constrained_gaussian`\n\nprior = constrained_gaussian(\"4d-unit-gauss\", 0.0, 1.0, -Inf, Inf, repeats=4)\n\nJ = 50  # number of ensemble members\ninitial_ensemble = construct_initial_ensemble(prior, J) # Initialize ensemble from prior (unconstrained u-space)\n\n# data\nydim = 5\ny = ones(ydim)\ncov_y = 0.01*I\n\n# basic EKI, finite-time\nekiobj = EnsembleKalmanProcess(initial_ensemble, y, obs_noise_cov, Inversion())\n\n# fancier observation container, infinite-time, verbose i/o\ny_obs = Observation(\n    Dict(\n        \"samples\" => y,\n        \"covariances\" => cov_y,\n        \"names\" => \"descriptive_name\",\n        \"metadata\" => \"some imporant information\"\n    ),\n)\n\nekiobj = EnsembleKalmanProcess(initial_ensemble, y_obs, Inversion(prior), verbose=true)\n\nSee the Prior distributions section to learn about the construction of priors in EnsembleKalmanProcesses.jl. See the Observations section to learn about more complex observation construction and minibatching utilities. Note that the initial ensemble is in the unconstrained u space, apply transform_unconstrained_to_constrained(prior, initial_ensemble) to see the resulting constrained parameter ensemble.","category":"section"},{"location":"ensemble_kalman_inversion/#Updating-the-Ensemble","page":"Ensemble Kalman Inversion","title":"Updating the Ensemble","text":"Once the ensemble Kalman inversion object ekiobj has been initialized, any number of updates can be performed using the inversion algorithm.\n\nA call to the inversion algorithm can be performed with the update_ensemble! function. This function takes as arguments the ekiobj and the evaluations of the forward map at each member of the current ensemble. The update_ensemble! function then stores the new updated ensemble and the inputted forward map evaluations in ekiobj. \n\nA typical use of the update_ensemble! function given the ensemble Kalman inversion object ekiobj, the dynamical model Ψ and the observation map H is\n\n# Given:\n# Ψ (some black box simulator)\n# H (some observation of the simulator output)\n# prior (prior distribution and parameter constraints)\n\nN_iter = 20 # Number of steps of the algorithm\n\nfor n in 1:N_iter\n    ϕ_n = get_ϕ_final(prior, ekiobj) # Get current ensemble in constrained \"ϕ\"-space\n    G_n = [H(Ψ(ϕ_n[:, i])) for i in 1:J]\n    g_ens = hcat(G_n...) # Evaluate forward map \n    update_ensemble!(ekiobj, g_ens) # Update ensemble\nend\n\nIn the previous update, note that the parameters stored in ekiobj are given in the unconstrained Gaussian space where the EKI algorithm is performed. The map mathcalT^-1 between this unconstrained space and the (possibly constrained) physical space of parameters is encoded in the prior object. The dynamical model Ψ accepts as inputs the parameters in (possibly constrained) physical space, so it is necessary to use the getter get_ϕ_final which applies transform_unconstrained_to_constrained to the ensemble. See the Prior distributions section for more details on parameter transformations.   ","category":"section"},{"location":"ensemble_kalman_inversion/#Solution","page":"Ensemble Kalman Inversion","title":"Solution","text":"The EKI algorithm drives the initial ensemble, sampled from the prior, towards the support region of the posterior distribution. The algorithm also drives the ensemble members towards consensus. The optimal parameter θ_optim found by the algorithm is given by the mean of the last ensemble (i.e., the ensemble after the last iteration),\n\nθ_optim = get_u_mean_final(ekiobj) # optimal parameter\n\nTo obtain the optimal value in the constrained space, we use the getter with the constrained prior as input\n\nϕ_optim = get_ϕ_mean_final(prior, ekiobj) # the optimal physical parameter value","category":"section"},{"location":"ensemble_kalman_inversion/#finite-vs-infinite-time","page":"Ensemble Kalman Inversion","title":"Inversion() vs Inversion(prior)","text":"note: Finite-time vs infinite-time\nDeeper description of these algorithms is discussed in detail in, for example, Section 4.5 of Calvello, Reich, Stuart). Finite-time algorithms have also been called \"transport\" algorithms, and infinite-time algorithms are also known as prior-enforcing, or Tikhonov EKI Chada, Stuart, Tong.\n\nThus far, we have presented the finite-time algorithm Inversion(). The infinite-time variant Inversion(prior) algorithm has two key practical distinctions.\n\nThe initial distribution does not need to come from the prior. \nThe particle distribution mean converges to the maximum a-posteriori estimator as Tto infty (not via an early-termination condition)\n\nBoth implementations perform the same update; but in the infinite-time variant, the forward-map, data and noise-covariance are augmented by a Gaussian prior N(mC) by working with the following:\n\ntildemathcalG(theta) =  mathcalG(theta) theta qquad tildey = left y m right^top qquad tildeGamma_y = beginbmatrix Gamma_y  0  0  C endbmatrix\n\nIt is implemented as follows (here, for three parameters)\n\nusing EnsembleKalmanProcesses\nusing EnsembleKalmanProcesses.ParameterDistributions\n# given `y` `obs_noise_cov` and `prior`\n\nJ = 50  # number of ensemble members\ninitial_dist = constrained_gaussian(\"not-the-prior\", 0, 1, -Inf, Inf, repeats=3)\ninitial_ensemble = construct_initial_ensemble(inital_dist, J) # Initialize ensemble from prior\n\nekiobj = EnsembleKalmanProcess(initial_ensemble, y, obs_noise_cov, Inversion(prior))\n\nOne can see this in-action with the finite- vs infinite-time comparison example here, which was used to produce the plots below:\n\nLeft: Inversion (finite-time), Right: Inversion(prior) (infinite-time, initialized off-prior)\n\n<img src=\"../assets/animations/animated_inversion-finite.gif\" width=\"300\"> <img src=\"../assets/animations/animated_inversion-infinite.gif\" width=\"300\"> \n\nComparative behaviour. \n\nInitialization: Inversion() must be initialized from the prior, Inversion(prior) can still find the posterior when initialized off-prior. This might be useful when the prior is very broad and can enter, for example, regions of instability of the users forward model\nPrior information: Inversion() only contains prior information due to its initialization, Inversion(prior) enforces the prior at every iteration.\nSolution: Inversion() terminated at T=1 (implemented by default) obtains an accurate MAP estimate, the ensemble spread at exactly T=1 can represent a snapshot of the true (Gaussian-approximated) uncertainty. Inversion(prior) obtains this in the limit Ttoinfty, and undergoes collapse providing no uncertainty information.\nTrust in prior Inversion(), when iterated beyond T=1 will lose prior information and thus move to find the MLE (minimize the data-misfit only) at Ttoinfty, this behaviour might be useful if the prior information is missprecified.  \nEfficiency: Inversion() is more efficient that Inversion(prior) as enforcing the prior in the infinite-time algorithm is performed via extending the linear systems to be solved. Performance is also impacted (positively or negatively) by the choice of initial distribution in the Inversion(prior)\n\nOne can learn more about the early termination for finite-time algorithms here.","category":"section"},{"location":"ensemble_kalman_inversion/#etki","page":"Ensemble Kalman Inversion","title":"Output-scalable variant: Ensemble Transform Kalman Inversion","text":"Ensemble transform Kalman inversion (ETKI) is a variant of EKI based on the ensemble transform Kalman filter (Bishop et al., 2001). It is a form of ensemble square-root inversion, and an implementation can be found in Huang et al., 2022. The main advantage of ETKI over EKI is that it has better scalability as the observation dimension grows: while the naive implementation of EKI scales as mathcalO(p^3) in the observation dimension p, ETKI scales as mathcalO(p). This, however, refers to the online cost. ETKI may have an offline cost of mathcalO(p^3) if Gamma is not easily invertible; see below.\n\nThe major disadvantage of ETKI is that it cannot be used with localization or sampling error correction. \n\nnote: Creating scalable observational covariances\nETKI requires storing and inverting the observation noise covariance, Gamma^-1. Without care, this can be prohibitively expensive. To this end, we have tools and an API for creating and using scalable or compact representations of covariances that are necessary for scalability. See here for details and examples. ","category":"section"},{"location":"ensemble_kalman_inversion/#Using-ETKI","page":"Ensemble Kalman Inversion","title":"Using ETKI","text":"An ETKI struct can be created using the EnsembleKalmanProcess constructor by specifying the TransformInversion process type: \n\nusing EnsembleKalmanProcesses\n# given the prior distribution `prior`, data `y` and covariance `obs_noise_cov`,\n\nJ = 50  # number of ensemble members\ninitial_ensemble = construct_initial_ensemble(prior, J) # Initialize ensemble from prior\n\netkiobj = EnsembleKalmanProcess(initial_ensemble, y, obs_noise_cov,\n                               TransformInversion())\n\nThe rest of the inversion process is the same as for regular EKI.","category":"section"},{"location":"ensemble_kalman_inversion/#seki","page":"Ensemble Kalman Inversion","title":"Sparsity-Inducing Ensemble Kalman Inversion","text":"We include Sparsity-inducing Ensemble Kalman Inversion (SEKI) to add approximate L^0 and L^1 penalization to the EKI (Schneider, Stuart, Wu, 2020).\n\nwarning: Warning\nThe algorithm suffers from robustness issues, and therefore we urge caution in using the tool","category":"section"},{"location":"troubleshooting/#troubleshooting","page":"Troubleshooting","title":"Troubleshooting and Workflow Tips","text":"","category":"section"},{"location":"troubleshooting/#get-results","page":"Troubleshooting","title":"Getting the results","text":"Data is stored within the EnsembleKalmanProcess. Accessing results is done via our API of getter functions, and transformations.\n\nThe most common of these is\n\n# given an\n# EnsembleKalmanProcess `ekp`\n# prior                 `prior`\n\n# getting the \"latest\" parameter ensemble, or at a chosen iteration\nϕ = get_ϕ_final(prior, ekp)\nϕ = get_ϕ(prior, ekp, iteration)\n\n# getting the \"latest\" output ensemble, or at a chosen iteration\ng = get_g_final(ekp)\ng = get_g(ekp, iteration)\n\n# getting the \"latest\" data vector, or at a chosen iteration [if e.g. minibatching]\ny = get_obs(ekp)\ny = get_obs(ekp, iteration)\n\n# getting the \"latest\" observational noise covariance, or at a chosen iteration [if e.g. minibatching]\nΓ = get_obs_noise_cov(ekp) \nΓ = get_obs_noise_cov(ekp, build=false) # do not build the matrix, keep it blocked \nΓ = get_obs_noise_cov(ekp, iteration) \n\n# get the computed error metrics over iterations from `compute_error!`\nmetrics = get_error_metrics(ekp)\n\n# get the corresponding algorithm time for the iterations performed\nΔt = get_algorithm_time(ekp) \n\n\n# get the latest computational parameters\nu = get_u_final(ekp)  \n# where ϕ = transform_unconstrained_to_unconstrained.(prior, get_u_final(ekp))","category":"section"},{"location":"troubleshooting/#Convergence-diagnosis-and-plotting","page":"Troubleshooting","title":"Convergence diagnosis and plotting","text":"Information about convenient plotting tools, or error metrics computed during updates, please see here.","category":"section"},{"location":"troubleshooting/#High-failure-rate","page":"Troubleshooting","title":"High failure rate","text":"While some EKI variants include failure handlers, excessively high failure rates (i.e., > 80%) can lead to inversions finding local minima or failing to converge. To address this:\n\nStabilize the Forward Model: Ensure the forward model remains stable for small parameter perturbations in offline tests. If the forward model is unstable for most of the parameter space, it is challenging to explore it with a calibration method.\nAdjust Priors: Reduce the uncertainty in priors. Priors with large variances can lead to forward evaluations that deviate significantly from the known prior means, increasing the likelihood of failures.\nIncrease Ensemble Size: Without localization or other methods that break the subspace property, the ensemble size should generally exceed the number of parameters being optimized. The ensemble size needs to be large enough to ensure a sufficient number of successful runs, given the failure rate.\nConsider a Preconditioner: While not currently a native feature in EKP, consider using a preconditioning method to find stable parameter pairs before the first iteration. A preconditioner, applied in each ensemble member, recursively draws from the prior distribution until a stable parameter pair is achieved. The successful parameter pairs serve as the parameter values for the first iteration. Depending on the stability of the forward model,this may need to be as high as 5-10 retries.\nImplement Parameter Inflation: High failure rates in the initial iterations can lead to rapid collapse of ensemble members. Prevent the ensemble from collapsing prematurely by adding parameter inflation. For more, see inflation","category":"section"},{"location":"troubleshooting/#Loss-does-not-converge-or-final-fits-are-inadequate","page":"Troubleshooting","title":"Loss does not converge or final fits are inadequate","text":"If either the loss decreases too slowly/diverges or the final fits appear inadequate:\n\nCheck Observation Noise in Data Space: Ensure that noise estimates are realistic and consistent across variables with different dimensions and variability characteristics. Observation noise that is unrealistically large for a given variable or data point may prevent convergence to solutions that closely fit the data. Carefully base noise estimates on empirical data or domain knowledge, and try reducing noise if the previous suggestions don’t work. This is especially common if using sigma^2 * I as artificial noise. Even if u appears incorrect, it is advisable to examine the graphs of G(u) compared to  y pm 2sigma to determine if the forward map lies within the noise level. If it does, further convergence cannot be achieved without reducing the noise or altering the loss function.\nCheck for Failures: Refer to the suggestions for handling a high failure rate.\nAdjust the Artificial Timestep: For indirect learning problems involving neural networks, larger timesteps [O(10)] are generally more effective and using variable timesteppers (e.g., DataMisfitController()) tends to yield the best results. For scheduler options, see scheduler docs.\nIf Batching, Increase Batch Size: Users employing minibatching (using subsamples of the full dataset in each EKI iteration) should consider modifying the batch size. \nReevaluate Loss Function: Consider exploring alternative loss functions with different variables.\nStructural Model Errors: If these troubleshooting tips do not work, remaining discrepancies might suggest inherent structural errors between the model and the data, which could lead to trade-offs in parameter estimation. Modifications may be needed to the underlying forward model. ","category":"section"},{"location":"troubleshooting/#I-have-a-model-\\Psi.-But-how-do-I-design-my-forward-map-G?","page":"Troubleshooting","title":"I have a model Psi. But how do I design my forward map G?","text":"Ensure prior means are chosen appropriately, and that any hard constraints (i.e., parameter values must be positive) are enforced.\nStart with a perfect model experiment, where an attempt is made to recover known parameter values in Psi through calibration, to learn about what outputs from Psi are sensitive to the parameters.\nFor time-evolving systems, consider aggregating data through spatial or temporal averaging, rather than using the full timeseries. \nFind out which observational data are available for the problem at hand, and what observational noise is provided for measuring instruments.","category":"section"},{"location":"troubleshooting/#common-messages","page":"Troubleshooting","title":"Common warning/error messages","text":"Info: \"Termination condition of scheduler `DataMisfitController` will be exceeded during the next iteration.\"\nWarning: Termination condition of scheduler `DataMisfitController` has been exceeded, returning `true` from `update_ensemble!` and preventing futher updates. Set on_terminate=\"continue\" in `DataMisfitController` to ignore termination\n\nThe DataMisfitController is an adaptive scheduler that can terminate the algorithm at a given value of algorithm time (rather than juat a given number of iterations). See here for details on changing the termination condition. Or how to handle this in your iteration loop.\n\nWarning: Acceleration is experimental for Sampler processes and may affect convergence.\n\nThis is found when providing something other than accelerator = DefaultAccelerator() in EKS.\n\nInfo: 1 particle failure(s) detected. Handler used: IgnoreFailures.\nInfo: 1 particle failure(s) detected. Handler used: SampleSuccGauss.\n\nBoth these messages arise when the EKP update_ensemble! has detected NaNs in the forward map evaluation array. One can choose how to handle failed ensemble members with the EKP keyword failure_handler_method for more information on the failure handling methods see here. Note that EKS does not yet have a consistent failure handler method.\n\nWarning: Detected 2 clashes where forward map evaluations are exactly equal (and not NaN), this is likely to cause `LinearAlgebra` difficulty. Please check forward evaluations for bugs.\n\nThis message arises when forward map evaluations from different paramters produce identical outputs and is usually due to (1) output-clipping practices, or (2) user-error in producing the output matrix for the ensemble and should be checked, as it may cause errors. (If not, the implication is that the model output is completely independent of the parameters) ","category":"section"},{"location":"API/Localizers/#Localizers","page":"Localizers","title":"Localizers","text":"","category":"section"},{"location":"API/Localizers/#EnsembleKalmanProcesses.Localizers.Localizer","page":"Localizers","title":"EnsembleKalmanProcesses.Localizers.Localizer","text":"Localizer{LM <: LocalizationMethod, T}\n\nStructure that defines a localize function, based on a localization method.\n\nFields\n\nlocalize::Function: Localizing function of the form: cov -> kernel .* cov\n\nConstructors\n\nLocalizer(localization, J)\nLocalizer(localization, J, T)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/Localizers.jl:147.\n\nLocalizer(localization, J)\nLocalizer(localization, J, T)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/Localizers.jl:153.\n\nLocalizer(localization, J, T)\nLocalizer(localization, J)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/Localizers.jl:168.\n\nLocalizer(localization, J)\nLocalizer(localization, J, T)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/Localizers.jl:207.\n\nLocalizer(localization, J)\nLocalizer(localization, J, T)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/Localizers.jl:229.\n\nLocalizer(localization, J, T)\nLocalizer(localization, J)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/Localizers.jl:269.\n\nLocalizer(localization, J)\nLocalizer(localization, J, T)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/Localizers.jl:335.\n\n\n\n\n\n","category":"type"},{"location":"API/Localizers/#EnsembleKalmanProcesses.Localizers.RBF","page":"Localizers","title":"EnsembleKalmanProcesses.Localizers.RBF","text":"RBF{FT <: Real} <: LocalizationMethod\n\nRadial basis function localization method. Covariance terms C_ij are damped through multiplication with a centered Gaussian with standardized deviation d(ij)= vert i-j vert  l.\n\nFields\n\nlengthscale::Real: Length scale defining the RBF kernel\n\n\n\n\n\n","category":"type"},{"location":"API/Localizers/#EnsembleKalmanProcesses.Localizers.BernoulliDropout","page":"Localizers","title":"EnsembleKalmanProcesses.Localizers.BernoulliDropout","text":"BernoulliDropout{FT <: Real} <: LocalizationMethod\n\nLocalization method that drops cross-covariance terms with probability 1-p, retaining a Hermitian structure.\n\nFields\n\nprob::Real: Probability of keeping a given cross-covariance term\n\n\n\n\n\n","category":"type"},{"location":"API/Localizers/#EnsembleKalmanProcesses.Localizers.SEC","page":"Localizers","title":"EnsembleKalmanProcesses.Localizers.SEC","text":"SEC{FT <: Real} <: LocalizationMethod\n\nSampling error correction that shrinks correlations by a factor of vert r vert ^alpha, as per Lee (2021). Sparsity of the resulting correlations can be imposed through the parameter r_0.\n\nLee, Y. (2021). Sampling error correction in ensemble Kalman inversion. arXiv:2105.11341 [cs, math]. http://arxiv.org/abs/2105.11341\n\nFields\n\nα::Real: Controls degree of sampling error correction\nr_0::Real: Cutoff correlation\n\n\n\n\n\n","category":"type"},{"location":"API/Localizers/#EnsembleKalmanProcesses.Localizers.SECFisher","page":"Localizers","title":"EnsembleKalmanProcesses.Localizers.SECFisher","text":"SECFisher <: LocalizationMethod\n\nSampling error correction for EKI, as per Lee (2021), but using the method from Flowerdew (2015) based on the Fisher transformation. Correlations are shrunk by a factor determined by the sample correlation and the ensemble size. \n\nFlowerdew, J. (2015). Towards a theory of optimal localisation. Tellus A: Dynamic Meteorology and Oceanography, 67(1), 25257. https://doi.org/10.3402/tellusa.v67.25257\n\nLee, Y. (2021). Sampling error correction in ensemble Kalman inversion. arXiv:2105.11341 [cs, math]. http://arxiv.org/abs/2105.11341\n\n\n\n\n\n","category":"type"},{"location":"API/Localizers/#EnsembleKalmanProcesses.Localizers.SECNice","page":"Localizers","title":"EnsembleKalmanProcesses.Localizers.SECNice","text":"SECNice{FT <: Real} <: LocalizationMethod\n\nSampling error correction as of Vishny, Morzfeld, et al. (2024), DOI. Correlations are shrunk by a factor determined by correlation and ensemble size. The factors are automatically determined by a discrepancy principle. Thus no algorithm parameters are required, though some tuning of the discrepancy principle tolerances are made available.\n\nFields\n\nn_samples::Int64: number of samples to approximate the std of correlation distribution (default 1000)\nδ_ug::Real: scaling for discrepancy principle for ug correlation (default 1.0)\nδ_gg::Real: scaling for discrepancy principle for gg correlation (default 1.0)\n\n\n\n\n\n","category":"type"},{"location":"API/Localizers/#EnsembleKalmanProcesses.Localizers.Delta","page":"Localizers","title":"EnsembleKalmanProcesses.Localizers.Delta","text":"Dirac delta localization method, with an identity matrix as the kernel.\n\n\n\n\n\n","category":"type"},{"location":"API/Localizers/#EnsembleKalmanProcesses.Localizers.NoLocalization","page":"Localizers","title":"EnsembleKalmanProcesses.Localizers.NoLocalization","text":"Idempotent localization method.\n\n\n\n\n\n","category":"type"},{"location":"API/Visualize/#Visualize","page":"Visualize","title":"Visualize","text":"note: Add a Makie-backend package to your Project.toml\nImport one of the Makie backends (GLMakie, CairoMakie, WGLMakie, RPRMakie, etc.) to enable these functions!","category":"section"},{"location":"API/Visualize/#EnsembleKalmanProcesses.Visualize.plot_parameter_distribution","page":"Visualize","title":"EnsembleKalmanProcesses.Visualize.plot_parameter_distribution","text":"Visualize.plot_parameter_distribution(fig::Union{Makie.Figure, Makie.GridLayout, Makie.GridPosition, Makie.GridSubposition},\n                       pd::ParameterDistribution;\n                       constrained = true,\n                       n_sample = 1e4,\n                       rng = Random.GLOBAL_RNG)\n\nPlot the distributions pd on fig.\n\n\n\n\n\nVisualize.plot_parameter_distribution(fig::Union{Makie.Figure, Makie.GridLayout},\n                       pd::PDT;\n                       constrained = true,\n                       n_sample = 1e4,\n                       rng = Random.GLOBAL_RNG)\n                       where {PDT <: ParameterDistributionType}\n\nPlot the distribution on fig.\n\n\n\n\n\n","category":"function"},{"location":"API/Visualize/#EnsembleKalmanProcesses.Visualize.plot_error_over_iters","page":"Visualize","title":"EnsembleKalmanProcesses.Visualize.plot_error_over_iters","text":"Visualize.plot_error_over_iters(gridposition, ekp; kwargs...)\n\nPlot the errors from ekp against the number of iterations on gridposition.\n\nAny keyword arguments is passed to the plotting function which takes in any keyword arguments supported by Makie.Lines.\n\n\n\n\n\n","category":"function"},{"location":"API/Visualize/#EnsembleKalmanProcesses.Visualize.plot_error_over_iters!","page":"Visualize","title":"EnsembleKalmanProcesses.Visualize.plot_error_over_iters!","text":"Visualize.plot_error_over_iters!(axis, ekp; kwargs...)\n\nPlot the errors from ekp against the number of iterations on axis.\n\nAny keyword arguments is passed to the plotting function which takes in any keyword arguments supported by Makie.Lines.\n\n\n\n\n\n","category":"function"},{"location":"API/Visualize/#EnsembleKalmanProcesses.Visualize.plot_error_over_time","page":"Visualize","title":"EnsembleKalmanProcesses.Visualize.plot_error_over_time","text":"Visualize.plot_error_over_time(gridposition, ekp; kwargs...)\n\nPlot the errors from ekp against time on gridposition.\n\nAny keyword arguments is passed to the plotting function which takes in any keyword arguments supported by Makie.Lines.\n\n\n\n\n\n","category":"function"},{"location":"API/Visualize/#EnsembleKalmanProcesses.Visualize.plot_error_over_time!","page":"Visualize","title":"EnsembleKalmanProcesses.Visualize.plot_error_over_time!","text":"Visualize.plot_error_over_time!(axis, ekp; kwargs...)\n\nPlot the errors from ekp against time on axis.\n\nAny keyword arguments is passed to the plotting function which takes in any keyword arguments supported by Makie.Lines.\n\n\n\n\n\n","category":"function"},{"location":"API/Visualize/#EnsembleKalmanProcesses.Visualize.plot_ϕ_over_iters","page":"Visualize","title":"EnsembleKalmanProcesses.Visualize.plot_ϕ_over_iters","text":"Visualize.plot_ϕ_over_iters(gridposition, ekp, prior, dim_idx; kwargs...)\n\nPlot the constrained parameter of index dim_idx against time on gridposition.\n\nAny keyword arguments is passed to the plotting function which takes in any keyword arguments supported by Makie.Scatter.\n\n\n\n\n\nVisualize.plot_ϕ_over_iters(gridpositions, ekp, prior, name; kwargs...)\n\nPlot the constrained parameter belonging to distribution name against the number of iterations on the iterable gridpositions.\n\nAny keyword arguments are passed to all plots.\n\n\n\n\n\n","category":"function"},{"location":"API/Visualize/#EnsembleKalmanProcesses.Visualize.plot_ϕ_over_iters!","page":"Visualize","title":"EnsembleKalmanProcesses.Visualize.plot_ϕ_over_iters!","text":"Visualize.plot_ϕ_over_iters!(axis, ekp, prior, dim_idx; kwargs...)\n\nPlot the constrained parameter of index dim_idx against time on axis.\n\nAny keyword arguments is passed to the plotting function which takes in any keyword arguments supported by Makie.Scatter.\n\n\n\n\n\n","category":"function"},{"location":"API/Visualize/#EnsembleKalmanProcesses.Visualize.plot_ϕ_over_time","page":"Visualize","title":"EnsembleKalmanProcesses.Visualize.plot_ϕ_over_time","text":"Visualize.plot_ϕ_over_time(gridposition, ekp, prior, dim_idx; kwargs...)\n\nPlot the constrained parameter of index dim_idx against time on gridposition.\n\nAny keyword arguments is passed to the plotting function which takes in any keyword arguments supported by Makie.Scatter.\n\n\n\n\n\nVisualize.plot_ϕ_over_time(gridpositions, ekp, prior, name; kwargs...)\n\nPlot the constrained parameter belonging to distribution name against time on the iterable gridpositions.\n\nAny keyword arguments are passed to all plots.\n\n\n\n\n\n","category":"function"},{"location":"API/Visualize/#EnsembleKalmanProcesses.Visualize.plot_ϕ_over_time!","page":"Visualize","title":"EnsembleKalmanProcesses.Visualize.plot_ϕ_over_time!","text":"Visualize.plot_ϕ_over_time!(axis, ekp, prior, dim_idx; kwargs...)\n\nPlot the constrained parameter of index dim_idx against time on axis.\n\nAny keyword arguments is passed to the plotting function which takes in any keyword arguments supported by Makie.Scatter.\n\n\n\n\n\n","category":"function"},{"location":"API/Visualize/#EnsembleKalmanProcesses.Visualize.plot_ϕ_mean_over_iters!","page":"Visualize","title":"EnsembleKalmanProcesses.Visualize.plot_ϕ_mean_over_iters!","text":"Visualize.plot_ϕ_mean_over_iters!(axis, ekp, prior, dim_idx; plot_std = false, kwargs...)\n\nPlot the mean constrained parameter of index dim_idx of prior against the number of iterations on axis.\n\nIf plot_std = true, then the standard deviation of the constrained parameters of the ensemble is also plotted.\n\nAny keyword arguments is passed to the plotting function which takes in any keyword arguments supported by Makie.Lines and Makie.Band if plot_std = true.\n\nKeyword arguments passed to line_kwargs and band_kwargs are merged with kwargs when possible. The keyword arguments in line_kwargs and band_kwargs take priority over the keyword arguments in kwargs.\n\n\n\n\n\n","category":"function"},{"location":"API/Visualize/#EnsembleKalmanProcesses.Visualize.plot_ϕ_mean_over_iters","page":"Visualize","title":"EnsembleKalmanProcesses.Visualize.plot_ϕ_mean_over_iters","text":"Visualize.plot_ϕ_mean_over_iters(gridposition, ekp, prior, dim_idx; plot_std = false, kwargs...)\n\nPlot the mean constrained parameter of index dim_idx of prior against the number of iterations on gridposition.\n\nIf plot_std = true, then the standard deviation of the constrained parameters of the ensemble is also plotted.\n\nAny keyword arguments is passed to the plotting function which takes in any keyword arguments supported by Makie.Lines and Makie.Band if plot_std = true.\n\nKeyword arguments passed to line_kwargs and band_kwargs are merged with kwargs when possible. The keyword arguments in line_kwargs and band_kwargs take priority over the keyword arguments in kwargs.\n\n\n\n\n\nVisualize.plot_ϕ_mean_over_iters(gridpositions, ekp::EnsembleKalmanProcess, prior, name; kwargs...)\n\nPlot the mean constrained parameter belonging to distribution name against the number of iterations on the iterable gridpositions.\n\nAny keyword arguments are passed to all plots.\n\n\n\n\n\n","category":"function"},{"location":"API/Visualize/#EnsembleKalmanProcesses.Visualize.plot_ϕ_mean_over_time!","page":"Visualize","title":"EnsembleKalmanProcesses.Visualize.plot_ϕ_mean_over_time!","text":"Visualize.plot_ϕ_mean_over_time!(axis, ekp, prior, dim_idx; plot_std = false, kwargs...)\n\nPlot the mean constrained parameter of index dim_idx of prior against time on axis.\n\nIf plot_std = true, then the standard deviation of the constrained parameter of the ensemble is also plotted.\n\nAny keyword arguments is passed to the plotting function which takes in any keyword arguments supported by Makie.Lines and Makie.Band if plot_std = true.\n\nKeyword arguments passed to line_kwargs and band_kwargs are merged with kwargs when possible. The keyword arguments in line_kwargs and band_kwargs take priority over the keyword arguments in kwargs.\n\n\n\n\n\n","category":"function"},{"location":"API/Visualize/#EnsembleKalmanProcesses.Visualize.plot_ϕ_mean_over_time","page":"Visualize","title":"EnsembleKalmanProcesses.Visualize.plot_ϕ_mean_over_time","text":"Visualize.plot_ϕ_mean_over_time(gridposition, ekp, prior, dim_idx; plot_std = false, kwargs...)\n\nPlot the mean constrained parameter of index dim_idx of prior against time on gridposition.\n\nIf plot_std = true, then the standard deviation of the constrained parameter of the ensemble is also plotted.\n\nAny keyword arguments is passed to the plotting function which takes in any keyword arguments supported by Makie.Lines and Makie.Band if plot_std = true.\n\nKeyword arguments passed to line_kwargs and band_kwargs are merged with kwargs when possible. The keyword arguments in line_kwargs and band_kwargs take priority over the keyword arguments in kwargs.\n\n\n\n\n\nVisualize.plot_ϕ_mean_over_time(gridpositions, ekp, prior, name; kwargs...)\n\nPlot the mean constrained parameter belonging to distribution name against time on the iterable gridpositions.\n\nAny keyword arguments are passed to all plots.\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#ParameterDistributions","page":"ParameterDistributions","title":"ParameterDistributions","text":"","category":"section"},{"location":"API/ParameterDistributions/#ParameterDistributionTypes","page":"ParameterDistributions","title":"ParameterDistributionTypes","text":"","category":"section"},{"location":"API/ParameterDistributions/#Constraints","page":"ParameterDistributions","title":"Constraints","text":"","category":"section"},{"location":"API/ParameterDistributions/#ParameterDistributions-2","page":"ParameterDistributions","title":"ParameterDistributions","text":"","category":"section"},{"location":"API/ParameterDistributions/#FunctionParameterDistributions","page":"ParameterDistributions","title":"FunctionParameterDistributions","text":"","category":"section"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.Parameterized","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.Parameterized","text":"struct Parameterized <: EnsembleKalmanProcesses.ParameterDistributions.ParameterDistributionType\n\nA distribution constructed from a parameterized formula (e.g Julia Distributions.jl)\n\nFields\n\ndistribution::Distributions.Distribution: A parameterized distribution\n\n\n\n\n\n","category":"type"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.Samples","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.Samples","text":"struct Samples{FT<:Real} <: EnsembleKalmanProcesses.ParameterDistributions.ParameterDistributionType\n\nA distribution comprised of only samples, stored as columns of parameters.\n\nFields\n\ndistribution_samples::AbstractMatrix{FT} where FT<:Real: Samples defining an empirical distribution, stored as columns\n\n\n\n\n\n","category":"type"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.VectorOfParameterized","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.VectorOfParameterized","text":"struct VectorOfParameterized{DT<:Distributions.Distribution} <: EnsembleKalmanProcesses.ParameterDistributions.ParameterDistributionType\n\nA distribution built from an array of Parametrized distributions. A utility to help stacking of distributions where a multivariate equivalent doesn't exist.\n\nFields\n\ndistribution::AbstractVector{DT} where DT<:Distributions.Distribution: A vector of parameterized distributions\n\n\n\n\n\n","category":"type"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.Constraint","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.Constraint","text":"struct Constraint{T} <: EnsembleKalmanProcesses.ParameterDistributions.ConstraintType\n\nClass describing a 1D bijection between constrained and unconstrained spaces. Included parametric types for T:\n\nNoConstraint\nBoundedBelow\nBoundedAbove\nBounded\n\nFields\n\nconstrained_to_unconstrained::Function: A map from constrained domain -> (-Inf,Inf)\nc_to_u_jacobian::Function: The jacobian of the map from constrained domain -> (-Inf,Inf)\nunconstrained_to_constrained::Function: Map from (-Inf,Inf) -> constrained domain\nbounds::Union{Nothing, Dict}: Dictionary of values used to build the Constraint (e.g. \"lowerbound\" or \"upperbound\")\n\n\n\n\n\n","category":"type"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.no_constraint","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.no_constraint","text":"no_constraint(\n\n) -> EnsembleKalmanProcesses.ParameterDistributions.Constraint{EnsembleKalmanProcesses.ParameterDistributions.NoConstraint}\n\n\nConstructs a Constraint with no constraints, enforced by maps x -> x and x -> x.\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.bounded_below","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.bounded_below","text":"bounded_below(\n    lower_bound::Real\n) -> Union{EnsembleKalmanProcesses.ParameterDistributions.Constraint{EnsembleKalmanProcesses.ParameterDistributions.BoundedBelow}, EnsembleKalmanProcesses.ParameterDistributions.Constraint{EnsembleKalmanProcesses.ParameterDistributions.NoConstraint}}\n\n\nConstructs a Constraint with provided lower bound, enforced by maps x -> log(x - lower_bound) and x -> exp(x) + lower_bound.\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.bounded_above","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.bounded_above","text":"bounded_above(\n    upper_bound::Real\n) -> Union{EnsembleKalmanProcesses.ParameterDistributions.Constraint{EnsembleKalmanProcesses.ParameterDistributions.BoundedAbove}, EnsembleKalmanProcesses.ParameterDistributions.Constraint{EnsembleKalmanProcesses.ParameterDistributions.NoConstraint}}\n\n\nConstructs a Constraint with provided upper bound, enforced by maps x -> log(upper_bound - x) and x -> upper_bound - exp(x).\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.bounded","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.bounded","text":"bounded(\n    lower_bound::Real,\n    upper_bound::Real\n) -> EnsembleKalmanProcesses.ParameterDistributions.Constraint\n\n\nConstructs a Constraint with provided upper and lower bounds, enforced by maps x -> log((x - lower_bound) / (upper_bound - x)) and x -> (upper_bound * exp(x) + lower_bound) / (exp(x) + 1).\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#Base.length-Tuple{CType} where CType<:EnsembleKalmanProcesses.ParameterDistributions.ConstraintType","page":"ParameterDistributions","title":"Base.length","text":"length(\n    c::EnsembleKalmanProcesses.ParameterDistributions.ConstraintType\n) -> Int64\n\n\nA constraint has length 1. \n\n\n\n\n\n","category":"method"},{"location":"API/ParameterDistributions/#Base.size-Tuple{CType} where CType<:EnsembleKalmanProcesses.ParameterDistributions.ConstraintType","page":"ParameterDistributions","title":"Base.size","text":"size(\n    c::EnsembleKalmanProcesses.ParameterDistributions.ConstraintType\n) -> Tuple{Int64}\n\n\nA constraint has size 1.\n\n\n\n\n\n","category":"method"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution","text":"struct ParameterDistribution{PDType<:EnsembleKalmanProcesses.ParameterDistributions.ParameterDistributionType, CType<:EnsembleKalmanProcesses.ParameterDistributions.ConstraintType, ST<:AbstractString}\n\nStructure to hold a parameter distribution, always stored as an array of distributions internally.\n\nFields\n\ndistribution::AbstractVector{PDType} where PDType<:EnsembleKalmanProcesses.ParameterDistributions.ParameterDistributionType: Vector of parameter distributions, defined in unconstrained space\nconstraint::AbstractVector{CType} where CType<:EnsembleKalmanProcesses.ParameterDistributions.ConstraintType: Vector of constraints defining transformations between constrained and unconstrained space\nname::AbstractVector{ST} where ST<:AbstractString: Vector of parameter names\n\nConstructors\n\nRecommended construction (for most problems) is via the constrained_gaussian() utility.\n\nParameterDistribution(distribution, constraint, name)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/ParameterDistributions.jl:291.\n\nParameterDistribution(param_dist_dict)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/ParameterDistributions.jl:309.\n\nParameterDistribution(distribution, constraint, name)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/ParameterDistributions.jl:391.\n\nParameterDistribution(\n    distribution_samples,\n    constraint,\n    name;\n    params_are_columns\n)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/ParameterDistributions.jl:430.\n\n\n\n\n\n","category":"type"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.constrained_gaussian","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.constrained_gaussian","text":"constrained_gaussian(\n    name::AbstractString,\n    μ_c::Real,\n    σ_c::Real,\n    lower_bound::Real,\n    upper_bound::Real;\n    repeats,\n    optim_algorithm,\n    optim_kwargs...\n) -> Union{EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution{EnsembleKalmanProcesses.ParameterDistributions.Parameterized, T} where T<:EnsembleKalmanProcesses.ParameterDistributions.Constraint, EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution{T, T1} where {T<:(EnsembleKalmanProcesses.ParameterDistributions.VectorOfParameterized{T} where T<:Distributions.Normal), T1<:EnsembleKalmanProcesses.ParameterDistributions.Constraint}}\n\n\nConstructor for a 1D ParameterDistribution consisting of a transformed Gaussian, constrained to have support on [lower_bound, upper_bound], with first two moments μ_c and σ_c^2. The  moment integrals can't be done in closed form, so we set the parameters of the distribution with numerical optimization.\n\nkwargs:\n\nrepeats=1 : K-dimensional distribution by stacking K independent copies of the defined univariate distribution by setting repeats = K.\n\nExample usage:\n\nd1 = constrained_gaussian(\"mean2-sd1-positive\", 2.0, 1.0, 0, Inf)\nd2 = constrained_gaussian(\"4-dim-mean0-sd10\", 0.0, 4.0, -Inf, Inf, repeats=4)\n# combine with:\nd = combine_distributions([d1,d2])\n\nnote: Note\nThe intended use case is defining priors set from user expertise for use in inference  with adequate data, so for the sake of performance we only require that the optimization reproduce μ_c, σ_c to a loose tolerance (1e-5). Warnings are logged when the optimization fails.\n\nnote: Note\nThe distribution may be bimodal for σ_c large relative to the width of the bound interval. In extreme cases the distribution becomes concentrated at the bound endpoints. We regard this as a feature, not a bug, and do not warn the user when bimodality occurs.\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.n_samples","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.n_samples","text":"n_samples(\n    d::EnsembleKalmanProcesses.ParameterDistributions.Samples\n) -> Any\n\n\nThe number of samples in the array.\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.get_name","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.get_name","text":"get_name(\n    pd::EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution\n) -> AbstractVector{ST} where ST<:AbstractString\n\n\nReturns a list of ParameterDistribution names.\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.get_dimensions","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.get_dimensions","text":"get_dimensions(\n    pd::EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution;\n    function_parameter_opt\n) -> Any\n\n\nThe number of dimensions of the parameter space. (Also represents other dimensions of interest for FunctionParameterDistributionTypes with keyword argument)\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.get_n_samples","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.get_n_samples","text":"get_n_samples(\n    pd::EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution\n) -> Dict{String, Any}\n\n\nThe number of samples in a Samples distribution\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.get_all_constraints-Tuple{EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution}","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.get_all_constraints","text":"get_all_constraints(\n    pd::EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution;\n    return_dict\n) -> Union{Dict{Any, Any}, AbstractVector{CType} where CType<:EnsembleKalmanProcesses.ParameterDistributions.ConstraintType}\n\n\nReturns the (flattened) array of constraints of the parameter distribution. or as a dictionary (\"param_name\" => constraints)\n\n\n\n\n\n","category":"method"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.get_constraint_type","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.get_constraint_type","text":"get_constraint_type(\n    c::EnsembleKalmanProcesses.ParameterDistributions.Constraint{T}\n) -> Any\n\n\nGets the parametric type T.\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.get_bounds","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.get_bounds","text":"get_bounds(\n    c::EnsembleKalmanProcesses.ParameterDistributions.Constraint\n) -> Union{Nothing, Dict}\n\n\nGets the bounds field from the Constraint.\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.batch","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.batch","text":"batch(\n    pd::Union{EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution, EnsembleKalmanProcesses.ParameterDistributions.VectorOfParameterized};\n    function_parameter_opt\n) -> Any\n\n\nReturns a list of contiguous [collect(1:i), collect(i+1:j),... ] used to split parameter arrays by distribution dimensions. function_parameter_opt is passed to ndims in the special case of FunctionParameterDistributionTypes.\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.get_distribution","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.get_distribution","text":"get_distribution(\n    pd::EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution\n) -> Dict{String, Any}\n\n\nReturns a Dict of ParameterDistribution distributions, with the parameter names as dictionary keys. For parameters represented by Samples, the samples are returned as a 2D (parameter_dimension x n_samples) array.\n\n\n\n\n\nget_distribution(\n    grfi::EnsembleKalmanProcesses.ParameterDistributions.GaussianRandomFieldInterface\n) -> EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution\n\n\ngets the, distribution over the coefficients\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#StatsBase.sample","page":"ParameterDistributions","title":"StatsBase.sample","text":"sample(\n    rng::Random.AbstractRNG,\n    pd::EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution,\n    n_draws::Integer\n) -> Any\n\n\nDraws n_draws samples from the parameter distributions pd. Returns an array, with  parameters as columns. rng is optional and defaults to Random.GLOBAL_RNG. n_draws is  optional and defaults to 1. Performed in computational space.\n\n\n\n\n\nsample(\n    rng::Random.AbstractRNG,\n    d::EnsembleKalmanProcesses.ParameterDistributions.Samples,\n    n_draws::Integer\n) -> Any\n\n\nDraws n_draws samples from the parameter distributions d. Returns an array, with  parameters as columns. rng is optional and defaults to Random.GLOBAL_RNG. n_draws is  optional and defaults to 1. Performed in computational space.\n\n\n\n\n\nsample(\n    rng::Random.AbstractRNG,\n    d::EnsembleKalmanProcesses.ParameterDistributions.Parameterized,\n    n_draws::Integer\n) -> Any\n\n\nDraws n_draws samples from the parameter distributions d. Returns an array, with  parameters as columns. rng is optional and defaults to Random.GLOBAL_RNG. n_draws is  optional and defaults to 1. Performed in computational space.\n\n\n\n\n\nsample(\n    rng::Random.AbstractRNG,\n    d::EnsembleKalmanProcesses.ParameterDistributions.VectorOfParameterized,\n    n_draws::Integer\n) -> Any\n\n\nDraws n_draws samples from the parameter distributions d. Returns an array, with  parameters as columns. rng is optional and defaults to Random.GLOBAL_RNG. n_draws is  optional and defaults to 1. Performed in computational space.\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#Distributions.logpdf","page":"ParameterDistributions","title":"Distributions.logpdf","text":"logpdf(\n    d::EnsembleKalmanProcesses.ParameterDistributions.Parameterized,\n    x::Real\n) -> Any\n\n\nObtains the independent logpdfs of the parameter distributions at xarray (non-Samples Distributions only), and returns their sum.\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#Statistics.mean","page":"ParameterDistributions","title":"Statistics.mean","text":"mean(\n    d::EnsembleKalmanProcesses.ParameterDistributions.Parameterized\n) -> Any\n\n\nReturns a concatenated mean of the parameter distributions. \n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#Statistics.var","page":"ParameterDistributions","title":"Statistics.var","text":"var(\n    d::EnsembleKalmanProcesses.ParameterDistributions.Parameterized\n) -> Any\n\n\nReturns a flattened variance of the distributions\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#Statistics.cov","page":"ParameterDistributions","title":"Statistics.cov","text":"cov(\n    d::EnsembleKalmanProcesses.ParameterDistributions.Parameterized\n) -> Any\n\n\nReturns a dense blocked (co)variance of the distributions.\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.transform_constrained_to_unconstrained-Tuple{EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution, AbstractVector}","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.transform_constrained_to_unconstrained","text":"transform_constrained_to_unconstrained(\n    pd::EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution,\n    x\n) -> Union{Matrix{T} where T<:Real, Vector{T} where T<:Real}\n\n\nApply the transformation to map parameter sample ensembles x from the (possibly) constrained space into unconstrained space. Here, x is an iterable of parameters sample ensembles for different EKP iterations.\n\n\n\n\n\n","category":"method"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.transform_constrained_to_unconstrained-Tuple{EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution, AbstractMatrix}","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.transform_constrained_to_unconstrained","text":"transform_constrained_to_unconstrained(\n    pd::EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution,\n    x\n) -> Union{Matrix{T} where T<:Real, Vector{T} where T<:Real}\n\n\nApply the transformation to map parameter sample ensembles x from the (possibly) constrained space into unconstrained space. Here, x is an iterable of parameters sample ensembles for different EKP iterations.\n\n\n\n\n\n","category":"method"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.transform_constrained_to_unconstrained-Tuple{EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution, Dict}","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.transform_constrained_to_unconstrained","text":"transform_constrained_to_unconstrained(\n    pd::EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution,\n    x::Dict\n) -> Dict{Any, Any}\n\n\nApply the transformation to map (possibly constrained) parameter samples x into the unconstrained space. Here, x contains parameter names as keys, and 1- or 2-arrays as parameter samples.\n\n\n\n\n\n","category":"method"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.transform_unconstrained_to_constrained-Tuple{EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution, AbstractVector}","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.transform_unconstrained_to_constrained","text":"transform_unconstrained_to_constrained(\n    pd::EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution,\n    x\n) -> Union{Matrix{T} where T<:Real, Vector{T} where T<:Real}\n\n\nApply the transformation to map parameter sample ensembles x from the unconstrained space into (possibly constrained) space. Here, x is an iterable of parameters sample ensembles for different EKP iterations. The build_flag will construct any function parameters\n\n\n\n\n\n","category":"method"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.transform_unconstrained_to_constrained-Tuple{EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution, AbstractMatrix}","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.transform_unconstrained_to_constrained","text":"transform_unconstrained_to_constrained(\n    pd::EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution,\n    x\n) -> Union{Matrix{T} where T<:Real, Vector{T} where T<:Real}\n\n\nApply the transformation to map parameter sample ensembles x from the unconstrained space into (possibly constrained) space. Here, x is an iterable of parameters sample ensembles for different EKP iterations. The build_flag will construct any function parameters\n\n\n\n\n\n","category":"method"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.transform_unconstrained_to_constrained-Tuple{EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution, Dict}","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.transform_unconstrained_to_constrained","text":"transform_unconstrained_to_constrained(\n    pd::EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution,\n    x::Dict;\n    build_flag\n) -> Dict{Any, Any}\n\n\nApply the transformation to map (possibly constrained) parameter samples x into the unconstrained space. Here, x contains parameter names as keys, and 1- or 2-arrays as parameter samples. The build_flag will reconstruct any function parameters onto their flattened, discretized, domains.\n\n\n\n\n\n","category":"method"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.GaussianRandomFieldsPackage","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.GaussianRandomFieldsPackage","text":"abstract type GaussianRandomFieldsPackage\n\nType to dispatch which Gaussian Random Field package to use:\n\nGRFJL uses the Julia Package GaussianRandomFields.jl \n\n\n\n\n\n","category":"type"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.GaussianRandomFieldInterface","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.GaussianRandomFieldInterface","text":"struct GaussianRandomFieldInterface <: EnsembleKalmanProcesses.ParameterDistributions.FunctionParameterDistributionType\n\nGaussianRandomFieldInterface object based on a GRF package. Only a ND->1D output-dimension field interface is implemented.\n\nFields\n\ngaussian_random_field::Any: GRF object, containing the mapping from the field of coefficients to the discrete function\npackage::EnsembleKalmanProcesses.ParameterDistributions.GaussianRandomFieldsPackage: the choice of GRF package\ndistribution::EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution: the distribution of the coefficients that we shall compute with\n\n\n\n\n\n","category":"type"},{"location":"API/ParameterDistributions/#Base.ndims-Tuple{EnsembleKalmanProcesses.ParameterDistributions.GaussianRandomFieldInterface}","page":"ParameterDistributions","title":"Base.ndims","text":"ndims(\n    grfi::EnsembleKalmanProcesses.ParameterDistributions.GaussianRandomFieldInterface;\n    function_parameter_opt\n) -> Any\n\n\nProvides a relevant number of dimensions in different circumstances, If function_parameter_opt =\n\n\"dof\"       : returns n_dofs(grfi), the degrees of freedom in the function\n\"eval\"      : returns n_eval_pts(grfi), the number of discrete evaluation points of the function\n\"constraint\": returns 1, the number of constraints in the evaluation space \n\n\n\n\n\n","category":"method"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.get_all_constraints-Tuple{EnsembleKalmanProcesses.ParameterDistributions.GaussianRandomFieldInterface}","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.get_all_constraints","text":"get_all_constraints(\n    grfi::EnsembleKalmanProcesses.ParameterDistributions.GaussianRandomFieldInterface\n) -> Union{Dict{Any, Any}, AbstractVector{CType} where CType<:EnsembleKalmanProcesses.ParameterDistributions.ConstraintType}\n\n\ngets all the constraints of the internally stored coefficient prior distribution of the GRFI\n\n\n\n\n\n","category":"method"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.transform_constrained_to_unconstrained-Union{Tuple{FT}, Tuple{EnsembleKalmanProcesses.ParameterDistributions.GaussianRandomFieldInterface, AbstractVector, AbstractVector{FT}}} where FT<:Real","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.transform_constrained_to_unconstrained","text":"transform_constrained_to_unconstrained(\n    d::EnsembleKalmanProcesses.ParameterDistributions.GaussianRandomFieldInterface,\n    constraint::AbstractVector,\n    x::AbstractArray{FT<:Real, 1}\n) -> Any\n\n\nAssume x is a flattened vector of evaluation points. Remove the constraint from constraint to the output space of the function. Note this is the inverse of transform_unconstrained_to_constrained(...,build_flag=false)\n\n\n\n\n\n","category":"method"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.transform_constrained_to_unconstrained-Union{Tuple{FT}, Tuple{EnsembleKalmanProcesses.ParameterDistributions.GaussianRandomFieldInterface, AbstractVector, AbstractMatrix{FT}}} where FT<:Real","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.transform_constrained_to_unconstrained","text":"transform_constrained_to_unconstrained(\n    d::EnsembleKalmanProcesses.ParameterDistributions.GaussianRandomFieldInterface,\n    constraint::AbstractVector,\n    x::AbstractArray{FT<:Real, 2}\n) -> Any\n\n\nAssume x is a matrix with columns as flattened samples of evaluation points. Remove the constraint from constraint to the output space of the function. Note this is the inverse of transform_unconstrained_to_constrained(...,build_flag=false)\n\n\n\n\n\n","category":"method"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.transform_unconstrained_to_constrained-Union{Tuple{FT}, Tuple{EnsembleKalmanProcesses.ParameterDistributions.GaussianRandomFieldInterface, AbstractVector, AbstractVector{FT}}} where FT<:Real","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.transform_unconstrained_to_constrained","text":"transform_unconstrained_to_constrained(\n    d::EnsembleKalmanProcesses.ParameterDistributions.GaussianRandomFieldInterface,\n    constraint::AbstractVector,\n    x::AbstractArray{FT<:Real, 1};\n    build_flag\n) -> Any\n\n\nOptional Args build_flag::Bool = true\n\nTwo functions, depending on build_flag If true, assume x is a vector of coefficients. Perform the following 3 maps. \n\nApply the transformation to map (possibly constrained) parameter samples x into the unconstrained space. Using internally stored constraints (given by the coefficient prior)\nBuild the unconstrained (flattened) function sample at the evaluation points from these constrained coefficients.\nApply the constraint from constraint to the output space of the function.\n\nIf false, Assume x is a flattened vector of evaluation points. Apply only step 3. above to x.\n\n\n\n\n\n","category":"method"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.transform_unconstrained_to_constrained-Union{Tuple{FT}, Tuple{EnsembleKalmanProcesses.ParameterDistributions.GaussianRandomFieldInterface, AbstractVector, AbstractMatrix{FT}}} where FT<:Real","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.transform_unconstrained_to_constrained","text":"transform_unconstrained_to_constrained(\n    d::EnsembleKalmanProcesses.ParameterDistributions.GaussianRandomFieldInterface,\n    constraint::AbstractVector,\n    x::AbstractArray{FT<:Real, 2};\n    build_flag\n) -> Any\n\n\nOptional args: build_flag::Bool = true\n\nTwo functions, depending on build_flag If true, assume x is a matrix with columns of coefficient samples. Perform the following 3 maps. \n\nApply the transformation to map (possibly constrained) parameter samples x into the unconstrained space. Using internally stored constraints (given by the coefficient prior)\nBuild the unconstrained (flattened) function sample at the evaluation points from these constrained coefficients.\nApply the constraint from constraint to the output space of the function.\n\nIf false, Assume x is a matrix with columns as flattened samples of evaluation points. Apply only step 3. above to x.\n\n\n\n\n\n","category":"method"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.get_grf","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.get_grf","text":"get_grf(\n    grfi::EnsembleKalmanProcesses.ParameterDistributions.GaussianRandomFieldInterface\n) -> Any\n\n\ngets the distribution, i.e. Gaussian random field object\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.build_function_sample","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.build_function_sample","text":"build_function_sample(\n    grfi::EnsembleKalmanProcesses.ParameterDistributions.GaussianRandomFieldInterface,\n    coeff_vecormat::AbstractVecOrMat,\n    n_draws::Int64\n) -> Any\n\n\nbuild function n_draw times on the discrete grid, given the coefficients coeff_vecormat.\n\nDefaults: n_draw = 1.\n\n\n\n\n\nbuild_function_sample(\n    rng::Random.AbstractRNG,\n    grfi::EnsembleKalmanProcesses.ParameterDistributions.GaussianRandomFieldInterface,\n    n_draws::Int64\n) -> Any\n\n\nsample function distribution n_draw times on the discrete grid, from the stored prior distributions.\n\nDefaults: n_draw = 1, rng = Random.GLOBAL_RNG, and coeff_vec sampled from the stored prior distribution\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.get_package","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.get_package","text":"get_package(\n    grfi::EnsembleKalmanProcesses.ParameterDistributions.GaussianRandomFieldInterface\n) -> EnsembleKalmanProcesses.ParameterDistributions.GaussianRandomFieldsPackage\n\n\ngets the package type used to construct the GRF\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.spectrum","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.spectrum","text":"spectrum(\n    grfi::EnsembleKalmanProcesses.ParameterDistributions.GaussianRandomFieldInterface\n) -> Any\n\n\nthe spectral information of the GRF, e.g. the Karhunen-Loeve coefficients and eigenfunctions if using this decomposition\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.n_dofs","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.n_dofs","text":"n_dofs(\n    grfi::EnsembleKalmanProcesses.ParameterDistributions.GaussianRandomFieldInterface\n) -> Any\n\n\nthe number of degrees of freedom / coefficients (i.e. the number of parameters)\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.eval_pts","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.eval_pts","text":"eval_pts(\n    grfi::EnsembleKalmanProcesses.ParameterDistributions.GaussianRandomFieldInterface\n) -> Any\n\n\nthe discrete evaluation point grid, stored as a range in each dimension\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.n_eval_pts","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.n_eval_pts","text":"n_eval_pts(\n    grfi::EnsembleKalmanProcesses.ParameterDistributions.GaussianRandomFieldInterface\n) -> Any\n\n\nthe number of total discrete evaluation points\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.input_dims","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.input_dims","text":"input_dims(\n    grfi::EnsembleKalmanProcesses.ParameterDistributions.GaussianRandomFieldInterface\n) -> Any\n\n\nthe number of input dimensions of the GRF\n\n\n\n\n\n","category":"function"},{"location":"API/TOMLInterface/#TOML-interface","page":"TOML Interface","title":"TOML interface","text":"","category":"section"},{"location":"API/TOMLInterface/#EnsembleKalmanProcesses.TOMLInterface.path_to_ensemble_member","page":"TOML Interface","title":"EnsembleKalmanProcesses.TOMLInterface.path_to_ensemble_member","text":"path_to_ensemble_member(\n    base_path,\n    iteration,\n    member,\n    pad_zeros = 3,\n)\n\nObtains the file path to a specified ensemble member. The likely form is base_path/iteration_X/member_Y/ with X,Y padded with zeros. The file path can be reconstructed with: base_path - base path to where EKP parameters are stored member - number of the ensemble member (without zero padding) iteration - iteration of ensemble method (if =nothing then only the load path is used) pad_zeros - amount of digits to pad to\n\n\n\n\n\nOne can also call this without the iteration level\n\n\n\n\n\n","category":"function"},{"location":"API/TOMLInterface/#EnsembleKalmanProcesses.TOMLInterface.get_parameter_distribution","page":"TOML Interface","title":"EnsembleKalmanProcesses.TOMLInterface.get_parameter_distribution","text":"get_parameter_distribution(param_dict, name)\n\nConstructs a ParameterDistribution for a single parameter\n\nArgs: param_dict - nested dictionary that has parameter names as keys and the                corresponding dictionary of parameter information (in particular,                the parameters' prior distributions and constraints) as values name - parameter name\n\nReturns a ParameterDistribution\n\n\n\n\n\nget_parameter_distribution(param_dict, names)\n\nConstructs a ParameterDistribution for an array of parameters\n\nArgs: param_dict - nested dictionary that has parameter names as keys and the                corresponding dictionary of parameter information (in particular,                the parameters' prior distributions and constraints) as values names - array of parameter names\n\nReturns a ParameterDistribution \n\n\n\n\n\n","category":"function"},{"location":"API/TOMLInterface/#EnsembleKalmanProcesses.TOMLInterface.get_parameter_values","page":"TOML Interface","title":"EnsembleKalmanProcesses.TOMLInterface.get_parameter_values","text":"get_parameter_values(param_dict, names)\n\nGets parameter values from a parameter dictionary, indexing by name.\n\nArgs: param_dict - nested dictionary that has parameter names as keys and the                corresponding dictionary of parameter information (in particular,                the parameters' values) name - iterable parameter names return_type - return type, default \"dict\", otherwise \"array\"\n\n\n\n\n\n","category":"function"},{"location":"API/TOMLInterface/#EnsembleKalmanProcesses.TOMLInterface.save_parameter_ensemble","page":"TOML Interface","title":"EnsembleKalmanProcesses.TOMLInterface.save_parameter_ensemble","text":"save_parameter_ensemble(\n    param_array,\n    param_distribution,\n    default_param_data,\n    save_path,\n    save_file,\n    iteration\n    pad_zeros=3,\napply_constraints=true\n)\n\nSaves the parameters in the given param_array to TOML files. The intended use is for saving the ensemble of parameters after each update of an ensemble Kalman process. Each ensemble member (column of param_array) is saved in a separate directory \"member<j>\" (j=1, ..., Nens). The name of the saved toml file is given by save_file; it is the same for all members. A directory \"iteration<iter>\" is created in `savepath`, which contains all the \"member_<j>\" subdirectories.\n\nArgs: param_array - array of size Nparam x Nens param_distribution - the parameter distribution underlying param_array apply_constraints -  apply the constraints in param_distribution default_param_data - dict of default parameters to be combined and saved with                        the parameters in param_array into a toml file save_path - path to where the parameters will be saved save_file - name of the toml files to be generated iteration - the iteration of the ensemble Kalman process represented by the given          param_array pad_zeros - the amount of zero-padding for the ensemble member number\n\n\n\n\n\nOne can also call this without the iteration level\n\n\n\n\n\n","category":"function"},{"location":"API/TOMLInterface/#EnsembleKalmanProcesses.TOMLInterface.get_admissible_parameters","page":"TOML Interface","title":"EnsembleKalmanProcesses.TOMLInterface.get_admissible_parameters","text":"get_admissible_parameters(param_dict)\n\nFinds all parameters in param_dict that are admissible for calibration.\n\nArgs: param_dict - nested dictionary that has parameter names as keys and the                corresponding dictionaries of parameter information as values\n\nReturns an array of the names of all admissible parameters in param_dict. Admissible parameters must have a key \"prior\" and the value value of this is not set to \"fixed\". This allows for other parameters to be stored within the TOML file.\n\n\n\n\n\n","category":"function"},{"location":"API/TOMLInterface/#EnsembleKalmanProcesses.TOMLInterface.get_regularization","page":"TOML Interface","title":"EnsembleKalmanProcesses.TOMLInterface.get_regularization","text":"get_regularization(param_dict, name)\n\nReturns the regularization information for a single parameter\n\nArgs: param_dict - nested dictionary that has parameter names as keys and the                corresponding dictionary of parameter information as values name - parameter name\n\nReturns a tuple (<regularizationtype>, <regularizationvalue>), where the regularization type is either \"L1\" or \"L2\", and the regularization value is a float. Returns (nothing, nothing) if parameter has no regularization information.\n\n\n\n\n\nget_regularization(param_dict, names)\n\nReturns the regularization information for an array of parameters\n\nArgs: param_dict - nested dictionary that has parameter names as keys and the                corresponding dictionary of parameter information as values names - array of parameter names\n\nReturns an arary of tuples (<regularizationtype>, <regularizationvalue>), with the ith tuple corresponding to the parameter names[i].  The regularization type is either \"L1\" or \"L2\", and the regularization  value is a float. Returns (nothing, nothing) for parameters that have no regularization information.\n\n\n\n\n\n","category":"function"},{"location":"API/TOMLInterface/#EnsembleKalmanProcesses.TOMLInterface.write_log_file","page":"TOML Interface","title":"EnsembleKalmanProcesses.TOMLInterface.write_log_file","text":"write_log_file(param_dict, file_path)\n\nWrites the parameters in param_dict into a .toml file\n\nArgs: param_dict - nested dictionary that has parameter names as keys and the                corresponding dictionaries of parameter information as values file_path - path of the file where parameters are saved\n\n\n\n\n\n","category":"function"},{"location":"examples/ClimateMachine_example/#HPC-interfacing-example:-ClimateMachine","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"info: How do I run this code?\nThe full code is found in the examples/ directory of the github repository","category":"section"},{"location":"examples/ClimateMachine_example/#Overview","page":"HPC interfacing example: ClimateMachine","title":"Overview","text":"This examples uses EnsembleKalmanProcesses.jl to calibrate a climate model, showcasing a workflow which is compatible with HPC resources managed with the SLURM workload manager. The workflow is based on read-write input/output files, and as such it is capable of interfacing with dynamical models in different code languages, or with complicated processing stages. The dynamical model for this example is ClimateMachine.jl, an Earth system model currently under development at CliMA.\n\nThe calibration example makes use of a simple single atmospheric column model configuration with two learnable parameters that control turbulent mixing processes in the lower troposphere. It is also a perfect model experiment, in the sense that the ground truth is generated using the same model and a prescribed combination of parameters. The parameters used to generate the ground truth are (C_smag, C_drag) = (0.21, 0.0011). The evolution of the atmosphere for this setup is strongly influenced by C_smag, and very weakly by C_drag. Thus, we expect the EKP to recover C_smag from observations.","category":"section"},{"location":"examples/ClimateMachine_example/#Prerequisites","page":"HPC interfacing example: ClimateMachine","title":"Prerequisites","text":"This example requires ClimateMachine.jl to be installed in the same parent directory as EnsembleKalmanProcesses.jl. You may install ClimateMachine.jl directly from GitHub,\n\n$ git clone https://github.com/CliMA/ClimateMachine.jl.git\n\nChange into the ClimateMachine.jl directory with \n\n$ cd ClimateMachine.jl\n\nand install all the required packages with:\n\n$ julia --project -e 'using Pkg; pkg\"instantiate\";'\n\nPre-compile the packages to allow the ClimateMachine.jl to start faster:\n\n$ julia --project -e 'using Pkg; pkg\"precompile\"'\n\nYou can find more information about ClimateMachine.jl here. ClimateMachine.jl is a rapidly evolving software and this example may stop working in the future, please open an issue if you find that to be the case!","category":"section"},{"location":"examples/ClimateMachine_example/#Structure","page":"HPC interfacing example: ClimateMachine","title":"Structure","text":"The example makes use of julia and bash scripts for interactions with the workload manager and running multiple forward model evaluations in parallel. The user-triggered script ekp_calibration.sbatch initializes and controls the flow of the calibration process, which in this case is a SLURM queue with job dependencies. The calibration bash scripts, in order of execution and with their associated julia scripts, are\n\nekp_init_calibration: Calls init_calibration.jl, which samples the initial parameter ensemble from a specified prior. The initial ensemble is stored in a set of parameter files.\nekp_single_cm_run: Script called in parallel by the workload manager. Each copy of the script submits a single forward model run (i.e., a ClimateMachine.jl run) given a specific pair of parameters (C_smag, C_drag) read from a corresponding file. The output of each forward model run is stored in a separate NetCDF file.\nekp_cont_calibration: Calls sstep_calibration.jl, which reads from a NetCDF file the output generated by ClimateMachine.jl in step 2 and performs an iteration of the Ensemble Kalman Inversion algorithm, updating the parameter ensemble. The new parameters are stored in new parameter files.\n\nThis flow follows steps 1->2->3->2->3->... for a user-specified number of iterations.","category":"section"},{"location":"examples/ClimateMachine_example/#Running-the-Example","page":"HPC interfacing example: ClimateMachine","title":"Running the Example","text":"From the parent directory of ClimateMachine.jl and EnsembleKalmanProcesses.jl, change into the example directory with\n\n$ cd EnsembleKalmanProcesses.jl/examples/ClimateMachine\n\nand install all the required packages for the example with:\n\n$ julia --project -e 'using Pkg; pkg\"instantiate\";'\n\nTo run the example using a SLURM workload manager, simply do:\n\n$ sbatch ekp_calibration.sbatch\n\nThe dynamical model outputs (i.e, Psi(phi)) for all runs of ClimateMachine.jl will be stored in NetCDF format in directories identifiable by their version number. Refer to the files version_XX.txt to identify each run with each ensemble member within the XX iteration of the Ensemble Kalman Process. \n\nIn this example, the parameters are defined through a Gaussian prior in init_calibration.jl. Hence, the transform from constrained to unconstrained space is the identity map, and we have phi=theta. Overall the forward map mathcalG(theta) is given by applying an observation map mathcalH to the dynamical model output Psi. In this case, mathcalH returns the time average of the horizontal velocity over a specified 30 min interval after initialization. Therefore, the observation vector y contains a time-averaged vertical profile of the horizontal velocity.","category":"section"},{"location":"examples/ClimateMachine_example/#Calibration-Solution","page":"HPC interfacing example: ClimateMachine","title":"Calibration Solution","text":"To aggregate the parameter ensembles theta^(1) theta^(2) dots theta^(J) generated during the calibration process, you may use the agg_clima_ekp(...) function located in helper_funcs.jl,\n\n$ julia --project\n\ninclude(joinpath(@__DIR__, \"helper_funcs.jl\"))\n\nagg_clima_ekp(2) # This generates the output containing the ensembles for each iteration, input is the number of parameters\n\nThis will create the JLD file ekp_clima.jld. We may read the file as follows\n\nusing JLD\n\nθ = load(\"ekp_clima.jld\")[\"ekp_u\"]\nprintln(typeof(θ)) # Array{Array{Float64,2},1}, outer dimension is N_iter, inner Array{Float64,2} of size = (J, p)\n\nThe optimal parameter vector determined by the ensemble Kalman inversion is the ensemble mean of the particles after the last iteration. Following the previous script,\n\nusing Statistics\n\nθ_opt = mean(θ[end], dims=1)","category":"section"},{"location":"unscented_kalman_inversion/#uki","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"One of the ensemble Kalman processes implemented in EnsembleKalmanProcesses.jl is the unscented Kalman inversion (Huang, Schneider, Stuart, 2022). The unscented Kalman inversion (UKI) is a derivative-free method for approximate Bayesian inference. This page additionally documents an output-scalable variant, the unscented transform Kalman inversion (UTKI).\n\nWe seek to find the posterior parameter distribution theta in mathbbR^p from the inverse problem\n\n y = mathcalG(theta) + eta\n\nwhere mathcalG denotes the forward map, y in mathbbR^d is the vector of observations and eta sim mathcalN(0 Gamma_y) is additive Gaussian noise. Note that p is the size of the parameter vector theta and d is taken to be the size of the observation vector y. The UKI algorithm has the following properties\n\nUKI has a fixed ensemble size, with members forming a quadrature stencil (rather than the random positioning of the particles from methods such as EKI). There are two quadrature options, symmetric (a 2p + 1-size stencil), and simplex (a p+2-size stencil).\nUKI has uncertainty quantification capabilities, it gives both mean and covariance approximation (no ensemble collapse and no empirical variance inflation) of the posterior distribution, the 3-sigma confidence interval covers the truth parameters for perfect models.","category":"section"},{"location":"unscented_kalman_inversion/#Algorithm","page":"Unscented Kalman Inversion","title":"Algorithm","text":"The UKI applies the unscented Kalman filter to the following stochastic dynamical system\n\nbeginaligned\n  textrmevolution    theta_n+1 = r + alpha (theta_n  - r) +  omega_n+1 omega_n+1 sim mathcalN(0Sigma_omega)\n  textrmobservation  y_n+1 = mathcalG(theta_n+1) + nu_n+1 nu_n+1 sim mathcalN(0Sigma_nu)\nendaligned\n\nThe free parameters in the UKI are alpha r Sigma_nu Sigma_omega. The UKI updates both the mean m_n and covariance C_n estimations of the parameter vector theta as following\n\nPrediction step :\n\nbeginaligned\n    hatm_n+1 =  r+alpha(m_n-r)\n    hatC_n+1 =  alpha^2 C_n + Sigma_omega\nendaligned\n\nGenerate sigma points (\"the ensemble\") :\n\nFor the sigma_points = symmetric quadrature option, the ensemble is generated as follows.\n\nbeginaligned\n    hattheta_n+1^0 = hatm_n+1 \n    hattheta_n+1^j = hatm_n+1 + c_j sqrthatC_n+1_j quad (1leq jleq J) \n    hattheta_n+1^j+J = hatm_n+1 - c_j sqrthatC_n+1_jquad (1leq jleq J)\nendaligned\n\nwhere sqrtC_j is the j-th column of the Cholesky factor of C. \n\nAnalysis step :\n\n   beginaligned\n        haty^j_n+1 = mathcalG(hattheta^j_n+1) qquad haty_n+1 = haty^0_n+1\n         hatC^theta p_n+1 = sum_j=1^2JW_j^c\n        (hattheta^j_n+1 - hatm_n+1 )(haty^j_n+1 - haty_n+1)^T \n        hatC^pp_n+1 = sum_j=1^2JW_j^c\n        (haty^j_n+1 - haty_n+1 )(haty^j_n+1 - haty_n+1)^T + Sigma_nu\n        m_n+1 = hatm_n+1 + hatC^theta p_n+1(hatC^pp_n+1)^-1(y - haty_n+1)\n        C_n+1 = hatC_n+1 - hatC^theta p_n+1(hatC^pp_n+1)^-1hatC^theta p_n+1^T\n    endaligned\n\nWhere the coefficients c_j W^c_j are given by\n\n    beginaligned\n    c_j = asqrtJ qquad W_j^c = frac12a^2J(j=1cdots2N_theta) qquad  a=minsqrtfrac4J  1 \n    endaligned","category":"section"},{"location":"unscented_kalman_inversion/#Choice-of-free-parameters","page":"Unscented Kalman Inversion","title":"Choice of free parameters","text":"The free parameters in the unscented Kalman inversion are alpha r Sigma_nu Sigma_omega, which are chosen based on theorems developed in Huang et al, 2021\n\nthe vector r is set to be the prior mean\nthe scalar alpha in (01 is a regularization parameter, which is used to overcome ill-posedness and overfitting. A practical guide is \nWhen the observation noise is negligible, and there are more observations than parameters (identifiable inverse problem) alpha = 1 (no regularization)\nOtherwise alpha  1. The smaller alpha is, the closer the UKI mean will converge to the prior mean.\nthe matrix Sigma_nu is the artificial observation error covariance. We set Sigma_nu = 2 Gamma_y, which makes the inverse problem consistent. \nthe matrix Sigma_omega is the artificial evolution error covariance. We set Sigma_omega = (2 - alpha^2)Lambda. We choose Lambda as following\nwhen there are more observations than parameters (identifiable inverse problem), Lambda = C_n, which is updated as the estimated covariance C_n in the n-th every iteration. This guarantees the converged covariance matrix is a good approximation to the posterior covariance matrix with an uninformative prior.\notherwise Lambda = C_0, this allows that the converged covariance matrix is a weighted average between the posterior covariance matrix with an uninformative prior and C_0.\n\nIn short, users only need to change the alpha (α_reg), and the frequency to update the Lambda to the current covariance (update_freq). The user can first try α_reg = 1.0 and update_freq = 0 (corresponding to Lambda = C_0).\n\nnote: Preventing ensemble divergence\nIf UKI suffers divergence (for example when inverse problems are not well-posed), one can prevent it by using prior regularization (see Huang, Schneider, Stuart, 2022). It is used by setting the impose_prior = true flag. In this mode, the free parameters are fixed to α_reg = 1.0, update_freq = 1. ","category":"section"},{"location":"unscented_kalman_inversion/#Implementation","page":"Unscented Kalman Inversion","title":"Implementation","text":"","category":"section"},{"location":"unscented_kalman_inversion/#Initialization","page":"Unscented Kalman Inversion","title":"Initialization","text":"An unscented Kalman inversion object can be created using the EnsembleKalmanProcess constructor by specifying the Unscented() process type. The initialization of the Unscented() process requires a prior distribution. We recommend using the defaults, with also the impose_prior=true flag.\n\nusing EnsembleKalmanProcesses\nusing EnsembleKalmanProcesses.ParameterDistributions\n\n# impose_prior=true : prior information is enforced at every iteration (ensures convergence to MAP, false => MLE) \nprocess = Unscented(prior; impose_prior=true) \nukiobj = EnsembleKalmanProcess(observation, process)\n\nget_N_ens(ukiobj) # gets the number of sigma-points based on the dimension of the problem\n\nSome other hyperparameters can also be set, (a full list can be seen with ?Unscented in the REPL)\n\n# need to choose regularization factor α ∈ (0,1],  \n# when you have enough observation data α=1: no regularization\nα_reg =  1.0\n# update_freq 1 : approximate posterior covariance matrix with an uninformative prior\n#             0 : weighted average between posterior covariance matrix with an uninformative prior and prior\nupdate_freq = 0\nimpose_prior=true : additional prior regularization for stability and convergence to MAP estimator\n\n# One can also use the prior mean and covariance\nusing Statistics\nprior_mean = mean(prior)\nprior_cov = cov(prior)\n\nprocess = Unscented(prior_mean, prior_cov; α_reg = α_reg, update_freq = update_freq, impose_prior=impose_prior)\n\nNote that no information about the forward map is necessary to initialize the Unscented process. The only forward map information required by the inversion process consists of model evaluations at the ensemble elements, necessary to update the ensemble.","category":"section"},{"location":"unscented_kalman_inversion/#Constructing-the-Forward-Map","page":"Unscented Kalman Inversion","title":"Constructing the Forward Map","text":"At the core of the forward map mathcalG is the dynamical model PsimathbbR^p rightarrow mathbbR^o (running Psi is usually where the computational heavy-lifting is done), but the map mathcalG may include additional components such as a transformation of the (unbounded) parameters theta to a constrained domain the dynamical model can work with, or some post-processing of the output of Psi to generate the observations. For example, mathcalG may take the following form:\n\nmathcalG = mathcalH circ Psi circ mathcalT^-1\n\nwhere mathcalHmathbbR^o rightarrow mathbbR^d is the observation map and mathcalT is the transformation from the constrained to the unconstrained parameter space, such that mathcalT(phi)=theta. A family of standard transformations and their inverses are available in the ParameterDistributions module.","category":"section"},{"location":"unscented_kalman_inversion/#Updating-the-Ensemble","page":"Unscented Kalman Inversion","title":"Updating the Ensemble","text":"Once the unscented Kalman inversion object UKIobj has been initialized, any number of updates can be performed using the inversion algorithm.\n\nA call to the inversion algorithm can be performed with the update_ensemble! function. This function takes as arguments the UKIobj and the evaluations of the forward map at each element of the current ensemble. The update_ensemble! function then stores the new updated ensemble and the inputted forward map evaluations in UKIobj.\n\nThe forward map mathcalG maps the space of unconstrained parameters theta to the outputs y in mathbbR^d. In practice, the user may not have access to such a map directly. And the map is a composition of several functions. The update_ensemble! uses only the evalutaions g_ens but not the forward map  \n\nFor implementational reasons, the update_ensemble is performed by computing analysis stage first, followed by a calculation of the next sigma ensemble. The first sigma ensemble is created in the initialization.\n\n# Given:\n# Ψ (some black box simulator)\n# H (some observation of the simulator output)\n# prior (prior distribution and parameter constraints)\n\nN_iter = 20 # Number of steps of the algorithm\n \nfor n in 1:N_iter\n    ϕ_n = get_ϕ_final(prior, ukiobj) # Get current ensemble in constrained \"ϕ\"-space\n    G_n = [H(Ψ(ϕ_n[:, i])) for i in 1:J]  # Evaluate forward map\n    g_ens = hcat(G_n...)  # Reformat into `d x N_ens` matrix\n    EnsembleKalmanProcesses.update_ensemble!(ukiobj, g_ens) # Update ensemble\nend","category":"section"},{"location":"unscented_kalman_inversion/#Solution","page":"Unscented Kalman Inversion","title":"Solution","text":"The solution of the unscented Kalman inversion algorithm is a Gaussian distribution whose mean and covariance can be extracted from the ''last ensemble'' (i.e., the ensemble after the last iteration). The sample mean of the last ensemble is also the \"optimal\" parameter (θ_optim) for the given calibration problem. These statistics can be accessed as follows: \n\n# mean of the Gaussian distribution, also the optimal parameter for the calibration problem\nθ_optim = get_u_mean_final(ukiobj)\n# covariance of the Gaussian distribution\nsigma_optim = get_u_cov_final(ukiobj)\n\nThere are two examples: Lorenz96 and Cloudy.","category":"section"},{"location":"unscented_kalman_inversion/#utki","page":"Unscented Kalman Inversion","title":"Output-scalable variant: Unscented Transform Kalman Inversion","text":"Unscented transform Kalman inversion (UTKI) is a variant of UKI based on applying the woodbury formula used in the ensemble transform Kalman filter (Bishop et al., 2001) to UKI update. It is a form of square-root inversion for UKI is that it has better scalability as the observation dimension grows: while the naive implementation of UKI scales as mathcalO(p^3) in the observation dimension p, UTKI scales as mathcalO(p). This, however, refers to the online cost. UTKI may have an offline cost of mathcalO(p^3) if Gamma is not easily invertible; see below.\n\nUTKI requires the inverse observation noise covariance, Gamma^-1. In typical applications, when Gamma is diagonal, this will be cheap to compute; however, if p is very large and Gamma has non-trivial cross-covariance structure, computing the inverse may be prohibitively expensive.\n\nnote: Creating scalable observational covariances\nUTKI requires storing and inverting the observation noise covariance, Gamma^-1. Without care, this can be prohibitively expensive. To this end, we have tools and an API for creating and using scalable or compact representations of covariances that are necessary for scalability. See here for details and examples. ","category":"section"},{"location":"unscented_kalman_inversion/#Using-UTKI","page":"Unscented Kalman Inversion","title":"Using UTKI","text":"An UTKI struct can be created using the EnsembleKalmanProcess constructor by specifying the TransformUnscented process. Configurables, such as keywords are identical to that of the Unscented process.\n\nusing EnsembleKalmanProcesses\n# given the prior distribution `prior`, data `y` and covariance `obs_noise_cov`,\n\nutkiobj = EnsembleKalmanProcess(y, obs_noise_cov, TransformUnscented(prior))\n\nThe rest of the inversion process is the same as for regular UKI.","category":"section"},{"location":"update_groups/#[Update-Groups]-(@id-update-groups)","page":"Update Groups","title":"[Update Groups] (@id update-groups)","text":"The UpdateGroup object facilitates blocked EKP updates, based on a provided updating a series user-defined pairs of parameters and data. This allows users to enforce any known (in)dependences between different groups of parameters during the update. For example, \n\n# update parameter 1 with data 1 and 2\n# update parameters 2 and 3 jointly with data 2, 3, and 4\nDict(\n    [\"parameter_1\"] => [\"data_1\", \"data_2\"], \n    [\"parameter_2\", \"parameter_3\"] => [\"data_2\", \"data_3\", \"data_4\"], \n)\n\nConstruction and passing of this into the EnsembleKalmanProcesses is detailed below.\n\nnote: This improves scaling at the cost of user-imposed structure\nAs many of the Process updates scale say with d^alpha, in the data dimension d and alpha  1 (super-linearly),  update groups with K groups of equal size will improving this scaling to K (fracdK)^alpha.","category":"section"},{"location":"update_groups/#Recommended-construction-shown-by-example","page":"Update Groups","title":"Recommended construction - shown by example","text":"The key component to construct update groups starts with constructing the prior and the observations. Parameter distributions and observations may be constructed in units and given names, and these names are utilized to build the update groups with a convenient constructor create_update_groups.\n\nFor illustration, we take code snippets from the example found here. This example is concerned with learning several parameters in a coupled two-scale Lorenz 96 system:\n\nbeginaligned\n fracpartial X_ipartial t  = -X_i-1(X_i-2 - X_i+1) - X_i - GY_i + F_1 + F_2sin(2pi t F_3)\n fracpartial Y_ipartial t  = -cbY_i+1(Y_i+2 - Y_i-1) - cY_i + frachcb X_i \nendaligned\n\nParameters are learnt by fitting estimated moments of a realized X and Y system, to some target moments over a time interval.\n\nWe create a prior by combining several named ParameterDistributions.\n\nparam_names = [\"F\", \"G\", \"h\", \"c\", \"b\"]\n\nprior_F = ParameterDistribution(\n    Dict(\n        \"name\" => param_names[1],\n        \"distribution\" => Parameterized(MvNormal([1.0, 0.0, -2.0], I)),\n        \"constraint\" => repeat([bounded_below(0)], 3),\n    ),\n) # gives 3-D dist\nprior_G = constrained_gaussian(param_names[2], 5.0, 4.0, 0, Inf)\nprior_h = constrained_gaussian(param_names[3], 5.0, 4.0, 0, Inf)\nprior_c = constrained_gaussian(param_names[4], 5.0, 4.0, 0, Inf)\nprior_b = constrained_gaussian(param_names[5], 5.0, 4.0, 0, Inf)\npriors = combine_distributions([prior_F, prior_G, prior_h, prior_c, prior_b])\n\nNow we likewise construct observed moments by combining several named Observations\n\n# given a list of vector statistics y and their covariances Γ \ndata_block_names = [\"<X>\", \"<Y>\", \"<X^2>\", \"<Y^2>\", \"<XY>\"]\n\nobservation_vec = []\nfor i in 1:length(data_block_names)\n    push!(\n        observation_vec,\n        Observation(Dict(\n            \"samples\" => y[i],\n            \"covariances\" => Γ[i],\n            \"names\" => data_block_names[i]\n        )),\n    )\nend\nobservation = combine_observations(observation_vec)\n\nFinally, we are ready to define the update groups. We may specify our choice by partitioning the parameter names as keys of a dictionary, and their paired data names as values. Here we create two groups:\n\n# update parameters F,G with data <X>, <X^2>, <XY>\n# update parameters h, c, b with data <Y>, <Y^2>, <XY>\ngroup_identifiers = Dict(\n    [\"F\", \"G\"] => [\"<X>\", \"<X^2>\", \"<XY>\"],\n    [\"h\", \"c\", \"b\"] => [\"<Y>\", \"<Y^2>\", \"<XY>\"],\n)\n\nWe then create the update groups with our convenient constructor\n\nupdate_groups = create_update_groups(prior, observation, group_identifiers)\n\nand this can then be entered into the EnsembleKalmanProcess object as a keyword argument\n\n# initial_params = construct_initial_ensemble(rng, priors, N_ens) \nekiobj = EnsembleKalmanProcess(\n    initial_params,\n    observation,\n    Inversion(),\n    update_groups = update_groups\n)","category":"section"},{"location":"update_groups/#What-happens-internally?","page":"Update Groups","title":"What happens internally?","text":"We simply perform an independent update_ensemble! for each provided pairing and combine model output and updated parameters afterwards. Note that even without specifying an update group, by default EKP will always be construct one under-the-hood.","category":"section"},{"location":"update_groups/#Advice-for-constructing-blocks","page":"Update Groups","title":"Advice for constructing blocks","text":"A parameter cannot appear in more than one block (i.e. parameters cannot be updated more than once)\nThe block structure is user-defined, and directly assumes that there is no correlation between blocks. It is up to the user to confirm if there truly is independence between different blocks. Otherwise convergence properties may suffer.\nThis can be used in conjunction with minibatching, so long as the defined data objects are available in all Observations in the series.\n\nnote: In future...\nIn theory this opens up the possibility to have different configurations, or even processes, in different groups. This could be useful when parameter-data pairings are highly heterogeneous and so the user may wish to exploit, for example, the different processes scaling properties. However this has not yet been implemented.","category":"section"},{"location":"examples/Cloudy_example/#Cloudy-example","page":"Cloudy","title":"Cloudy Example","text":"info: How do I run this code?\nThe full code is found in the examples/ directory of the github repository\n\nwarn: version control for Cloudy\nDue to rapid developments in Cloudy, this example will not work with the latest version. It is known to work pinned to specific commit b4fa7e3, please add Cloudy to the example Project using command add Cloudy#b4fa7e3 in Pkg to avoid errors.","category":"section"},{"location":"examples/Cloudy_example/#Overview","page":"Cloudy","title":"Overview","text":"This example is based on Cloudy, a microphysics model that simulates how cloud droplets collide and coalesce into larger drops. Collision-coalescence is a crucial process for the formation of rain. \n\nCloudy is initialized with a mass distribution of the cloud droplets; this distribution is then evolved in time, with more and more droplets colliding and combining into bigger drops according to the droplet-droplet interactions specified by a collision-coalescence kernel. The evolution is completely determined by the shape of the initial distribution and the form of the kernel.\n\nThis example shows how ensemble Kalman methods can be used to learn the parameters of the initial cloud droplet mass distribution from observations of the moments of that mass distribution at a later time. The collision-coalescence kernel is assumed to be known, but one could also learn the parameters of the kernel instead of the parameters of the droplet distribution (or both).\n\nCloudy is used here in a \"perfect model\" (aka \"known truth\") setting, which means that the \"observations\" are generated by Cloudy itself, by running it with the true parameter values. In more realistic applications, this parameter estimation procedure will use actual measurements of cloud properties to obtain an estimated droplet mass distribution at a previous time.","category":"section"},{"location":"examples/Cloudy_example/#Prerequisites","page":"Cloudy","title":"Prerequisites","text":"In order to run this example, you need to install Cloudy.jl (the \"#master\" lets you install the current master branch):\n\npkg > add Cloudy#master","category":"section"},{"location":"examples/Cloudy_example/#Structure","page":"Cloudy","title":"Structure","text":"The file Cloudy_example_eki.jl sets up the inverse problem and solves it using ensemble Kalman inversion, and the file Cloudy_example_uki.jl does the same using unscented Kalman inversion. The file DynamicalModel.jl provides the functionality to run the dynamical model Psi, which in this example is Cloudy.","category":"section"},{"location":"examples/Cloudy_example/#Running-the-Example","page":"Cloudy","title":"Running the Example","text":"Once Cloudy is installed, the examples can be run from the julia REPL:\n\n# Solve inverse problem using ensemble Kalman inversion\ninclude(\"Cloudy_example_eki.jl\")\n\nor\n\n# Solve inverse problem using unscented Kalman inversion\ninclude(\"Cloudy_example_uki.jl\")","category":"section"},{"location":"examples/Cloudy_example/#What-Does-Cloudy-Do?","page":"Cloudy","title":"What Does Cloudy Do?","text":"The mathematical starting point of Cloudy is the stochastic collection equation (SCE; sometimes also called Smoluchowski equation after Marian Smoluchowski), which describes the time rate of change of f = f(m t), the mass distribution function of liquid water droplets, due to the process of collision and coalescence. The distribution function f depends on droplet mass m and time t and is defined such that f(m) text dm denotes the number of droplets with masses in the interval m m + dm per unit volume. \n\nThe stochastic collection equation is an integro-differential equation that can be written as \n\n    fracpartial f(m t)partial t = frac12 int_m=0^infty f(m t) f(m-m t)  mathcalC(m m-m)textdm - f(m t) int_m=0^infty f(m t)mathcalC(m m) textdm \n\nwhere mathcalC(m m) is the collision-coalescence kernel, which  encapsulates the physics of droplet-droplet interactions – it describes the rate at which two drops of masses m and m come into contact and coalesce into a drop of mass m + m. The first term on the right-hand side of the SCE describes the rate of increase of the number of drops having a mass m due to collision and coalescence of drops of masses m and m-m (where the factor frac12 avoids double counting), while the second term describes the rate of reduction of drops of mass m due to collision and coalescence of drops having a mass m with other drops. \n\nWe can rewrite the SCE in terms of the moments M_k of f, which are the prognostic variables in Cloudy. They are defined by\n\n    M_k = int_0^infty m^k f(m t) textdm\n\nThe time rate of change of the k-th moment of f is obtained by multiplying the SCE by m^k and integrating over the entire range of droplet masses (from m=0 to infty), which yields\n\n    fracpartial M_k(t)partial t = frac12int_0^infty left((m+m)^k - m^k - m^kright) mathcalC(m m)f(m t)f(m t)  textdm textdm  (1)\n\nIn this example, the kernel is set to be constant – mathcalC(m m) = B = textconst – and the cloud droplet mass distribution is assumed to be a textGamma(k_t theta_t) distribution, scaled by a factor N_0t which denotes the droplet number concentration:\n\nf(m t) = fracN_0tGamma(k_t)theta_t^k m^k_t-1 exp(-mtheta_t)\n\nThe parameter vector phi_t= N_0t k_t theta_t changes over time (as indicated by the subscript t), as the shape of the distribution evolves. In fact, there is a priori no reason to assume that the distribution would retain its Gamma shape over time, but this is a common assumption that is made in order to solve the closure problem (without this assumption, one would have to keep track of infinitely many moments of the mass distribution in order to uniquely identify the distribution f at each time step, which is obviously not practicable).\n\nFor Gamma mass distribution functions, specifying the first three moments (M_0, M_1, and M_2) is sufficient to uniquely determine the parameter vector phi_t, hence Cloudy solves equation (1) for k = 0 1 2. This mapping of the parameters of the initial cloud droplet mass distribution to the (zeroth-, first-, and second-order) moments of the distribution at a specified end time is done by DynamicalModel.jl.","category":"section"},{"location":"examples/Cloudy_example/#Setting-up-the-Inverse-Problem","page":"Cloudy","title":"Setting up the Inverse Problem","text":"The goal is to learn the distribution  parameters at time t = 0, phi_0 = N_00 k_0 theta_0, from observations y = M_0(t_end) M_1(t_end) M_2(t_end) of the zeroth-, first-, and second-order moments of the distribution at time t_end  0 (where t_end = 1.0 in this example). This is a known truth experiment, in which the true parameters phi_0 texttrue are defined to be:\n\nN0_true = 300.0  # number of particles (scaling factor for Gamma distribution)\nθ_true = 1.5597  # scale parameter of Gamma distribution\nk_true = 0.0817  # shape parameter of Gamma distribution","category":"section"},{"location":"examples/Cloudy_example/#Priors","page":"Cloudy","title":"Priors","text":"In the code, the priors are constructed as follows:\n\npar_names = [\"N0\", \"θ\", \"k\"]\n# constrained_gaussian(\"name\", desired_mean, desired_std, lower_bd, upper_bd)\nprior_N0 = constrained_gaussian(par_names[1], 400, 300, 0.4 * N0_true, Inf)\nprior_θ = constrained_gaussian(par_names[2], 1.0, 5.0, 1e-1, Inf)\nprior_k = constrained_gaussian(par_names[3], 0.2, 1.0, 1e-4, Inf)\npriors = combine_distributions([prior_N0, prior_θ, prior_k])\n\nWe use the recommended constrained_gaussian to add the desired scale and bounds to the prior distribution, in particular we place lower bounds to preserve positivity (and numerical stability). ","category":"section"},{"location":"examples/Cloudy_example/#Observational-Noise","page":"Cloudy","title":"Observational Noise","text":"Cloudy produces output  y = M_0(t_end) M_1(t_end) M_2(t_end), which is assumed to be related to the parameter vector theta according to:\n\n    y = mathcalG(theta) + eta\n\nwhere mathcalG = Psi circ mathcalT^-1 is the forward map, and the observational noise eta is assumed to be drawn from a  3-dimensional  Gaussian  with distribution mathcalN(0 Gamma_y). In a perfect model setting, the observational noise represents the internal model variability. Since Cloudy is a purely deterministic model, there is no straightforward way of coming up with a covariance Gamma_y for this internal noise. We decide to use a diagonal covariance with the following entries (variances):\n\nΓy = convert(Array, Diagonal([100.0, 5.0, 30.0]))\n\nArtificial observations (\"truth samples\") are then generated by adding random samples from eta to G_t, the forward map evaluated for the true parameters:\n\nfor i in 1:n_samples\n    y_t[:, i] = G_t .+ rand(MvNormal(μ, Γy))\nend\n\ntruth = Observation(y_t, Γy, data_names)","category":"section"},{"location":"examples/Cloudy_example/#Solution-and-Output","page":"Cloudy","title":"Solution and Output","text":"Cloudy_example_eki.jl: The optimal parameter vector determined by the ensemble Kalman inversion is the ensemble mean of the particles after the last iteration, which is printed to standard output. An output directory is created, where two files are stored: parameter_storage_eki.jld2 and data_storage_eki.jld2, which contain all parameters and model output from the ensemble Kalman iterations, respectively (both as DataContainers.DataContainer objects). In addition, an animation is produced that shows the evolution of the ensemble of particles over subsequent iterations of the optimization, both in the computational (unconstrained) and physical (constrained) spaces.\nCloudy_example_uki.jl: In addition to a point estimate of the optimal parameter (which is again given by the ensemble mean of the last iteration and printed to standard output), unscented Kalman inversion also provides a covariance approximation of the posterior distribution. Together, the mean and covariance allow for the reconstruction of a Gaussian approximation of the posterior distribution. The evolution of this Gaussian approximation over subsequent iterations is shown as an animation over the computational (unconstrained) space. All parameters as well as the model output from the unscented Kalman inversion are stored in an output directory, as parameter_storage_uki.jld2 and data_storage_uki.jld2. ","category":"section"},{"location":"examples/Cloudy_example/#Playing-Around","page":"Cloudy","title":"Playing Around","text":"If you want to play around with the Cloudy examples, you can e.g. change the type or the parameters of the initial cloud droplet mass distribution (see Cloudy.ParticleDistributions for the available distributions), by modifying these lines:\n\nϕ_true = [N0_true, θ_true, k_true]\ndist_true = ParticleDistributions.GammaPrimitiveParticleDistribution(ϕ_true...)\n\n(Don't forget to also change dist_type accordingly).\n\nYou can also experiment with different noise covariances (Γy), priors, vary the number of iterations (N_iter) or ensemble particles (N_ens), etc.","category":"section"},{"location":"API/SparseInversion/#Sparse-Ensemble-Kalman-Inversion","page":"SparseInversion","title":"Sparse Ensemble Kalman Inversion","text":"","category":"section"},{"location":"API/SparseInversion/#EnsembleKalmanProcesses.SparseInversion","page":"SparseInversion","title":"EnsembleKalmanProcesses.SparseInversion","text":"SparseInversion <: Process\n\nA sparse ensemble Kalman Inversion process\n\nFields\n\nγ::AbstractFloat: upper limit of l1-norm\nthreshold_value::AbstractFloat: threshold below which the norm of parameters is pruned to zero\nuc_idx::Union{Colon, AbstractVector}: indices of parameters included in the evaluation of l1-norm constraint\nreg::AbstractFloat: a small regularization value to enhance robustness of convex optimization\n\nConstructors\n\nSparseInversion(; γ, threshold_value, uc_idx, reg)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/SparseEnsembleKalmanInversion.jl:20.\n\nSparseInversion(γ, threshold_value, uc_idx, reg)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/SparseEnsembleKalmanInversion.jl:21.\n\nSparseInversion(γ; threshold_value, uc_idx, reg)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/SparseEnsembleKalmanInversion.jl:31.\n\n\n\n\n\n","category":"type"},{"location":"API/SparseInversion/#EnsembleKalmanProcesses.sparse_eki_update","page":"SparseInversion","title":"EnsembleKalmanProcesses.sparse_eki_update","text":" sparse_eki_update(\n    ekp::EnsembleKalmanProcess{FT, IT, SparseInversion{FT}},\n    u::AbstractMatrix{FT},\n    g::AbstractMatrix{FT},\n    y::AbstractMatrix{FT},\n    obs_noise_cov::Union{AbstractMatrix{CT}, UniformScaling{CT}},\n) where {FT <: Real, CT <: Real, IT}\n\nReturns the sparse updated parameter vectors given their current values and the corresponding forward model evaluations, using the inversion algorithm from eqns. (3.7) and (3.14) of Schneider et al. (2021).\n\nLocalization is applied following Tong and Morzfeld (2022).\n\n\n\n\n\n","category":"function"},{"location":"API/SparseInversion/#EnsembleKalmanProcesses.sparse_qp","page":"SparseInversion","title":"EnsembleKalmanProcesses.sparse_qp","text":"sparse_qp(\n    ekp::EnsembleKalmanProcess{FT, IT, SparseInversion{FT}},\n    v_j::Vector{FT},\n    cov_vv_inv::AbstractMatrix{FT},\n    H_u::SparseArrays.SparseMatrixCSC{FT},\n    H_g::SparseArrays.SparseMatrixCSC{FT},\n    y_j::Vector{FT};\n    H_uc::SparseArrays.SparseMatrixCSC{FT} = H_u,\n) where {FT, IT}\n\nSolving quadratic programming problem with sparsity constraint.\n\n\n\n\n\n","category":"function"},{"location":"literated/loss_minimization_sparse_eki/#Minimization-of-simple-loss-functions-with-sparse-EKI","page":"Sparse Minimization Loss","title":"Minimization of simple loss functions with sparse EKI","text":"info: How do I run this code?\nThe full code is found in the examples/ directory of the github repository\n\nFirst we load the required packages.\n\nusing Distributions, LinearAlgebra, Random, Plots\n\nusing EnsembleKalmanProcesses\nusing EnsembleKalmanProcesses.ParameterDistributions\nconst EKP = EnsembleKalmanProcesses","category":"section"},{"location":"literated/loss_minimization_sparse_eki/#Loss-function-with-single-minimum","page":"Sparse Minimization Loss","title":"Loss function with single minimum","text":"Here, we minimize the loss function\n\nG₁(u) = u - u_* \n\nwhere u is a 2-vector of parameters and u_* is given; here u_* = (1 0).\n\nu★ = [1, 0]\nG₁(u) = [sqrt((u[1] - u★[1])^2 + (u[2] - u★[2])^2)]\nnothing # hide\n\nWe set the seed for pseudo-random number generator for reproducibility.\n\nrng_seed = 41\nrng = Random.seed!(Random.GLOBAL_RNG, rng_seed)\nnothing # hide\n\nWe set a stabilization level, which can aid the algorithm convergence\n\ndim_output = 1\nstabilization_level = 1e-3\nΓ_stabilization = stabilization_level * Matrix(I, dim_output, dim_output)\n\nThe functional is positive so to minimize it we may set the target to be 0,\n\nG_target = [0]\nnothing # hide","category":"section"},{"location":"literated/loss_minimization_sparse_eki/#Prior-distributions","page":"Sparse Minimization Loss","title":"Prior distributions","text":"As we work with a Bayesian method, we define a prior. This will behave like an \"initial guess\" for the likely region of parameter space we expect the solution to live in. Here we define Normal(02^2) distributions with no constraints\n\nprior_u1 = constrained_gaussian(\"u1\", 0, 2, -Inf, Inf)\nprior_u2 = constrained_gaussian(\"u1\", 0, 2, -Inf, Inf)\nprior = combine_distributions([prior_u1, prior_u2])\nnothing # hide\n\nnote: Note\nIn this example there are no constraints, therefore no parameter transformations.","category":"section"},{"location":"literated/loss_minimization_sparse_eki/#Calibration","page":"Sparse Minimization Loss","title":"Calibration","text":"We choose the number of ensemble members and the number of iterations of the algorithm\n\nN_ensemble = 20\nN_iterations = 10\nnothing # hide\n\nThe initial ensemble is constructed by sampling the prior\n\ninitial_ensemble = EKP.construct_initial_ensemble(rng, prior, N_ensemble)\n\nSparse EKI parameters\n\nγ = 1.0\nthreshold_value = 1e-2\nreg = 1e-3\nuc_idx = [1, 2]\n\nprocess = SparseInversion(γ, threshold_value, uc_idx, reg)\n\nWe then initialize the Ensemble Kalman Process algorithm, with the initial ensemble, the target, the stabilization and the process type (for sparse EKI this is SparseInversion).\n\nensemble_kalman_process = EKP.EnsembleKalmanProcess(initial_ensemble, G_target, Γ_stabilization, process)\nnothing # hide\n\nThen we calibrate by (i) obtaining the parameters, (ii) calculate the loss function on the parameters (and concatenate), and last (iii) generate a new set of parameters using the model outputs:\n\nfor i in 1:N_iterations\n    params_i = get_u_final(ensemble_kalman_process)\n\n    g_ens = hcat([G₁(params_i[:, i]) for i in 1:N_ensemble]...)\n\n    EKP.update_ensemble!(ensemble_kalman_process, g_ens)\nend\n\nand visualize the results:\n\nu_init = get_u_prior(ensemble_kalman_process)\n\nanim_unique_minimum = @animate for i in 1:N_iterations\n    u_i = get_u(ensemble_kalman_process, i)\n\n    plot(\n        [u★[1]],\n        [u★[2]],\n        seriestype = :scatter,\n        markershape = :star5,\n        markersize = 11,\n        markercolor = :red,\n        label = \"optimum u⋆\",\n    )\n\n    plot!(\n        u_i[1, :],\n        u_i[2, :],\n        seriestype = :scatter,\n        xlims = extrema(u_init[1, :]),\n        ylims = extrema(u_init[2, :]),\n        xlabel = \"u₁\",\n        ylabel = \"u₂\",\n        markersize = 5,\n        markeralpha = 0.6,\n        markercolor = :blue,\n        label = \"particles\",\n        title = \"EKI iteration = \" * string(i),\n    )\nend\nnothing # hide\n\nThe results show that the minimizer of G_1 is u=u_*.\n\ngif(anim_unique_minimum, \"unique_minimum_sparse.gif\", fps = 1) # hide\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"#EnsembleKalmanProcesses","page":"Home","title":"EnsembleKalmanProcesses","text":"EnsembleKalmanProcesses.jl (EKP) is a library of derivative-free Bayesian optimization techniques based on ensemble Kalman Filters, a well known family of approximate filters used for data assimilation. The tools in this library enable fitting parameters found in expensive black-box computer codes without the need for adjoints or derivatives. This property makes them particularly useful when calibrating non-deterministic models, or when the training data are noisy.","category":"section"},{"location":"#Our-processes-and-quick-recommendations","page":"Home","title":"Our processes and quick recommendations","text":"Here are loose recommendations and rough scalability in the current implementations\n\nPlayground option: Inversion (10^3 inputs, 10^3 outputs) - simple, and  handles large input spaces, and very modifiable with all the bells-and-whistles of the package. \nEfficient option: TransformUnscented (10^1 inputs, 10^7 outputs) - Very efficient, and quick converging for large outputs. However, strongly couples ensemble-size to input dimension, and can not as robust to model failures and noise.\nScalable and Robust option: TransformInversion (10^2 inputs, 10^7 outputs) - Less efficient convergence than TransformUnscented, but only weakly couples ensemble size with input dimension, and more robust to model failures and noise. \nWith uncertainty: Sampler (10^2 inputs, 10^3 outputs) - generally slower to converge than inversion tools, but the final ensemble spread quantifies uncertainty.","category":"section"},{"location":"#Quick-links!","page":"Home","title":"Quick links!","text":"How do I build prior distributions?\nHow do I access parameters/outputs from the ekp object?\nHow do I plot convergence errors or parameter distributions?\nHow do I build good observational noise covariances\nHow do I build my observations and encode batching?\nWhat ensemble size should I take? Which process should I use? What is the recommended configuration?\nWhat is the difference between get_u and get_ϕ? Why do the stored parameters apperar to be outside their bounds?\nWhat can be parallelized? How do I do it in Julia?\nWhat is going on in my own code?\nWhat is this error/warning/message?\nWhere can i walk through a simple example?\n\nLearning the amplitude and vertical shift of a sine curve (Image: Ensemble of parameter estimates by iteration) See full example for the code.","category":"section"},{"location":"#The-library","page":"Home","title":"The library","text":"Currently, the following processes are implemented in the library. More details given on respective pages:\n\nInversion() creates Ensemble Kalman Inversion (EKI) \"finite time\" - The traditional optimization technique based on the (perturbed-observation-based) Ensemble Kalman Filter EnKF (Iglesias, Law, Stuart, 2013). This takes a transport view, initializing ensembles at the prior, and the posterior mode and (roughty-approximated) uncertainty) are estimated at finite algorithm time.\n\n<img src=\"assets/animations/animated_inversion-finite.gif\" width=\"300\"> <img src=\"assets/animations/animated_inversion-finite_stochG.gif\" width=\"300\">\n\nInversion(prior) creates Ensemble Kalman Inversion (EKI) \"infinite time\" - EKI with an augmented state that enforces the prior, (e.g., TEKI Chada, Stuart, Tong). Can be initialized off-the-prior, and ensemble collapses to the posterior mode at infinite algorithm time (e.g., Section 4.5 of Calvello, Reich, Stuart).\n\n<img src=\"assets/animations/animated_inversion-infinite.gif\" width=\"300\"> <img src=\"assets/animations/animated_inversion-infinite_stochG.gif\" width=\"300\">\n\nTransformInversion() Ensemble Transform Kalman Inversion (ETKI) \"finite time\" - An optimization technique based on the (square-root-based) ensemble transform Kalman filter  (Bishop et al., 2001, Huang et al., 2022).\n\n<img src=\"assets/animations/animated_transform-finite.gif\" width=\"300\"> <img src=\"assets/animations/animated_transform-finite_stochG.gif\" width=\"300\">\n\nTransformInversion(prior) Ensemble Transform Kalman Inversion (ETKI) \"infinite time\" - ETKI with an augmented state that enforces the prior. (see EKI \"infinite time\")\n\n<img src=\"assets/animations/animated_transform-infinite.gif\" width=\"300\"> <img src=\"assets/animations/animated_transform-infinite_stochG.gif\" width=\"300\">\n\nGaussNewtonInversion(prior) Gauss Newton Kalman Inversion (GNKI) [a.k.a. Iterative Ensemble Kalman Filter with Satistical Linearization] - An optimization technique based on the Gauss Newton optimization update and the iterative extended Kalman filter (Chada et al., 2021, Chen & Oliver, 2013),\n\n<img src=\"assets/animations/animated_gauss-newton.gif\" width=\"300\"> <img src=\"assets/animations/animated_gauss-newton_stochG.gif\" width=\"300\">\n\nSampler(prior) Ensemble Kalman Sampler (EKS) - also obtains a Gaussian Approximation of the posterior distribution, through a Monte Carlo integration Garbuno-Inigo et al, 2020, (\"ALDI\" variant)\n\n<img src=\"assets/animations/animated_sampler.gif\" width=\"300\"> <img src=\"assets/animations/animated_sampler_stochG.gif\" width=\"300\">\n\nUnscented(prior) Unscented Kalman Inversion (UKI) - also obtains a Gaussian Approximation of the posterior distribution, through a quadrature based integration approach (Huang, Schneider, Stuart, 2022),\n\n<img src=\"assets/animations/animated_unscented-infinite.gif\" width=\"300\"> <img src=\"assets/animations/animated_unscented-infinite_stochG.gif\" width=\"300\">\n\nTransformUnscented(prior) Unscented Kalman Inversion (UKI) - An implementation of the UKI algorithm based on the linear-algebra tricks of the square-root filter (see ETKI).\n\n<img src=\"assets/animations/animated_transform-unscented-infinite.gif\" width=\"300\"> <img src=\"assets/animations/animated_transform-unscented-infinite_stochG.gif\" width=\"300\">\n\nSparseInversion(prior) Sparsity-inducing Ensemble Kalman Inversion (SEKI) - Additionally adds approximate L^0 and L^1 penalization to the EKI (Schneider, Stuart, Wu, 2020).\n\nModule Purpose\nEnsembleKalmanProcesses.jl Collection of all tools\nEnsembleKalmanProcess.jl Implementations of EKI, ETKI, EKS, UKI, and SEKI\nObservations.jl Structure to hold observational data and minibatching\nParameterDistributions.jl Structures to hold prior and posterior distributions\nDataContainers.jl Structure to hold model parameters and outputs\nLocalizers.jl Covariance localization kernels","category":"section"},{"location":"#Authors","page":"Home","title":"Authors","text":"EnsembleKalmanProcesses.jl is being developed by the Climate Modeling Alliance. The main developers are Oliver R. A. Dunbar and Ignacio Lopez-Gomez.","category":"section"},{"location":"parallel_hpc/#parallel-hpc","page":"Parallelism and HPC","title":"Parallelism and High Performance Computing (HPC)","text":"One benefit of ensemble methods is their ability to be parallelized. The parallellism occurs outside of the EKP update, and so is not present in the source code. On this page we provide suggestions for parallelization for two types of problems:\n\nRun a parallel loop or map within your current Julia session.\nRun a parallel loop through a read/write file interface and workload manager. We provide some utilities to read/write files in a TOML format.","category":"section"},{"location":"parallel_hpc/#Case-1:-Parallel-code-within-Julia","page":"Parallelism and HPC","title":"Case 1: Parallel code within Julia","text":"Let's look at the simple example for the Lorenz 96 dynamical system. In particular we'll focus on the evaluations of the Lorenz 96 solver explicitly written in GModel.jl that contains the following loop over the N_ens-sized ensemble:\n\nfunction run_ensembles(settings, lorenz_params, nd, N_ens)\n    g_ens = zeros(nd, N_ens)\n    for i in 1:N_ens\n        # run the model with the current parameters, i.e., map θ to G(θ)\n        g_ens[:, i] = lorenz_forward(settings, lorenz_params[i])\n    end\n    return g_ens\nend\n\nEach ensemble member i runs the Lorenz 96 model with settings configuration, and model parameters lorenz_params[i]. The runs do not interact with each other, and the user has several options to parallelize.","category":"section"},{"location":"parallel_hpc/#Running-examples:","page":"Parallelism and HPC","title":"Running examples:","text":"All of the following example cases are covered in distributed_Lorenz_example.jl. At the top of file uncomment one of the following options\n\ncase = multithread \ncase = pmap \ncase = distfor ","category":"section"},{"location":"parallel_hpc/#Multithreading,-@threads","page":"Parallelism and HPC","title":"Multithreading, @threads","text":"To parallelize with multithreading, julia must call the file with a prespecified number of threads. For example, for 4 threads, \n\n$ julia --project -t 4 distributed_Lorenz_example.jl\n\nWe exploit the multithreading over N_ens ensemble members in this example with the following loop in GModel_multithread.jl:\n\nfunction run_ensembles(settings, lorenz_params, nd, N_ens)\n    g_ens = zeros(nd, N_ens)\n    Threads.@threads for i in 1:N_ens\n        # run the model with the current parameters, i.e., map θ to G(θ)\n        g_ens[:, i] = lorenz_forward(settings, lorenz_params[i])\n    end\n    return g_ens\nend\n\nYou can read more about multi-threading here.","category":"section"},{"location":"parallel_hpc/#Parallel-map,-pmap","page":"Parallelism and HPC","title":"Parallel map, pmap","text":"When using multiple processes, the Julia environment must first be loaded on each worker processor. We include these lines in the main file\n\nusing Distributed\naddprocs(4; exeflags = \"--project\")\n\nAnd we would call the file is called by\n\n$ julia --project distributed_Lorenz_example\n\nThis ensures that we obtain 4 worker processes that are loaded with julia's current environment specified by --project (unlike when calling julia --project -p 4). We use  pmap to apply a function to each element of the list (i.e the ensemble member configurations). For example, see the following code from GModel_pmap.jl,\n\nusing Distributed\nfunction run_ensembles(settings, lorenz_params, nd, N_ens)\n    g_ens = zeros(nd, N_ens)\n    g_ens[:, :] = vcat(pmap(x -> lorenz_forward(settings, x), lorenz_params)...)\n    return g_ens\nend\n\nIf pmap is called within a module, that module will also need to be loaded on all workers. For this we use the macro @everywhere module XYZ.\n\nYou can read more about pmap here.","category":"section"},{"location":"parallel_hpc/#Distributed-loop,-@distributed-for","page":"Parallelism and HPC","title":"Distributed loop, @distributed for","text":"When using multiple processes, the Julia environment must also be loaded on each worker processor. We include these lines in the main file\n\nusing Distributed\naddprocs(4; exeflags = \"--project\")\n\nAnd we would call the file is called by\n\n$ julia --project distributed_Lorenz_example\n\nWhen using distributed loops, it is necessary to be able to write to shared memory. To do this we use the SharedArrays package. For example, see the following distributed loop in GModel_distfor \n\nusing Distributed\nusing SharedArrays\nfunction run_ensembles(settings, lorenz_params, nd, N_ens)\n    g_ens = SharedArray{Float64}(nd, N_ens)\n    @sync @distributed for i in 1:N_ens\n        # run the model with the current parameters, i.e., map θ to G(θ)\n        g_ens[:, i] = lorenz_forward(settings, lorenz_params[i])\n    end\n    return g_ens\nend\n\n@sync forces the code to wait until all processes in the @distributed for loop are complete before continuing.\n\nIf @distributed for is used within another module, that module will also need to be loaded on each worker processor. For this we use the macro @everywhere module XYZ.\n\nnote: Note\n@distributed for is most performant when there is a large ensemble, N_ens, and the forward map is computationally cheap. Otherwise, pmap is usually the preferred choice.\n\nYou can read more about @distributed here","category":"section"},{"location":"parallel_hpc/#Case-2:-HPC-interface","page":"Parallelism and HPC","title":"Case 2: HPC interface","text":"Some applications involve interfacing with non-Julia code or using HPC workload managers. In these cases we suggest using an alternative workflow where one interleaves scripts that launch EKP updates and scripts that runs the model. One possible implementation is the following loop\n\nStep 0(a). Write an ensemble of parameter files parameters_0_i for i = 1:N_ens, with each parameter file containing a sample from the prior distribution.\nStep 0(b). Construct EKP EKP object and save in jld2 format e.g. ekpobject.jld2.\nfor n = 1,..., N_it, do:\nStep n(a). Run N_ens forward models, with forward model i running with the corresponding parameter file parameters_{n-1}_i. Write processed output data to file data_{n-1}_i.\nStep n(b). Run the ensemble update, by loading both ekpobject.jld2 and reading in the parameter files data_{n-1}_i for i = 1:N_ens. Perform the EKP update step. Write new parameter files parameters_n_i for i = 1:N_ens. Save the ensemble object in ekpobject.jld2\niterate n -> n+1.\n\nFor a simple implementation of this, please see the example in examples/SinusoidInterface, which is a runnable reimplementation of our sinusoid example in such a formulation.\n\nIn HPC interfacing example: ClimateMachine we implement a similar loop to interface with a SLURM workload manager for HPC. Here, sbatch scripts are used to run each component of the calibration procedure. The outer loop over the EKP iterations lives in the overarching sbatch script, and for each iteration, the inner loop are realised as \"arrays\" of slurm jobs (1, ..., N_ens), launched for each ensemble member. The code excerpt below, taken from ekp_calibration.sbatch for details), illustrates this procedure:\n\n# First call to calibrate.jl will create the ensemble files from the priors\nid_init_ens=$(sbatch --parsable ekp_init_calibration.sbatch)\nfor it in $(seq 1 1 $n_it)\ndo\n# Parallel runs of forward model\nif [ \"$it\" = \"1\" ]; then\n    id_ens_array=$(sbatch --parsable --kill-on-invalid-dep=yes --dependency=afterok:$id_init_ens --array=1-$n ekp_single_cm_run.sbatch $it)\nelse\n    id_ens_array=$(sbatch --parsable --kill-on-invalid-dep=yes --dependency=afterok:$id_ek_upd --array=1-$n ekp_single_cm_run.sbatch $it)\nfi\nid_ek_upd=$(sbatch --parsable --kill-on-invalid-dep=yes --dependency=afterok:$id_ens_array --export=n=$n ekp_cont_calibration.sbatch $it)\ndone\n\nHere a dependency tree is set up in SLURM, which iterates calls to the scripts ekp_single_cm_run.sbatch (which runs the forward model on HPC) and ekp_cont_calibration.sbatch (which performs an EKI update). We find this a smooth workflow that uses HPC resources well, and can likely be set up on other workload managers.\n\nFor more details see the code and docs for the HPC interfacing example: ClimateMachine.","category":"section"},{"location":"API/DataContainers/#DataContainers","page":"DataContainers","title":"DataContainers","text":"","category":"section"},{"location":"API/DataContainers/#EnsembleKalmanProcesses.DataContainers.DataContainer","page":"DataContainers","title":"EnsembleKalmanProcesses.DataContainers.DataContainer","text":"struct DataContainer{FT<:Real}\n\nContainer to store data samples as columns in an array.\n\nFields\n\ndata::AbstractMatrix{FT} where FT<:Real: stored data, each piece of data is a column [data dimension × number samples]\n\nConstructors\n\nDataContainer(data::AVorM; data_are_columns = true) where {AVorM <: AbstractVecOrMat}\n\n\n\n\n\n","category":"type"},{"location":"API/DataContainers/#EnsembleKalmanProcesses.DataContainers.PairedDataContainer","page":"DataContainers","title":"EnsembleKalmanProcesses.DataContainers.PairedDataContainer","text":"struct PairedDataContainer{FT<:Real}\n\nStores input - output pairs as data containers, there must be an equal number of inputs and outputs.\n\nFields\n\ninputs::EnsembleKalmanProcesses.DataContainers.DataContainer: container for inputs, each Container holds an array size [data/parameter dimension × number samples]\noutputs::EnsembleKalmanProcesses.DataContainers.DataContainer: container for ouputs, each Container holds an array size [data/parameter dimension × number samples]\n\nConstructors\n\nPairedDataContainer(\n    inputs::AVorM1,\n    outputs::AVorM2;\n    data_are_columns = true,\n) where {AVorM1 <: AbstractVecOrMat, AVorM2 <: AbstractVecOrMat}\n\nPairedDataContainer(inputs::DataContainer, outputs::DataContainer)\n\n\n\n\n\n","category":"type"},{"location":"API/DataContainers/#Base.size","page":"DataContainers","title":"Base.size","text":"size(\n    c::EnsembleKalmanProcesses.ParameterDistributions.ConstraintType\n) -> Tuple{Int64}\n\n\nA constraint has size 1.\n\n\n\n\n\nsize(dc::DataContainer, idx::IT) where {IT <: Integer}\n\nReturns the size of the stored data. If idx provided, it returns the size along dimension idx.\n\n\n\n\n\nsize(pdc::PairedDataContainer, idx::IT) where {IT <: Integer}\n\nReturns the sizes of the inputs and ouputs along dimension idx (if provided).\n\n\n\n\n\n","category":"function"}]
}
