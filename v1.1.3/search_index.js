var documenterSearchIndex = {"docs":
[{"location":"API/Sampler/#Ensemble-Kalman-Sampler","page":"Sampler","title":"Ensemble Kalman Sampler","text":"","category":"section"},{"location":"API/Sampler/","page":"Sampler","title":"Sampler","text":"CurrentModule = EnsembleKalmanProcesses","category":"page"},{"location":"API/Sampler/","page":"Sampler","title":"Sampler","text":"Sampler\neks_update","category":"page"},{"location":"API/Sampler/#EnsembleKalmanProcesses.Sampler","page":"Sampler","title":"EnsembleKalmanProcesses.Sampler","text":"Sampler{FT<:AbstractFloat,IT<:Int} <: Process\n\nAn ensemble Kalman Sampler process.\n\nFields\n\nprior_mean::Vector{FT} where FT<:AbstractFloat\nMean of Gaussian parameter prior in unconstrained space\nprior_cov::Union{LinearAlgebra.UniformScaling{FT}, AbstractMatrix{FT}} where FT<:AbstractFloat\nCovariance of Gaussian parameter prior in unconstrained space\n\nConstructors\n\nSampler(prior_mean, prior_cov)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/EnsembleKalmanSampler.jl:17.\n\nSampler(prior)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/EnsembleKalmanSampler.jl:23.\n\n\n\n\n\n","category":"type"},{"location":"API/Sampler/#EnsembleKalmanProcesses.eks_update","page":"Sampler","title":"EnsembleKalmanProcesses.eks_update","text":" eks_update(\n    ekp::EnsembleKalmanProcess{FT, IT, Sampler{FT}},\n    u::AbstractMatrix{FT},\n    g::AbstractMatrix{FT},\n) where {FT <: Real, IT}\n\nReturns the updated parameter vectors given their current values and the corresponding forward model evaluations, using the sampler algorithm.\n\nThe current implementation assumes that rows of u and g correspond to ensemble members, so it requires passing the transpose of the u and g arrays associated with ekp.\n\n\n\n\n\n","category":"function"},{"location":"API/Unscented/#Unscented-Kalman-Inversion","page":"Unscented","title":"Unscented Kalman Inversion","text":"","category":"section"},{"location":"API/Unscented/","page":"Unscented","title":"Unscented","text":"CurrentModule = EnsembleKalmanProcesses","category":"page"},{"location":"API/Unscented/","page":"Unscented","title":"Unscented","text":"Unscented\nconstruct_sigma_ensemble\nconstruct_mean\nconstruct_cov\nupdate_ensemble_prediction!\nupdate_ensemble_analysis!","category":"page"},{"location":"API/Unscented/#EnsembleKalmanProcesses.Unscented","page":"Unscented","title":"EnsembleKalmanProcesses.Unscented","text":"Unscented{FT<:AbstractFloat, IT<:Int} <: Process\n\nAn unscented Kalman Inversion process.\n\nFields\n\nu_mean::Any\nan interable of arrays of size N_parameters containing the mean of the parameters (in each uki iteration a new array of mean is added), note - this is not the same as the ensemble mean of the sigma ensemble as it is taken prior to prediction\nuu_cov::Any\nan iterable of arrays of size (N_parameters x N_parameters) containing the covariance of the parameters (in each uki iteration a new array of cov is added), note - this is not the same as the ensemble cov of the sigma ensemble as it is taken prior to prediction\nobs_pred::Any\nan iterable of arrays of size N_y containing the predicted observation (in each uki iteration a new array of predicted observation is added)\nc_weights::AbstractVecOrMat{FT} where FT<:AbstractFloat\nweights in UKI\nmean_weights::AbstractVector{FT} where FT<:AbstractFloat\ncov_weights::AbstractVector{FT} where FT<:AbstractFloat\nN_ens::Int64\nnumber of particles 2N+1 or N+2\nΣ_ω::AbstractMatrix{FT} where FT<:AbstractFloat\ncovariance of the artificial evolution error\nΣ_ν_scale::AbstractFloat\ncovariance of the artificial observation error\nα_reg::AbstractFloat\nregularization parameter\nr::AbstractVector{FT} where FT<:AbstractFloat\nregularization vector\nupdate_freq::Int64\nupdate frequency\nimpose_prior::Bool\nusing augmented system (Tikhonov regularization with Kalman inversion in Chada     et al 2020 and Huang et al (2022)) to regularize the inverse problem, which also imposes prior     for posterior estimation.\nprior_mean::Any\nprior mean - defaults to initial mean\nprior_cov::Any\nprior covariance - defaults to initial covariance\niter::Int64\ncurrent iteration number\n\nConstructors\n\nUnscented(\n    u0_mean::AbstractVector{FT},\n    uu0_cov::AbstractMatrix{FT};\n    α_reg::FT = 1.0,\n    update_freq::IT = 0,\n    modified_unscented_transform::Bool = true,\n    impose_prior::Bool = false,\n    prior_mean::Any,\n    prior_cov::Any,\n    sigma_points::String = symmetric\n) where {FT <: AbstractFloat, IT <: Int}\n\nConstruct an Unscented Inversion Process.\n\nInputs:\n\nu0_mean: Mean at initialization.\nuu0_cov: Covariance at initialization.\nα_reg: Hyperparameter controlling regularization toward the prior mean (0 < α_reg ≤ 1),\n\ndefault should be 1, without regulariazion.\n\nupdate_freq: Set to 0 when the inverse problem is not identifiable, \n\nnamely the inverse problem has multiple solutions, the covariance matrix   will represent only the sensitivity of the parameters, instead of   posterior covariance information; set to 1 (or anything > 0) when   the inverse problem is identifiable, and the covariance matrix will   converge to a good approximation of the posterior covariance with an   uninformative prior.\n\nmodified_unscented_transform: Modification of the UKI quadrature given in Huang et al (2021).\nimpose_prior: using augmented system (Tikhonov regularization with Kalman inversion in Chada   et al 2020 and Huang et al (2022)) to regularize the inverse problem, which also imposes prior   for posterior estimation. If impose_prior == true, prior mean and prior cov must be provided.   This is recommended to use, especially when the number of observations is smaller than the number   of parameters (ill-posed inverse problems). When this is used, other regularizations are turned off  automatically.\nprior_mean: Prior mean used for regularization.\nprior_cov: Prior cov used for regularization.\nsigma_points: String of sigma point type, it can be symmetric with 2N_par+1   ensemble members or simplex with N_par+2 ensemble members.\n\nUnscented(u_mean, uu_cov, obs_pred, c_weights, mean_weights, cov_weights, N_ens, Σ_ω, Σ_ν_scale, α_reg, r, update_freq, impose_prior, prior_mean, prior_cov, iter)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/UnscentedKalmanInversion.jl:57.\n\nUnscented(u0_mean, uu0_cov)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/UnscentedKalmanInversion.jl:91.\n\nUnscented(prior)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/UnscentedKalmanInversion.jl:212.\n\n\n\n\n\n","category":"type"},{"location":"API/Unscented/#EnsembleKalmanProcesses.construct_sigma_ensemble","page":"Unscented","title":"EnsembleKalmanProcesses.construct_sigma_ensemble","text":"construct_sigma_ensemble(\n    process::Unscented,\n    x_mean::Array{FT},\n    x_cov::AbstractMatrix{FT},\n) where {FT <: AbstractFloat, IT <: Int}\n\nConstruct the sigma ensemble based on the mean x_mean and covariance x_cov.\n\n\n\n\n\n","category":"function"},{"location":"API/Unscented/#EnsembleKalmanProcesses.construct_mean","page":"Unscented","title":"EnsembleKalmanProcesses.construct_mean","text":"construct_mean(\n    uki::EnsembleKalmanProcess{FT, IT, Unscented},\n    x::AbstractVecOrMat{FT};\n    mean_weights = uki.process.mean_weights,\n) where {FT <: AbstractFloat, IT <: Int}\n\nconstructs mean x_mean from an ensemble x.\n\n\n\n\n\n","category":"function"},{"location":"API/Unscented/#EnsembleKalmanProcesses.construct_cov","page":"Unscented","title":"EnsembleKalmanProcesses.construct_cov","text":"construct_cov(\n    uki::EnsembleKalmanProcess{FT, IT, Unscented},\n    x::AbstractVecOrMat{FT},\n    x_mean::Union{FT, AbstractVector{FT}, Nothing} = nothing;\n    cov_weights = uki.process.cov_weights,\n) where {FT <: AbstractFloat, IT <: Int}\n\nConstructs covariance xx_cov from ensemble x and mean x_mean.\n\n\n\n\n\nconstruct_cov(\n    uki::EnsembleKalmanProcess{FT, IT, Unscented},\n    x::AbstractMatrix{FT},\n    x_mean::AbstractVector{FT},\n    obs_mean::AbstractMatrix{FT},\n    y_mean::AbstractVector{FT};\n    cov_weights = uki.process.cov_weights,\n) where {FT <: AbstractFloat, IT <: Int, P <: Process}\n\nConstructs covariance xy_cov from ensemble x and mean x_mean, ensemble obs_mean and mean y_mean.\n\n\n\n\n\n","category":"function"},{"location":"API/Unscented/#EnsembleKalmanProcesses.update_ensemble_prediction!","page":"Unscented","title":"EnsembleKalmanProcesses.update_ensemble_prediction!","text":"update_ensemble_prediction!(process::Unscented, Δt::FT) where {FT <: AbstractFloat}\n\nUKI prediction step : generate sigma points.\n\n\n\n\n\n","category":"function"},{"location":"API/Unscented/#EnsembleKalmanProcesses.update_ensemble_analysis!","page":"Unscented","title":"EnsembleKalmanProcesses.update_ensemble_analysis!","text":"update_ensemble_analysis!(\n    uki::EnsembleKalmanProcess{FT, IT, Unscented},\n    u_p::AbstractMatrix{FT},\n    g::AbstractMatrix{FT},\n) where {FT <: AbstractFloat, IT <: Int}\n\nUKI analysis step  : g is the predicted observations  Ny x N_ens matrix\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcess","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcess","text":"","category":"section"},{"location":"API/EnsembleKalmanProcess/","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcess","text":"CurrentModule = EnsembleKalmanProcesses","category":"page"},{"location":"API/EnsembleKalmanProcess/","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcess","text":"EnsembleKalmanProcess\nFailureHandler\nSampleSuccGauss\nIgnoreFailures\nget_u\nget_g\nget_ϕ\nget_u_final\nget_g_final\nget_ϕ_final\nget_u_mean\nget_u_cov\nget_g_mean\nget_ϕ_mean\nget_u_mean_final\nget_u_cov_final\nget_g_mean_final\nget_ϕ_mean_final\ncompute_error!\nget_error\nget_N_iterations\nconstruct_initial_ensemble\nupdate_ensemble!\nsample_empirical_gaussian\nsplit_indices_by_success","category":"page"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.EnsembleKalmanProcess","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.EnsembleKalmanProcess","text":"EnsembleKalmanProcess{FT <: AbstractFloat, IT <: Int, P <: Process}\n\nStructure that is used in Ensemble Kalman processes.\n\nFields\n\nu::Array{EnsembleKalmanProcesses.DataContainers.DataContainer{FT}} where FT<:AbstractFloat\narray of stores for parameters (u), each of size [N_par × N_ens]\nobs_mean::Vector{FT} where FT<:AbstractFloat\nvector of the observed vector size [N_obs]\nobs_noise_cov::Union{LinearAlgebra.UniformScaling{FT}, AbstractMatrix{FT}} where FT<:AbstractFloat\ncovariance matrix of the observational noise, of size [N_obs × N_obs]\nN_ens::Int64\nensemble size\ng::Array{EnsembleKalmanProcesses.DataContainers.DataContainer{FT}} where FT<:AbstractFloat\nArray of stores for forward model outputs, each of size  [N_obs × N_ens]\nerr::Vector{FT} where FT<:AbstractFloat\nvector of errors\nscheduler::EnsembleKalmanProcesses.LearningRateScheduler\nScheduler to calculate the timestep size in each EK iteration\naccelerator::EnsembleKalmanProcesses.Accelerator\naccelerator object that informs EK update steps, stores additional state variables as needed\nΔt::Vector{FT} where FT<:AbstractFloat\nstored vector of timesteps used in each EK iteration\nprocess::EnsembleKalmanProcesses.Process\nthe particular EK process (Inversion or Sampler or Unscented or TransformInversion or SparseInversion)\nrng::Random.AbstractRNG\nRandom number generator object (algorithm + seed) used for sampling and noise, for reproducibility. Defaults to Random.GLOBAL_RNG.\nfailure_handler::FailureHandler\nstruct storing failsafe update directives, implemented for (Inversion, SparseInversion, Unscented, TransformInversion)\nlocalizer::EnsembleKalmanProcesses.Localizers.Localizer\nLocalization kernel, implemented for (Inversion, SparseInversion, Unscented)\nverbose::Bool\nWhether to print diagnostics for each EK iteration\n\nGeneric constructor\n\nEnsembleKalmanProcess(\n    params::AbstractMatrix{FT},\n    obs_mean,\n    obs_noise_cov::Union{AbstractMatrix{FT}, UniformScaling{FT}},\n    process::P;\n    scheduler = DefaultScheduler(1),\n    Δt = FT(1),\n    rng::AbstractRNG = Random.GLOBAL_RNG,\n    failure_handler_method::FM = IgnoreFailures(),\n    localization_method::LM = NoLocalization(),\n    verbose::Bool = false,\n) where {FT <: AbstractFloat, P <: Process, FM <: FailureHandlingMethod, LM <: LocalizationMethod}\n\nInputs:\n\nparams                 :: Initial parameter ensemble\nobs_mean               :: Vector of observations\nobs_noise_cov          :: Noise covariance associated with the observations obs_mean\nprocess                :: Algorithm used to evolve the ensemble\nscheduler              :: Adaptive timestep calculator \nΔt                     :: Initial time step or learning rate\nrng                    :: Random number generator\nfailure_handler_method :: Method used to handle particle failures\nlocalization_method    :: Method used to localize sample covariances\nverbose                :: Whether to print diagnostic information\n\nOther constructors:\n\nEnsembleKalmanProcess(u, obs_mean, obs_noise_cov, N_ens, g, err, scheduler, accelerator, Δt, process, rng, failure_handler, localizer, verbose)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/EnsembleKalmanProcess.jl:117.\n\nEnsembleKalmanProcess(params, obs_mean, obs_noise_cov, process)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/EnsembleKalmanProcess.jl:147.\n\nEnsembleKalmanProcess(obs_mean, obs_noise_cov, process)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/UnscentedKalmanInversion.jl:223.\n\n\n\n\n\n","category":"type"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.FailureHandler","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.FailureHandler","text":"FailureHandler{P <: Process, FM <: FailureHandlingMethod}\n\nStructure defining the failure handler method used in the EnsembleKalmanProcess.\n\nFields\n\nfailsafe_update::Function\nFailsafe algorithmic update equation\n\nConstructors\n\nFailureHandler(process, method)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/EnsembleKalmanInversion.jl:10.\n\nFailureHandler(process, method)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/EnsembleKalmanInversion.jl:22.\n\nFailureHandler(process, method)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/EnsembleKalmanSampler.jl:31.\n\nFailureHandler(process, method)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/EnsembleTransformKalmanInversion.jl:17.\n\nFailureHandler(process, method)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/EnsembleTransformKalmanInversion.jl:29.\n\nFailureHandler(process, method)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/SparseEnsembleKalmanInversion.jl:40.\n\nFailureHandler(process, method)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/SparseEnsembleKalmanInversion.jl:52.\n\nFailureHandler(process, method)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/UnscentedKalmanInversion.jl:236.\n\nFailureHandler(process, method)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/UnscentedKalmanInversion.jl:255.\n\n\n\n\n\n","category":"type"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.SampleSuccGauss","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.SampleSuccGauss","text":"\"     SampleSuccGauss <: FailureHandlingMethod\n\nFailure handling method that substitutes failed ensemble members by new samples from the empirical Gaussian distribution defined by the updated successful ensemble.\n\n\n\n\n\n","category":"type"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.IgnoreFailures","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.IgnoreFailures","text":"Failure handling method that ignores forward model failures\n\n\n\n\n\n","category":"type"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_u","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_u","text":"get_u(ekp::EnsembleKalmanProcess, iteration::IT; return_array=true) where {IT <: Integer}\n\nReturns the unconstrained parameters at the given iteration. Returns a DataContainer object unless return_array is true.\n\n\n\n\n\nget_u(ekp::EnsembleKalmanProcess; return_array=true)\n\nReturns the unconstrained parameters from all iterations. The outer dimension is given by the number of iterations, and the inner objects are DataContainer objects unless return_array is true.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_g","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_g","text":"get_g(ekp::EnsembleKalmanProcess, iteration::IT; return_array=true) where {IT <: Integer}\n\nReturns the forward model evaluations at the given iteration. Returns a DataContainer object unless return_array is true.\n\n\n\n\n\nget_g(ekp::EnsembleKalmanProcess; return_array=true)\n\nReturns the forward model evaluations from all iterations. The outer dimension is given by the number of iterations, and the inner objects are DataContainer objects unless return_array is true.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_ϕ","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_ϕ","text":"get_ϕ(prior::ParameterDistribution, ekp::EnsembleKalmanProcess, iteration::IT)\n\nReturns the constrained parameters at the given iteration.\n\n\n\n\n\nget_ϕ(prior::ParameterDistribution, ekp::EnsembleKalmanProcess)\n\nReturns the constrained parameters from all iterations. The outer dimension is given by the number of iterations.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_u_final","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_u_final","text":"get_u_final(ekp::EnsembleKalmanProcess, return_array=true)\n\nGet the unconstrained parameters at the last iteration, returning a DataContainer Object if return_array is false.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_g_final","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_g_final","text":"get_g_final(ekp::EnsembleKalmanProcess, return_array=true)\n\nGet forward model outputs at the last iteration, returns a DataContainer Object if return_array is false.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_ϕ_final","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_ϕ_final","text":"get_ϕ_final(ekp::EnsembleKalmanProcess)\n\nGet the constrained parameters at the last iteration.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_u_mean","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_u_mean","text":"get_u_mean(ekp::EnsembleKalmanProcess, iteration::IT) where {IT <: Integer}\n\nReturns the mean unconstrained parameter at the given iteration.\n\n\n\n\n\nget_u_mean(uki::EnsembleKalmanProcess{FT, IT, Unscented}, iteration::IT)\n\nReturns the mean unconstrained parameter at the requested iteration.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_u_cov","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_u_cov","text":"get_u_cov(ekp::EnsembleKalmanProcess, iteration::IT) where {IT <: Integer}\n\nReturns the unconstrained parameter sample covariance at the given iteration.\n\n\n\n\n\nget_u_cov(uki::EnsembleKalmanProcess{FT, IT, Unscented}, iteration::IT)\n\nReturns the unconstrained parameter covariance at the requested iteration.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_g_mean","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_g_mean","text":"get_g_mean(ekp::EnsembleKalmanProcess, iteration::IT) where {IT <: Integer}\n\nReturns the mean forward map evaluation at the given iteration.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_ϕ_mean","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_ϕ_mean","text":"get_ϕ_mean(prior::ParameterDistribution, ekp::EnsembleKalmanProcess, iteration::IT)\n\nReturns the constrained transform of the mean unconstrained parameter at the given iteration.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_u_mean_final","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_u_mean_final","text":"get_u_mean_final(ekp::EnsembleKalmanProcess)\n\nGet the mean unconstrained parameter at the last iteration.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_u_cov_final","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_u_cov_final","text":"get_u_cov_final(ekp::EnsembleKalmanProcess)\n\nGet the mean unconstrained parameter covariance at the last iteration.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_g_mean_final","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_g_mean_final","text":"get_g_mean_final(ekp::EnsembleKalmanProcess)\n\nGet the mean forward model evaluation at the last iteration.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_ϕ_mean_final","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_ϕ_mean_final","text":"get_ϕ_mean_final(prior::ParameterDistribution, ekp::EnsembleKalmanProcess)\n\nGet the constrained transform of the mean unconstrained parameter at the last iteration.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.compute_error!","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.compute_error!","text":"compute_error!(ekp::EnsembleKalmanProcess)\n\nComputes the covariance-weighted error of the mean forward model output, (ḡ - y)'Γ_inv(ḡ - y). The error is stored within the EnsembleKalmanProcess.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_error","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_error","text":"get_error(ekp::EnsembleKalmanProcess)\n\nReturns the mean forward model output error as a function of algorithmic time.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.get_N_iterations","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.get_N_iterations","text":"get_N_iterations(ekp::EnsembleKalmanProcess)\n\nGet number of times update has been called (equals size(g), or size(u)-1).\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.construct_initial_ensemble","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.construct_initial_ensemble","text":"construct_initial_ensemble(\n    rng::AbstractRNG,\n    prior::ParameterDistribution,\n    N_ens::IT\n) where {IT <: Int}\nconstruct_initial_ensemble(prior::ParameterDistribution, N_ens::IT) where {IT <: Int}\n\nConstruct the initial parameters, by sampling N_ens samples from specified prior distribution. Returned with parameters as columns.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.update_ensemble!","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.update_ensemble!","text":"update_ensemble!(\n    ekp::EnsembleKalmanProcess,\n    g::AbstractMatrix{FT};\n    multiplicative_inflation::Bool = false,\n    additive_inflation::Bool = false,\n    use_prior_cov::Bool = false,\n    s::FT = 0.0,\n    ekp_kwargs...,\n) where {FT, IT}\n\nUpdates the ensemble according to an Inversion process. Inputs:\n\nekp :: The EnsembleKalmanProcess to update.\ng :: Model outputs, they need to be stored as a N_obs × N_ens array (i.e data are columms).\nmultiplicative_inflation :: Flag indicating whether to use multiplicative inflation.\nadditive_inflation :: Flag indicating whether to use additive inflation.\nusepriorcov :: Bool specifying whether to use prior covariance estimate for additive inflation.      If false (default), parameter covariance from the current iteration is used.\ns :: Scaling factor for time step in inflation step.\nekpkwargs :: Keyword arguments to pass to standard ekp updateensemble!.\n\n\n\n\n\nupdate_ensemble!(\n    ekp::EnsembleKalmanProcess{FT, IT, Inversion},\n    g::AbstractMatrix{FT},\n    process::Inversion;\n    deterministic_forward_map::Bool = true,\n    failed_ens = nothing,\n) where {FT, IT}\n\nUpdates the ensemble according to an Inversion process. \n\nInputs:\n\nekp :: The EnsembleKalmanProcess to update.\ng :: Model outputs, they need to be stored as a N_obs × N_ens array (i.e data are columms).\nprocess :: Type of the EKP.\ndeterministicforwardmap :: Whether output g comes from a deterministic model.\nfailed_ens :: Indices of failed particles. If nothing, failures are computed as columns of g with NaN entries.\n\n\n\n\n\nupdate_ensemble!(\n    ekp::EnsembleKalmanProcess{FT, IT, TransformInversion},\n    g::AbstractMatrix{FT},\n    process::TransformInversion;\n    failed_ens = nothing,\n) where {FT, IT}\n\nUpdates the ensemble according to a TransformInversion process. \n\nInputs:\n\nekp :: The EnsembleKalmanProcess to update.\ng :: Model outputs, they need to be stored as a N_obs × N_ens array (i.e data are columms).\nprocess :: Type of the EKP.\nfailed_ens :: Indices of failed particles. If nothing, failures are computed as columns of g with NaN entries.\n\n\n\n\n\nupdate_ensemble!(\n    ekp::EnsembleKalmanProcess{FT, IT, SparseInversion{FT}},\n    g::AbstractMatrix{FT},\n    process::SparseInversion{FT};\n    deterministic_forward_map = true,\n    failed_ens = nothing,\n) where {FT, IT}\n\nUpdates the ensemble according to a SparseInversion process. \n\nInputs:\n\nekp :: The EnsembleKalmanProcess to update.\ng :: Model outputs, they need to be stored as a N_obs × N_ens array (i.e data are columms).\nprocess :: Type of the EKP.\ndeterministic_forward_map :: Whether output g comes from a deterministic model.\nfailed_ens :: Indices of failed particles. If nothing, failures are computed as columns of g  with NaN entries.\n\n\n\n\n\nupdate_ensemble!(\n    ekp::EnsembleKalmanProcess{FT, IT, Sampler{FT}},\n    g::AbstractMatrix{FT},\n    process::Sampler{FT};\n    failed_ens = nothing,\n) where {FT, IT}\n\nUpdates the ensemble according to a Sampler process. \n\nInputs:\n\nekp :: The EnsembleKalmanProcess to update.\ng :: Model outputs, they need to be stored as a N_obs × N_ens array (i.e data are columms).\nprocess :: Type of the EKP.\nfailed_ens :: Indices of failed particles. If nothing, failures are computed as columns of g  with NaN entries.\n\n\n\n\n\nupdate_ensemble!(\n    uki::EnsembleKalmanProcess{FT, IT, Unscented},\n    g_in::AbstractMatrix{FT},\n    process::Unscented;\n    failed_ens = nothing,\n) where {FT <: AbstractFloat, IT <: Int}\n\nUpdates the ensemble according to an Unscented process. \n\nInputs:\n\nuki        :: The EnsembleKalmanProcess to update.\ng_in       :: Model outputs, they need to be stored as a N_obs × N_ens array (i.e data are columms).\nprocess :: Type of the EKP.\nfailed_ens :: Indices of failed particles. If nothing, failures are computed as columns of g  with NaN entries.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.sample_empirical_gaussian","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.sample_empirical_gaussian","text":"sample_empirical_gaussian(\n    u::AbstractMatrix{FT},\n    n::IT;\n    inflation::Union{FT, Nothing} = nothing,\n) where {FT <: Real, IT <: Int}\n\nReturns n samples from an empirical Gaussian based on point estimates u, adding inflation if the covariance is singular.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.split_indices_by_success","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.split_indices_by_success","text":" split_indices_by_success(g::AbstractMatrix{FT}) where {FT <: Real}\n\nReturns the successful/failed particle indices given a matrix with output vectors stored as columns. Failures are defined for particles containing at least one NaN output element.\n\n\n\n\n\n","category":"function"},{"location":"API/EnsembleKalmanProcess/#scheduler_api","page":"EnsembleKalmanProcess","title":"Learning Rate Schedulers","text":"","category":"section"},{"location":"API/EnsembleKalmanProcess/","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcess","text":"DefaultScheduler\nMutableScheduler\nEKSStableScheduler\nDataMisfitController\ncalculate_timestep!","category":"page"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.DefaultScheduler","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.DefaultScheduler","text":"struct DefaultScheduler{FT} <: EnsembleKalmanProcesses.LearningRateScheduler\n\nScheduler containing a default constant step size, users can override this temporarily within update_ensemble!.\n\nΔt_default::Any\nstep size\n\n\n\n\n\n","category":"type"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.MutableScheduler","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.MutableScheduler","text":"struct MutableScheduler{FT} <: EnsembleKalmanProcesses.LearningRateScheduler\n\nScheduler containing a mutable constant step size, users can override this permanently within update_ensemble!.\n\nΔt_mutable::Vector\nmutable step size\n\n\n\n\n\n","category":"type"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.EKSStableScheduler","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.EKSStableScheduler","text":"struct EKSStableScheduler{FT} <: EnsembleKalmanProcesses.LearningRateScheduler\n\nScheduler known to be stable for EKS, In particular, Delta t = fracalphaU + varepsilon where U = (G(u) - barG(u))^TGamma^-1(G(u) - y).  Cannot be overriden.\n\nnumerator::Any\nthe numerator alpha\nnugget::Any\nthe nugget term varepsilon\n\n\n\n\n\n","category":"type"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.DataMisfitController","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.DataMisfitController","text":"struct DataMisfitController{FT, M, S} <: EnsembleKalmanProcesses.LearningRateScheduler\n\nScheduler from Iglesias, Yang, 2021, Based on Bayesian Tempering. Terminates at T=1 by default, and at this time, ensemble spread provides a (more) meaningful approximation of posterior uncertainty In particular, for parameters theta_j at step n, to calculate the next timestep Delta t_n = minleft(maxleft(fracJ2Phi sqrtfracJ2langle Phi Phi rangleright) 1-sum^n-1_i t_iright) where Phi_j = Gamma^-frac12(G(theta_j) - y)^2.  Cannot be overriden by user provided timesteps. By default termination returns true from update_ensemble! and \n\nif on_terminate == \"stop\", stops further iteration.\nif on_terminate == \"continue_fixed\", continues iteration with the final timestep fixed\nif on_terminate == \"continue\", continues the algorithm (though no longer compares to 1-sum^n-1_i t_i) \n\nThe user may also change the T with terminate_at keyword.\n\niteration::Vector{Int64}\nthe current iteration\ninv_sqrt_noise::Vector\nthe inverse square-root of the noise covariance is stored\nterminate_at::Any\nthe algorithm time for termination, default: 1.0\non_terminate::Any\nthe action on termination, default: \"stop\",\n\n\n\n\n\n","category":"type"},{"location":"API/EnsembleKalmanProcess/#EnsembleKalmanProcesses.calculate_timestep!","page":"EnsembleKalmanProcess","title":"EnsembleKalmanProcesses.calculate_timestep!","text":"calculate_timestep!(ekp::EnsembleKalmanProcess, g::AbstractMatrix, Δt_new::Union{Nothing, AbstractFloat}) -> Any\n\n\nCalculates next timestep by pushing to ekp.Δt,  !isnothing(return_value) implies termination condition has been met\n\n\n\n\n\n","category":"function"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"EditURL = \"../../../examples/AerosolActivation/aerosol_activation.jl\"","category":"page"},{"location":"literated/aerosol_activation/#Aerosol-Activation-Example","page":"Aerosol activation","title":"Aerosol Activation Example","text":"","category":"section"},{"location":"literated/aerosol_activation/#Overview","page":"Aerosol activation","title":"Overview","text":"","category":"section"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"This example is based on AerosolActivation module which is a part of the   CloudMicrophysics.jl package. The AerosolActivation module computes the total number and mass   of aerosol particles that get activated and become cloud droplets,   given the atmospheric conditions and   the initial aerosol size distribution and properties. See the AerosolActivation module   docs   for derivation and description of all input parameters.","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"In this example we use the ensemble Kalman methods to learn   two parameters that describe the chemical composition of aerosol particles   based on the observed total number and mass of activated particles. The AerosolActivation model is used here in a \"perfect model\" setting,   meaning that the observations are generated by the same module   we are calibrating.","category":"page"},{"location":"literated/aerosol_activation/#Prerequisites","page":"Aerosol activation","title":"Prerequisites","text":"","category":"section"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"The example depends on some standard Julia libraries,   as well as the CliMA packages:   EnsembleKalmanProcess.jl,   CLIMAParameters.jl and   CloudMicrophysics.jl. To ensure that all the dependencies are met   start Julia using the Project.toml file provided in the example   and run the Julia package manager to download all the dependecies.","category":"page"},{"location":"literated/aerosol_activation/#Example","page":"Aerosol activation","title":"Example","text":"","category":"section"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"We begin by importing some standard Julia modules, the Ensemble Kalman Process modules, CLIMA Parameter modules and Aerosol Activation modules.","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"using Plots\nusing Distributions\nusing LinearAlgebra\nusing Random\n\nrng_seed = 44\nrng = Random.seed!(Random.GLOBAL_RNG, rng_seed)\n\nusing EnsembleKalmanProcesses\nusing EnsembleKalmanProcesses.ParameterDistributions\nconst EKP = EnsembleKalmanProcesses\n\nimport CLIMAParameters\nconst CP = CLIMAParameters\nstruct EarthParameterSet <: CP.AbstractEarthParameterSet end\nconst param_set = EarthParameterSet()\n\nimport CloudMicrophysics\nconst AM = CloudMicrophysics.AerosolModel\nconst AA = CloudMicrophysics.AerosolActivation\n\nimport Thermodynamics\nconst TD = Thermodynamics\nnothing # hide","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"Next, we provide the information about the priors of the parameters we want to learn. We are calibrating two parameters decribing the aerosol properties - namely the aerosol molar mass and the osmotic coefficient.","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"parameter_names = [\"molar_mass\", \"osmotic_coeff\"]\nnothing # hide","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"In this test we do know the parameter values. We use them to test the convergence of EKP for aerosol activation.","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"molar_mass_true = 0.058443\nosmotic_coeff_true = 0.9\ndefault_params = [molar_mass_true, osmotic_coeff_true]\nnothing # hide","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"We must define parameter priors. Both parameters have to be positive definite, therefore we define the constraints to be bounded below by zero.  We don't have much other prior knowledge about the parameters. We simply constrain their scale to be loosely of size 1. For more details see constrained_gaussian","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"prior1 = constrained_gaussian(parameter_names[1], 1, 1, 0, Inf)\nprior2 = constrained_gaussian(parameter_names[2], 1, 1, 0, Inf)\npriors = combine_distributions([prior1, prior2])\nnothing # hide","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"Next we define the atmospheric conditions for which the calibration will take place, (air temperature in K, air pressure in Pa vertical velocity in m/s and vapor specific humidity assuming its saturated in kg/kg) This can be changed later to include more than one (T p w) combination in the calibration process","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"T = 283.15\np = 1e5\nw = 5.0\np_vs = TD.saturation_vapor_pressure(param_set, T, TD.Liquid())\nq_vs = 1 / (1 - CP.Planet.molmass_ratio(param_set) * (p_vs - p) / p_vs)\nq = TD.PhasePartition(q_vs, 0.0, 0.0)\nnothing # hide","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"We also define the aerosol size distribution (lognormal, 1 mode) with (mean radius in m, geometric stdev, number concentration 1/m³). These can also be changed later to include different initial size distributions.","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"r_dry = 0.243e-6\nstdev = 1.4\nN = 100.0 * 1e6 # since 1/cm³ = 1e6 1/m³\nnothing # hide","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"Finally, we define additional parameters that describe the aerosol properties. The chosen aerosol is sea salt.","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"dissoc_seasalt = 2.0\nsoluble_mass_frac_seasalt = 1.0\nrho_seasalt = 2170.0;\nnothing # hide","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"We define a wrapper function that runs the aerosol activation module with two input parameters that will be calibrated by EKP. The output observations are the number and mass of activated aerosol.","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"function run_activation_model(molar_mass_calibrated, osmotic_coeff_calibrated)\n\n    accum_mode_seasalt = AM.Mode_B(\n        r_dry,\n        stdev,\n        N,\n        (1.0,),\n        (soluble_mass_frac_seasalt,),\n        (osmotic_coeff_calibrated,),\n        (molar_mass_calibrated,),\n        (dissoc_seasalt,),\n        (rho_seasalt,),\n        1,\n    )\n\n    aerosol_distr = AM.AerosolDistribution((accum_mode_seasalt,))\n    N_act = AA.total_N_activated(param_set, aerosol_distr, T, p, w, q)\n    M_act = AA.total_M_activated(param_set, aerosol_distr, T, p, w, q)\n    return [N_act, M_act]\nend\nnothing # hide","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"This example is run in a \"perfect model setting\", meaning the model that we calibrate is also used to generate observations. We use the total number and mass of activated aerosol particles as our observational data.","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"observation_data_names = [\"N_act\", \"M_act\"];\nnothing # hide","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"We generate artificial truth samples based on the default values of parameters we are calibrating.","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"n_samples = 10\nG_t = run_activation_model(molar_mass_true, osmotic_coeff_true)\ny_t = zeros(length(G_t), n_samples)\n\nΓy = convert(Array, LinearAlgebra.Diagonal([0.01 * G_t[1], 0.01 * G_t[2]]))\nμ = zeros(length(G_t));\nnothing # hide","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"And add noise to the generated truth sample.","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"for i in 1:n_samples\n    y_t[:, i] = G_t .+ rand(Distributions.MvNormal(μ, Γy))\nend\n\ntruth_array = EKP.Observations.Observation(y_t, Γy, observation_data_names)\nnothing # hide","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"One could try for the truth to be a mean of the generated array. Or do the calibration for all individual truth samples and then compute the mean of calibrated parameters. For now we are just taking one truth array member.","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"truth_sample = truth_array.samples[1];\nnothing # hide","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"We use 50 ensemble members and do 10 iterations.","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"N_ens = 50\nN_iter = 10\n\ninitial_par = EKP.construct_initial_ensemble(rng, priors, N_ens)\nekiobj = EKP.EnsembleKalmanProcess(initial_par, truth_sample, truth_array.obs_noise_cov, EKP.Inversion())\nnothing # hide","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"Finally, we can run the Ensemble Kalman Process calibration.","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"ϕ_n_values = []\nfor n in 1:N_iter\n    ϕ_n = EKP.get_ϕ_final(priors, ekiobj)\n    G_n = [run_activation_model(ϕ_n[:, i]...) for i in 1:N_ens]\n    G_ens = hcat(G_n...)\n    EKP.update_ensemble!(ekiobj, G_ens)\n\n    global ϕ_n_values = vcat(ϕ_n_values, [ϕ_n])\nend\nnothing # hide","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"We define some simple functions for plotting the data.","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"function plot_ensemble_scatter(id)\n\n    ensemble_member = 1:N_ens\n\n    if id == 1\n        ylabel = \"Molar mass [kg/mol]\"\n        filename = \"molar_mass_scatter.pdf\"\n    elseif id == 2\n        ylabel = \"Osmotic coefficient [-]\"\n        filename = \"osmotic_coeff_scatter.pdf\"\n    end\n\n    plot(\n        ensemble_member,\n        ϕ_n_values[1][id, 1:N_ens],\n        seriestype = :scatter,\n        xlabel = \"Ensemble Number\",\n        ylabel = ylabel,\n        legend = false,\n    )\n\n    for it in 2:N_iter\n        plot!(ensemble_member .+ ((it - 1) * 50), ϕ_n_values[it][id, 1:N_ens], seriestype = :scatter, legend = false)\n    end\n\n    current()\n    savefig(filename)\nend\n\nfunction plot_ensemble_means(id)\n\n    number_of_iters = 1:N_iter\n    means = zeros(N_iter)\n\n    for it in 1:N_iter\n        means[it] = mean(ϕ_n_values[it][id, 1:N_ens])\n    end\n\n    if id == 1\n        ylabel = \"Molar mass [kg/mol]\"\n        filename = \"molar_mass_average.pdf\"\n    end\n    if id == 2\n        ylabel = \"Osmotic coefficient [-]\"\n        filename = \"osmotic_coeff_average.pdf\"\n    end\n\n    plot(\n        number_of_iters,\n        means,\n        markershape = :star5,\n        xticks = number_of_iters,\n        xlabel = \"Iteration Number\",\n        ylabel = ylabel,\n        label = \"Ensemble Mean\",\n    )\n    hline!([default_params[id]], label = \"true value\")\n\n    savefig(filename)\nend\nnothing # hide","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"We plot the ensemble members and the ensemble mean for the molar mass and osmotic coefficient.","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"plot_ensemble_scatter(1)\nplot_ensemble_means(1)\nplot_ensemble_scatter(2)\nplot_ensemble_means(2)","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"(Image: ) (Image: ) (Image: ) (Image: )","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"Finally, we test that the parameter values obtained via EnsembleKalmanProcesses.jl are close to the known true parameter values.","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"molar_mass_ekp = round(mean(ϕ_n_values[N_iter][1, 1:N_ens]), digits = 6)\nosmotic_coeff_ekp = round(mean(ϕ_n_values[N_iter][2, 1:N_ens]), digits = 6)\n\nprintln(\"Molar mass [kg/mol]: \", molar_mass_ekp, \" vs \", molar_mass_true)\nprintln(\"Osmotic coefficient [-]: \", osmotic_coeff_ekp, \" vs \", osmotic_coeff_true)","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"","category":"page"},{"location":"literated/aerosol_activation/","page":"Aerosol activation","title":"Aerosol activation","text":"This page was generated using Literate.jl.","category":"page"},{"location":"contributing/#Contributing","page":"Contributing","title":"Contributing","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Thank you for considering contributing to EnsembleKalmanProcesses! We encourage opening issues and pull requests (PRs).","category":"page"},{"location":"contributing/#What-to-contribute?","page":"Contributing","title":"What to contribute?","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"The easiest way to contribute is by using EnsembleKalmanProcesses, identifying problems and opening issues;\nYou can try to tackle an existing issue. It is best to outline your proposed solution in the issue thread before implementing it in a PR;\nWrite an example or tutorial. It is likely that other users may find your use of EnsembleKalmanProcesses insightful;\nImprove documentation or comments if you found something hard to use;\nImplement a new feature if you need it. We strongly encourage opening an issue to make sure the administrators are on board before opening a PR with an unsolicited feature addition.","category":"page"},{"location":"contributing/#Using-git","page":"Contributing","title":"Using git","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"If you are unfamiliar with git and version control, the following guides will be helpful:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Atlassian (bitbucket) git tutorials. A set of tips and tricks for getting started with git.\nGitHub's git tutorials. A set of resources from GitHub to learn git.","category":"page"},{"location":"contributing/#Forks-and-branches","page":"Contributing","title":"Forks and branches","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Create your own fork of EnsembleKalmanProcesses on GitHub and check out your copy:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"$ git clone https://github.com/<your-username>/EnsembleKalmanProcesses.jl.git\n$ cd EnsembleKalmanProcesses.jl","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Now you have access to your fork of EnsembleKalmanProcesses through origin. Create a branch for your feature; this will hold your contribution:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"$ git checkout -b <branchname>","category":"page"},{"location":"contributing/#Some-useful-tips","page":"Contributing","title":"Some useful tips","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"When you start working on a new feature branch, make sure you start from main by running: git checkout main and git pull.\nCreate a new branch from main by using git checkout -b <branchname>.","category":"page"},{"location":"contributing/#Develop-your-feature","page":"Contributing","title":"Develop your feature","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Make sure you add tests for your code in test/ and appropriate documentation in the code and/or in docs/. Before committing your changes, you can verify their behavior by running the tests, the examples, and building the documentation locally. In addition, make sure your feature follows the formatting guidelines by running","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"julia --project=.dev .dev/climaformat.jl .","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"from the EnsembleKalmanProcesses.jl directory.","category":"page"},{"location":"contributing/#Squash-and-rebase","page":"Contributing","title":"Squash and rebase","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"When your PR is ready for review, clean up your commit history by squashing and make sure your code is current with EnsembleKalmanProcesses.jl main by rebasing. The general rule is that a PR should contain a single commit with a descriptive message.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"To make sure you are up to date with main, you can use the following workflow:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"$ git checkout main\n$ git pull\n$ git checkout <name_of_local_branch>\n$ git rebase main","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"This may create conflicts with the local branch. The conflicted files will be outlined by git. To resolve conflicts, we have to manually edit the files (e.g. with vim). The conflicts will appear between >>>>, ===== and <<<<<. We need to delete these lines and pick what version we want to keep.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"To squash your commits, you can use the following command:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"$ git rebase -i HEAD~n","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"where n is the number of commits you need to squash into one. Then, follow the instructions in the terminal. For example, to squash 4 commits:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"$ git rebase -i HEAD~4","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"will open the following file in (typically) vim:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"   pick 01d1124 <commit message 1>\n   pick 6340aaa <commit message 2>\n   pick ebfd367 <commit message 3>\n   pick 30e0ccb <commit message 4>\n\n   # Rebase 60709da..30e0ccb onto 60709da\n   #\n   # Commands:\n   #  p, pick = use commit\n   #  e, edit = use commit, but stop for amending\n   #  s, squash = use commit, but meld into previous commit\n   #\n   # If you remove a line here THAT COMMIT WILL BE LOST.\n   # However, if you remove everything, the rebase will be aborted.\n##","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"We want to keep the first commit and squash the last 3. We do so by changing the last three commits to squash and then do :wq on vim.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"   pick 01d1124 <commit message 1>\n   squash 6340aaa <commit message 2>\n   squash ebfd367 <commit message 3>\n   squash 30e0ccb <commit message 4>\n\n   # Rebase 60709da..30e0ccb onto 60709da\n   #\n   # Commands:\n   #  p, pick = use commit\n   #  e, edit = use commit, but stop for amending\n   #  s, squash = use commit, but meld into previous commit\n   #\n   # If you remove a line here THAT COMMIT WILL BE LOST.\n   # However, if you remove everything, the rebase will be aborted.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Then in the next screen that appears, we can just delete all messages that we do not want to show in the commit. After this is done and we are back to  the console, we have to force push. We need to force push because we rewrote the local commit history.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"$ git push -uf origin <name_of_local_branch>","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"You can find more information about squashing here.","category":"page"},{"location":"contributing/#Unit-testing","page":"Contributing","title":"Unit testing","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Currently a number of checks are run per commit for a given PR.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"JuliaFormatter checks if the PR is formatted with .dev/climaformat.jl.\nDocumentation rebuilds the documentation for the PR and checks if the docs are consistent and generate valid output.\nUnit Tests run subsets of the unit tests defined in tests/, using Pkg.test(). The tests are run in parallel to ensure that they finish in a reasonable time. The tests only run the latest commit for a PR, branch and will kill any stale jobs on push. These tests are only run on linux (Ubuntu LTS).","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Unit tests are run against every new commit for a given PR, the status of the unit-tests are not checked during the merge process but act as a sanity check for developers and reviewers. Depending on the content changed in the PR, some CI checks that are not necessary will be skipped.  For example doc only changes do not require the unit tests to be run.","category":"page"},{"location":"contributing/#The-merge-process","page":"Contributing","title":"The merge process","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"We use bors to manage merging PR's in the the EnsembleKalmanProcesses repo. If you're a collaborator and have the necessary permissions, you can type bors try in a comment on a PR to have integration test suite run on that PR, or bors r+ to try and merge the code.  Bors ensures that all integration tests for a given PR always pass before merging into main. The integration tests currently run example cases in examples/. Any breaking changes will need to also update the examples/, else bors will fail.","category":"page"},{"location":"observations/#Observations","page":"Observations","title":"Observations","text":"","category":"section"},{"location":"observations/","page":"Observations","title":"Observations","text":"The Observations object is used to store the truth for convenience of the user. The ingredients are","category":"page"},{"location":"observations/","page":"Observations","title":"Observations","text":"Samples of the data Vector{Vector{Float}} or Array{Float, 2}. If provided as a 2D array, the samples must be provided as columns. They are stored internally as Vector{Vector{Float}}\nAn optional covariance matrix can be provided.\nThe names of the data in this object as a String or Vector{String}","category":"page"},{"location":"observations/","page":"Observations","title":"Observations","text":"The empirical mean is calculated automatically. If a covariance matrix is not provided, then the empirical covariance is also calculated automatically.","category":"page"},{"location":"observations/#A-simple-example:","page":"Observations","title":"A simple example:","text":"","category":"section"},{"location":"observations/","page":"Observations","title":"Observations","text":"Here is a typical construction of the object:","category":"page"},{"location":"observations/","page":"Observations","title":"Observations","text":"μ = zeros(5)\nΓy = rand(5, 5)\nΓy = Γy' * Γy\nyt = rand(MvNormal(μ, Γy), 100) # generate 100 samples\nname = \"zero-mean mvnormal\"\n\ntrue_data = Observations.Observation(yt, Γy, name)","category":"page"},{"location":"observations/","page":"Observations","title":"Observations","text":"Currently, the data is retrieved by accessing the stored variables, e.g the fifth data sample is given by truth_data.samples[5], or the covariance matrix by truth_data.cov.","category":"page"},{"location":"ensemble_kalman_sampler/#Ensemble-Kalman-Sampling","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampling","text":"","category":"section"},{"location":"ensemble_kalman_sampler/#What-Is-It-and-What-Does-It-Do?","page":"Ensemble Kalman Sampler","title":"What Is It and What Does It Do?","text":"","category":"section"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"The Ensemble Kalman Sampler (EKS) (Garbuno-Inigo et al, 2020, Cleary et al, 2020, Garbuno-Inigo et al, 2020) is a derivative-free tool for approximate Bayesian inference. It does so by approximately sampling from the posterior distribution. That is, EKS provides both point estimation (through the mean of the final ensemble) and uncertainty quantification (through the covariance of the final ensemble), this is in contrast to EKI, which only provides point estimation.","category":"page"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"The EKS is an interacting particle system in stochastic differential equation form, and it is based on a dynamic which transforms an arbitrary initial probability distribution into an approximation of the desired posterior distribution over an infinite time horizon – see Garbuno-Inigo et al, 2020, for a comprehensive description of the method. While there are noisy variants of the standard EKI, EKS differs from them in its noise structure (as its noise is added in parameter space, not in  data space), and its update rule explicitly accounts for the prior (rather than having it enter through initialization). The EKS algorithm can be understood as well as an affine invariant system of interacting particles (Garbuno-Inigo et al, 2020) for which a finite-sample correction is introduced to overcome its computational finite-sample implementation. This finite-sample corrected version of EKS is also known as the affine invariant Langevin dynamics (ALDI).","category":"page"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"Perhaps as expected, the approximate posterior characterization through EKS needs more iterations, and thus more forward model evaluations, than EKI to converge to a suitable solution. This is because of the discrete-time implementation of the EKS diffusion process and the need to maintain a stable interacting particle system. However, the posterior approximation through EKS is obtained with far less computational effort than a typical Markov Chain Monte Carlo (MCMC) like Metropolis-Hastings.","category":"page"},{"location":"ensemble_kalman_sampler/#Problem-Formulation","page":"Ensemble Kalman Sampler","title":"Problem Formulation","text":"","category":"section"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"The data y and parameter vector theta are assumed to be related according to:","category":"page"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"    y = mathcalG(theta) + eta ","category":"page"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"where mathcalG  mathbbR^p rightarrow mathbbR^d denotes the forward map, y in mathbbR^d is the vector of observations, and eta is the observational noise, which is assumed to be drawn from a d-dimensional Gaussian with distribution mathcalN(0 Gamma_y). The objective of the inverse problem is to compute the unknown parameters theta given the observations y, the known forward map mathcalG, and noise characteristics eta of the process.","category":"page"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"note: Note\nTo obtain Bayesian characterization for the posterior from EKS, the user must specify a Gaussian prior distribution. See Prior distributions to see how one can apply flexible constraints while maintaining Gaussian priors. ","category":"page"},{"location":"ensemble_kalman_sampler/#Ensemble-Kalman-Sampling-Algorithm","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampling Algorithm","text":"","category":"section"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"The EKS is based on the following update equation for the parameter vector theta^(j)_n of ensemble member j at the n-iteration:","category":"page"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"beginaligned\ntheta_n+1^(* j) = theta_n^(j) - dfracDelta t_nJsum_k=1^Jlangle mathcalG(theta_n^(k)) - barmathcalG_n Gamma_y^-1(mathcalG(theta_n^(j)) - y) rangle theta_n^(k) + fracd+1J left(theta_n^(j) - bar theta_n right) - Delta t_n mathsfC(Theta_n) Gamma_theta^-1 theta_n + 1^(* j)  \ntheta_n + 1^j = theta_n+1^(* j) + sqrt2 Delta t_n mathsfC(Theta_n) xi_n^j \nendaligned","category":"page"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"where the subscript n=1 dots N_textit indicates the iteration, J is the ensemble size (i.e., the number of particles in the ensemble), Delta t_n is an internal adaptive time step (thus no need for the user to specify), Gamma_theta is the prior covariance, and xi_n^(j) sim mathcalN(0 mathrmI_p). barmathcalG_n is the ensemble mean of the forward map mathcalG(theta),","category":"page"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"barmathcalG_n = dfrac1Jsum_k=1^JmathcalG(theta_n^(k))","category":"page"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"The p times p matrix mathsfC(Theta_n), where Theta_n = lefttheta^(j)_nright_j=1^J is the set of all ensemble particles in the n-th iteration, denotes the empirical covariance between particles","category":"page"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"mathsfC(Theta_n) = frac1J sum_k=1^J (theta^(k)_n - bartheta_n) otimes (theta^(k)_n - bartheta_n)","category":"page"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"where bartheta_n is the ensemble mean of the particles,","category":"page"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"bartheta_n = dfrac1Jsum_k=1^Jtheta^(k)_n ","category":"page"},{"location":"ensemble_kalman_sampler/#Constructing-the-Forward-Map","page":"Ensemble Kalman Sampler","title":"Constructing the Forward Map","text":"","category":"section"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"At the core of the forward map mathcalG is the dynamical model PsimathbbR^p rightarrow mathbbR^o (running Psi is usually where the computational heavy-lifting is done), but the map mathcalG may include additional components such as a transformation of the (unbounded) parameters theta to a constrained domain the dynamical model can work with, or some post-processing of the output of Psi to generate the observations. For example, mathcalG may take the following form:","category":"page"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"mathcalG = mathcalH circ Psi circ mathcalT^-1","category":"page"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"where mathcalHmathbbR^o rightarrow mathbbR^d is the observation map and mathcalT is the transformation from the constrained to the unconstrained parameter space, such that mathcalT(phi)=theta. A family of standard transformations and their inverses are available in the ParameterDistributions module.","category":"page"},{"location":"ensemble_kalman_sampler/#How-to-Construct-an-Ensemble-Kalman-Sampler","page":"Ensemble Kalman Sampler","title":"How to Construct an Ensemble Kalman Sampler","text":"","category":"section"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"An EKS object can be created using the EnsembleKalmanProcess constructor by specifying the Sampler type. The constructor takes two arguments, the prior mean prior_mean and the prior covariance prior_cov.","category":"page"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"Creating an EKS object requires as arguments:","category":"page"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"An initial parameter ensemble – an array of size p × N_ens, where N_ens is the  ensemble size;\nThe mean value of the observed data – a vector of length d;\nThe covariance matrix of the observational noise – an array of size d × d;\nThe Sampler(prior_mean, prior_cov) process type, with the mean (a vector of length p) and the covariance (an array of size p x p) of the parameter's prior distribution.","category":"page"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"The following example shows how an EKS object is instantiated. An observation (y) and the covariance of the observational noise (obs_cov) are assumed to be defined previously in the code.","category":"page"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"using EnsembleKalmanProcesses\nusing EnsembleKalmanProcesses.ParameterDistributions  # required to create the prior\n\n# Construct prior (see `ParameterDistributions.jl` docs)\nprior = ParameterDistribution(...)\nprior_mean = mean(prior)\nprior_cov = cov(prior)\n\n# Construct initial ensemble\nN_ens = 50  # ensemble size\ninitial_ensemble = construct_initial_ensemble(prior, N_ens)\n\n# Construct ensemble Kalman process\neks_process = Sampler(prior_mean, prior_cov)\neksobj = EnsembleKalmanProcess(initial_ensemble, y, obs_noise_cov, eks_process)","category":"page"},{"location":"ensemble_kalman_sampler/#Updating-the-ensemble","page":"Ensemble Kalman Sampler","title":"Updating the ensemble","text":"","category":"section"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"Once the EKS object eksobj has been initialized, the initial ensemble of particles is iteratively updated by the update_ensemble! function, which takes as arguments the eksobj and the evaluations of the forward model at each member of the current ensemble. In the following example, the forward map G maps a parameter to the corresponding data – this is done for each parameter in the ensemble, such that the resulting g_ens is of size d x N_ens. The update_ensemble! function then stores the updated ensemble as well as the evaluations of the forward map in eksobj.","category":"page"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"A typical use of the update_ensemble! function given the EKS object eksobj, the dynamical model Ψ, and the observation map H (the latter two are assumed to be defined elsewhere, e.g. in a separate module)  may look as follows:","category":"page"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"# Given:\n# Ψ (some black box simulator)\n# H (some observation of the simulator output)\n# prior (prior distribution and parameter constraints)\n\nN_iter = 100 # Number of iterations\n\nfor n in 1:N_iter\n    ϕ_n = get_ϕ_final(prior, eksobj) # Get current ensemble in constrained \"ϕ\"-space\n    G_n = [H(Ψ(ϕ_n[:, i])) for i in 1:J]  # Evaluate forward map\n    g_ens = hcat(G_n...)  # Reformat into `d x N_ens` matrix\n    update_ensemble!(eksobj, g_ens) # Update ensemble\nend","category":"page"},{"location":"ensemble_kalman_sampler/#Solution","page":"Ensemble Kalman Sampler","title":"Solution","text":"","category":"section"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"The solution of the EKS algorithm is an approximate Gaussian distribution whose mean (θ_post) and covariance (Γ_post) can be extracted from the ''final ensemble'' (i.e., after the last iteration). The sample mean of the last ensemble is also the \"optimal\" parameter (θ_optim) for the given calibration problem. These statistics can be accessed as follows:","category":"page"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"# mean of the Gaussian distribution, the optimal parameter in computational θ-space\nθ_post = get_u_mean_final(eksobj)\n# (empirical) covariance of the Gaussian distribution in computational θ-space\nΓ_post = get_u_cov_final(eksobj)","category":"page"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"To obtain samples of this approximate posterior in the constrained space, we first sample the distribution, then transform using the constraints contained within the prior ","category":"page"},{"location":"ensemble_kalman_sampler/","page":"Ensemble Kalman Sampler","title":"Ensemble Kalman Sampler","text":"using Random, Distributions\n\nten_post_samples = rand(MvNormal(θ_post,Γ_post), 10)\nten_post_samples_phys = transform_unconstrained_to_constrained(prior, ten_post_samples) # the optimal physical parameter value","category":"page"},{"location":"examples/sinusoid_example_toml/#TOML-interface-for-fitting-parameters-of-a-sinusoid","page":"TOML interface","title":"TOML interface for fitting parameters of a sinusoid","text":"","category":"section"},{"location":"examples/sinusoid_example_toml/","page":"TOML interface","title":"TOML interface","text":"Here we revisit the simple sinusoid example, with the purpose of demonstrating how the EKP tools can interact with models in a non-intrusive fashion. In particular, this example will be useful for users whose model is written in another language, requires HPC management, or requires additional data processing stages.","category":"page"},{"location":"examples/sinusoid_example_toml/","page":"TOML interface","title":"TOML interface","text":"We can generally orchestrate the different stages of the calibration process using a script file. Here we employ a Linux bash script:","category":"page"},{"location":"examples/sinusoid_example_toml/","page":"TOML interface","title":"TOML interface","text":"> bash calibrate_script","category":"page"},{"location":"examples/sinusoid_example_toml/","page":"TOML interface","title":"TOML interface","text":"Which executes the following sequence of processes (in this case, calls to julia --project)","category":"page"},{"location":"examples/sinusoid_example_toml/","page":"TOML interface","title":"TOML interface","text":"# generate data\njulia --project generate_data.jl \n\n# create initial ensemble and build EKI\njulia --project initialize_EKP.jl \n\n# the EKI loop\nfor i in $(seq 1 $N_iterations); do\n\n    # run the model at each ensemble member\n    for j in $(seq 1 $N_ensemble); do\n        julia --project run_computer_model.jl $i $j\n    done\n\n    # update the ensemble with EKI\n    julia --project update_EKP.jl $i\n    \ndone","category":"page"},{"location":"examples/sinusoid_example_toml/","page":"TOML interface","title":"TOML interface","text":"The interaction between the calibration tools and the forward map are only through simple readable files stored in a nested file structure: for iteration i, ensemble member j","category":"page"},{"location":"examples/sinusoid_example_toml/","page":"TOML interface","title":"TOML interface","text":"Parameter values are stored (with their priors) in output/iteration_i/member_j/parameters.toml\nComputer model outputs are stored in output/iteration_i/member_j/model_output.jld2","category":"page"},{"location":"examples/sinusoid_example_toml/#Inputs-and-Outputs","page":"TOML interface","title":"Inputs and Outputs","text":"","category":"section"},{"location":"examples/sinusoid_example_toml/","page":"TOML interface","title":"TOML interface","text":"The prior distributions are provided in priors.toml in TOML format.","category":"page"},{"location":"examples/sinusoid_example_toml/","page":"TOML interface","title":"TOML interface","text":"[amplitude]\nprior = \"Parameterized(Normal(0.5815754049028404, 0.47238072707743883))\"\nconstraint = \"bounded_below(0.0)\"\ndescription = \"\"\"\nThe amplitude of the sine curve.\nThis yields a physical prior that is log-normal with approximate (mean,sd) = (2,1)\n\"\"\"\n\n[vert_shift]\nprior = \"Parameterized(Normal(0,5))\"\nconstraint = \"no_constraint()\"\ndescription = \"\"\"\nThe vertical shift of the sine curve.\nThis yields a physical prior that is Normal with (mean,sd) = (0,5)\n\"\"\"","category":"page"},{"location":"examples/sinusoid_example_toml/","page":"TOML interface","title":"TOML interface","text":"More information on the priors and constraints are given here. More examples defining priors in TOML format may be found in test/TOMLInterface/toml/.","category":"page"},{"location":"examples/sinusoid_example_toml/","page":"TOML interface","title":"TOML interface","text":"After running the example, (it takes several minutes to run), results are stored in output/eki.jld2. To view the results, one can interact with the stored objects by loading julia --project and proceeding as follows:","category":"page"},{"location":"examples/sinusoid_example_toml/","page":"TOML interface","title":"TOML interface","text":"julia> using EnsembleKalmanProcesses, JLD2\n\njulia> @load \"output/eki.jld2\"\n3-element Vector{Symbol}:\n :eki\n :param_dict\n :prior","category":"page"},{"location":"examples/sinusoid_example_toml/","page":"TOML interface","title":"TOML interface","text":"Then, for example, the final 6-member parameter ensemble is retrieved with:","category":"page"},{"location":"examples/sinusoid_example_toml/","page":"TOML interface","title":"TOML interface","text":"julia> get_ϕ_final(prior,eki)\n2×6 Matrix{Float64}:\n 1.38462  1.36124  1.32444  1.26686  1.33462  1.3636\n 6.51158  6.31867  6.68542  6.12809  6.44726  6.52448","category":"page"},{"location":"examples/sinusoid_example_toml/","page":"TOML interface","title":"TOML interface","text":"while the initial ensemble is retrieved with:","category":"page"},{"location":"examples/sinusoid_example_toml/","page":"TOML interface","title":"TOML interface","text":"julia> get_ϕ(prior, eki, 1)\n2×6 Matrix{Float64}:\n  1.05344   1.67949    6.29847  0.951586  2.07678    1.62284\n -7.00616  -0.931872  -6.11603  0.984338  0.274007  -1.39082","category":"page"},{"location":"examples/sinusoid_example_toml/","page":"TOML interface","title":"TOML interface","text":"note: Why is it so slow?\nThe example is slow because the forward map is written in Julia, and so 99.99% of computation for each call to julia --project is precompilation. Ensembles in Julia can be accelerated by using methods discussed here, or by compiling system images with PackageCompiler.jl, for example. This example is for instructional purposes only, and so is not optimized.","category":"page"},{"location":"inflation/#Inflation","page":"Inflation","title":"Inflation","text":"","category":"section"},{"location":"inflation/","page":"Inflation","title":"Inflation","text":"Inflation is an approach that slows down collapse in ensemble Kalman methods. Two distinct forms of inflation are implemented in this package. Both involve perturbing the ensemble members following the standard update rule of the chosen Kalman process. Multiplicative inflation expands ensemble members away from their mean in a deterministic manner, whereas additive inflation hinges on the addition of stochastic noise to ensemble members.","category":"page"},{"location":"inflation/","page":"Inflation","title":"Inflation","text":"For both implementations, a scaling factor s is included to extend functionality to cases with mini-batching.  The scaling factor s multiplies the artificial time step Delta t in the inflation equations to account for sampling error. For mini-batching, the scaling factor should be:","category":"page"},{"location":"inflation/","page":"Inflation","title":"Inflation","text":"    s = fracBC","category":"page"},{"location":"inflation/","page":"Inflation","title":"Inflation","text":"where B is the mini-batch size and C is the full dataset size.","category":"page"},{"location":"inflation/#Multiplicative-Inflation","page":"Inflation","title":"Multiplicative Inflation","text":"","category":"section"},{"location":"inflation/","page":"Inflation","title":"Inflation","text":"Multiplicative inflation effectively scales parameter vectors in parameter space, such that the perturbed ensemble remains in the linear span of the original ensemble. The implemented update equation follows Huang et al, 2022 eqn. 41:","category":"page"},{"location":"inflation/","page":"Inflation","title":"Inflation","text":"beginaligned\n    m_n+1 = m_n  qquad u^j_n + 1 = m_n+1 + sqrtfrac11 - s Deltat left(u^j_n - m_n right) qquad (1)\nendaligned","category":"page"},{"location":"inflation/","page":"Inflation","title":"Inflation","text":"where m is the ensemble average. In this way, the parameter covariance is inflated by a factor of frac11 - s Deltat, while the ensemble mean remains fixed.","category":"page"},{"location":"inflation/","page":"Inflation","title":"Inflation","text":"     C_n + 1 = frac11 - s Deltat C_n qquad (2)","category":"page"},{"location":"inflation/","page":"Inflation","title":"Inflation","text":"Multiplicative inflation can be used by flagging the update_ensemble! method as follows:","category":"page"},{"location":"inflation/","page":"Inflation","title":"Inflation","text":"    EKP.update_ensemble!(ekiobj, g_ens; multiplicative_inflation = true, s = 1.0)","category":"page"},{"location":"inflation/#Additive-Inflation","page":"Inflation","title":"Additive Inflation","text":"","category":"section"},{"location":"inflation/","page":"Inflation","title":"Inflation","text":"Additive inflation is implemented by systematically adding stochastic perturbations to the parameter ensemble in the form of Gaussian noise. Additive inflation breaks the linear subspace property, meaning the parameter ensemble can evolve outside of the span of the initial ensemble. In additive inflation, the ensemble is perturbed in the following manner after the standard Kalman update:","category":"page"},{"location":"inflation/","page":"Inflation","title":"Inflation","text":"     u_n+1 = u_n + zeta_n qquad (3) \n    zeta_n sim N(0 fracs Deltat 1 - s Deltat C_n) qquad (4)","category":"page"},{"location":"inflation/","page":"Inflation","title":"Inflation","text":"This inflates the parameter covariance by a factor of frac11 - s Deltat as in eqn. 2 , while the ensemble mean remains fixed.","category":"page"},{"location":"inflation/","page":"Inflation","title":"Inflation","text":"Additive inflation can be used by flagging the update_ensemble! method as follows:","category":"page"},{"location":"inflation/","page":"Inflation","title":"Inflation","text":"    EKP.update_ensemble!(ekiobj, g_ens; additive_inflation = true, s = 1.0)","category":"page"},{"location":"inflation/","page":"Inflation","title":"Inflation","text":"Alternatively, the prior covariance matrix may be used to generate additive noise, following:","category":"page"},{"location":"inflation/","page":"Inflation","title":"Inflation","text":"    zeta_n sim N(0 fracs Deltat 1 - s Deltat C_0) qquad (5)","category":"page"},{"location":"inflation/","page":"Inflation","title":"Inflation","text":"This results in an additive increase in the parameter covariance by fracs Deltat 1 - s Deltat * C_0 , while the mean remains fixed.","category":"page"},{"location":"inflation/","page":"Inflation","title":"Inflation","text":"     C_n + 1 = C_n + fracs Deltat 1 - s Deltat C_0 qquad (6)","category":"page"},{"location":"inflation/","page":"Inflation","title":"Inflation","text":"Additive inflation using the scaled prior covariance (parameter covariance of initial ensemble) can be used by flagging the update_ensemble! method as follows:","category":"page"},{"location":"inflation/","page":"Inflation","title":"Inflation","text":"    EKP.update_ensemble!(ekiobj, g_ens; additive_inflation = true, use_prior_cov = true, s = 1.0)","category":"page"},{"location":"API/Observations/#Observations","page":"Observations","title":"Observations","text":"","category":"section"},{"location":"API/Observations/","page":"Observations","title":"Observations","text":"CurrentModule = EnsembleKalmanProcesses.Observations","category":"page"},{"location":"API/Observations/","page":"Observations","title":"Observations","text":"Observation","category":"page"},{"location":"API/Observations/#EnsembleKalmanProcesses.Observations.Observation","page":"Observations","title":"EnsembleKalmanProcesses.Observations.Observation","text":"Observation{FT <: AbstractFloat}\n\nStructure that contains the observations\n\nFields\n\nsamples::Array{Vector{FT}, 1} where FT<:AbstractFloat\nvector of observational samples, each of length sample_dim\nobs_noise_cov::Union{Nothing, LinearAlgebra.UniformScaling{FT}, AbstractMatrix{FT}, FT} where FT<:AbstractFloat\ncovariance of the observational noise (assumed to be normally     distributed); sample_dim x sample_dim (where sample_dim is the number of     elements in each sample), or a scalar if the sample dim is 1. If not     supplied, obs_noise_cov is set to a diagonal matrix whose non-zero elements     are the variances of the samples, or to a scalar variance in the case of     1d samples. obs_noise_cov is set to nothing if only a single sample is     provided.\nmean::Union{AbstractVector{FT}, FT} where FT<:AbstractFloat\nsample mean\ndata_names::Union{String, AbstractVector{String}}\nnames of the data\n\n\n\n\n\n","category":"type"},{"location":"installation_instructions/#Installation","page":"Installation instructions","title":"Installation","text":"","category":"section"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"EnsembleKalmanProcesses.jl is a registered Julia package. You can install the latest version of EnsembleKalmanProcesses.jl through the built-in package manager. Press ] in the Julia REPL command prompt and","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"julia> ]\n(v1.7) pkg> add EnsembleKalmanProcesses\n(v1.7) pkg> instantiate","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"This will install the latest tagged release of the package.","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"info: But I wanna be on the bleeding edge...\nIf you want the most recent developer's version of the package thenjulia> ]\n(v1.7) pkg> add EnsembleKalmanProcesses#main\n(v1.7) pkg> instantiate","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"You can run the tests via the package manager by:","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"julia> ]\n(v1.7) pkg> test EnsembleKalmanProcesses","category":"page"},{"location":"installation_instructions/#Cloning-the-repository","page":"Installation instructions","title":"Cloning the repository","text":"","category":"section"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"If you are interested in getting your hands dirty and modifying the code then, you can also clone the repository and then instantiate, e.g.,","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"> cd EnsembleKalmanProcesses.jl\n> julia --project -e 'using Pkg; Pkg.instantiate()'","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"info: Do I need to clone the repository?\nMost times, cloning the repository in not necessary. If you only want to use the package's functionality, adding the packages as a dependency on your project is enough.","category":"page"},{"location":"installation_instructions/#Running-the-test-suite","page":"Installation instructions","title":"Running the test suite","text":"","category":"section"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"You can run the package's tests:","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"> julia --project -e 'using Pkg; Pkg.test()'","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"Alternatively, you can do this from within the repository:","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"> julia --project\njulia> ]\n(EnsembleKalmanProcesses) pkg> test","category":"page"},{"location":"installation_instructions/#Building-the-documentation-locally","page":"Installation instructions","title":"Building the documentation locally","text":"","category":"section"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"Once the project is built, you can build the project documentation under the docs/ sub-project:","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"> julia --project=docs/ -e 'using Pkg; Pkg.develop(PackageSpec(path=pwd())); Pkg.instantiate()'\n> julia --project=docs/ docs/make.jl","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"The locally rendered HTML documentation can be viewed at docs/build/index.html","category":"page"},{"location":"installation_instructions/#Running-repository-examples","page":"Installation instructions","title":"Running repository examples","text":"","category":"section"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"We have a selection of examples, found within the examples/ directory to demonstrate different use of our toolbox. Each example directory contains a Project.toml","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"To build with the latest EnsembleKalmanProcesses.jl release:","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"> cd examples/example-name/\n> julia --project -e 'using Pkg; Pkg.instantiate()'\n> julia --project example-file-name.jl","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"If you wish to run a local modified version of EnsembleKalmanProcesses.jl then try the following (starting from the EnsembleKalmanProcesses.jl package root)","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"> cd examples/example-name/\n> julia --project \n> julia> ]\n> (example-name)> rm EnsembleKalmanProcesses.jl\n> (example-name)> dev ../..\n> (example-name)> instantiate","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"followed by","category":"page"},{"location":"installation_instructions/","page":"Installation instructions","title":"Installation instructions","text":"> julia --project example-file-name.jl","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"EditURL = \"../../../examples/LossMinimization/loss_minimization.jl\"","category":"page"},{"location":"literated/loss_minimization/#Minimization-of-simple-loss-functions","page":"Minimization Loss","title":"Minimization of simple loss functions","text":"","category":"section"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"First we load the required packages.","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"using Distributions, LinearAlgebra, Random, Plots\n\nusing EnsembleKalmanProcesses\nusing EnsembleKalmanProcesses.ParameterDistributions\n\nconst EKP = EnsembleKalmanProcesses","category":"page"},{"location":"literated/loss_minimization/#Loss-function-with-single-minimum","page":"Minimization Loss","title":"Loss function with single minimum","text":"","category":"section"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"Here, we minimize the loss function","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"G₁(u) = u - u_* ","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"where u is a 2-vector of parameters and u_* is given; here u_* = (-1 1).","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"u★ = [1, -1]\nG₁(u) = [sqrt((u[1] - u★[1])^2 + (u[2] - u★[2])^2)]\nnothing # hide","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"We set the seed for pseudo-random number generator for reproducibility.","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"rng_seed = 41\nrng = Random.seed!(Random.GLOBAL_RNG, rng_seed)\nnothing # hide","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"We set a stabilization level, which can aid the algorithm convergence","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"dim_output = 1\nstabilization_level = 1e-3\nΓ_stabilization = stabilization_level * Matrix(I, dim_output, dim_output)","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"The functional is positive so to minimize it we may set the target to be 0,","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"G_target = [0]\nnothing # hide","category":"page"},{"location":"literated/loss_minimization/#Prior-distributions","page":"Minimization Loss","title":"Prior distributions","text":"","category":"section"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"As we work with a Bayesian method, we define a prior. This will behave like an \"initial guess\" for the likely region of parameter space we expect the solution to live in. Here we define Normal(01) distributions with no constraints","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"prior_u1 = constrained_gaussian(\"u1\", 0, 1, -Inf, Inf)\nprior_u2 = constrained_gaussian(\"u1\", 0, 1, -Inf, Inf)\nprior = combine_distributions([prior_u1, prior_u2])\nnothing # hide","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"note: Note\nIn this example there are no constraints, therefore no parameter transformations.","category":"page"},{"location":"literated/loss_minimization/#Calibration","page":"Minimization Loss","title":"Calibration","text":"","category":"section"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"We choose the number of ensemble members and the number of iterations of the algorithm","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"N_ensemble = 20\nN_iterations = 10\nnothing # hide","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"The initial ensemble is constructed by sampling the prior","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"initial_ensemble = EKP.construct_initial_ensemble(rng, prior, N_ensemble)","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"We then initialize the Ensemble Kalman Process algorithm, with the initial ensemble, the target, the stabilization and the process type (for EKI this is Inversion, initialized with Inversion()).","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"ensemble_kalman_process = EKP.EnsembleKalmanProcess(initial_ensemble, G_target, Γ_stabilization, Inversion())\nnothing # hide","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"Then we calibrate by (i) obtaining the parameters, (ii) calculate the loss function on the parameters (and concatenate), and last (iii) generate a new set of parameters using the model outputs:","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"for i in 1:N_iterations\n    params_i = get_u_final(ensemble_kalman_process)\n\n    g_ens = hcat([G₁(params_i[:, i]) for i in 1:N_ensemble]...)\n\n    EKP.update_ensemble!(ensemble_kalman_process, g_ens)\nend","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"and visualize the results:","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"u_init = get_u_prior(ensemble_kalman_process)\n\nanim_unique_minimum = @animate for i in 1:N_iterations\n    u_i = get_u(ensemble_kalman_process, i)\n\n    plot(\n        [u★[1]],\n        [u★[2]],\n        seriestype = :scatter,\n        markershape = :star5,\n        markersize = 11,\n        markercolor = :red,\n        label = \"optimum u⋆\",\n    )\n\n    plot!(\n        u_i[1, :],\n        u_i[2, :],\n        seriestype = :scatter,\n        xlims = extrema(u_init[1, :]),\n        ylims = extrema(u_init[2, :]),\n        xlabel = \"u₁\",\n        ylabel = \"u₂\",\n        markersize = 5,\n        markeralpha = 0.6,\n        markercolor = :blue,\n        label = \"particles\",\n        title = \"EKI iteration = \" * string(i),\n    )\nend\nnothing # hide","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"The results show that the minimizer of G_1 is u=u_*.","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"gif(anim_unique_minimum, \"unique_minimum.gif\", fps = 1) # hide","category":"page"},{"location":"literated/loss_minimization/#Loss-function-with-two-minima","page":"Minimization Loss","title":"Loss function with two minima","text":"","category":"section"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"Now let's do an example in which the loss function has two minima. We minimize the loss function","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"G₂(u) = u - v_* u - w_* ","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"where again u is a 2-vector, and v_* and w_* are given 2-vectors. Here, we take v_* = (1 -1) and w_* = (-1 -1).","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"v★ = [1, -1]\nw★ = [-1, -1]\nG₂(u) = [sqrt(((u[1] - v★[1])^2 + (u[2] - v★[2])^2) * ((u[1] - w★[1])^2 + (u[2] - w★[2])^2))]\nnothing # hide","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"The procedure is same as the single-minimum example above.","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"We set the seed for pseudo-random number generator for reproducibility,","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"rng_seed = 10\nrng = Random.seed!(Random.GLOBAL_RNG, rng_seed)\nnothing # hide","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"A positive function can be minimized with a target of 0,","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"G_target = [0]","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"We choose the stabilization as in the single-minimum example","category":"page"},{"location":"literated/loss_minimization/#Prior-distributions-2","page":"Minimization Loss","title":"Prior distributions","text":"","category":"section"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"We define the prior. We can place prior information on e.g., u₁, demonstrating a belief that u₁ is more likely to be negative. This can be implemented by setting a bias in the mean of its prior distribution to e.g., -05:","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"prior_u1 = constrained_gaussian(\"u1\", -0.5, sqrt(2), -Inf, Inf)\nprior_u2 = constrained_gaussian(\"u1\", 0, sqrt(2), -Inf, Inf)\nprior = combine_distributions([prior_u1, prior_u2])","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"note: Note\nIn this example there are no constraints, therefore no parameter transformations.","category":"page"},{"location":"literated/loss_minimization/#Calibration-2","page":"Minimization Loss","title":"Calibration","text":"","category":"section"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"We choose the number of ensemble members, the number of EKI iterations, construct our initial ensemble and the EKI with the Inversion() constructor (exactly as in the single-minimum example):","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"N_ensemble = 20\nN_iterations = 20\n\ninitial_ensemble = EKP.construct_initial_ensemble(rng, prior, N_ensemble)\n\nensemble_kalman_process = EKP.EnsembleKalmanProcess(initial_ensemble, G_target, Γ_stabilization, Inversion())","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"We calibrate by (i) obtaining the parameters, (ii) calculating the loss function on the parameters (and concatenate), and last (iii) generate a new set of parameters using the model outputs:","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"for i in 1:N_iterations\n    params_i = get_u_final(ensemble_kalman_process)\n\n    g_ens = hcat([G₂(params_i[:, i]) for i in 1:N_ensemble]...)\n\n    EKP.update_ensemble!(ensemble_kalman_process, g_ens)\nend","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"and visualize the results:","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"u_init = get_u_prior(ensemble_kalman_process)\n\nanim_two_minima = @animate for i in 1:N_iterations\n    u_i = get_u(ensemble_kalman_process, i)\n\n    plot(\n        [v★[1]],\n        [v★[2]],\n        seriestype = :scatter,\n        markershape = :star5,\n        markersize = 11,\n        markercolor = :red,\n        label = \"optimum v⋆\",\n    )\n\n    plot!(\n        [w★[1]],\n        [w★[2]],\n        seriestype = :scatter,\n        markershape = :star5,\n        markersize = 11,\n        markercolor = :green,\n        label = \"optimum w⋆\",\n    )\n\n    plot!(\n        u_i[1, :],\n        u_i[2, :],\n        seriestype = :scatter,\n        xlims = extrema(u_init[1, :]),\n        ylims = extrema(u_init[2, :]),\n        xlabel = \"u₁\",\n        ylabel = \"u₂\",\n        markersize = 5,\n        markeralpha = 0.6,\n        markercolor = :blue,\n        label = \"particles\",\n        title = \"EKI iteration = \" * string(i),\n    )\nend\nnothing # hide","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"Our bias in the prior shifts the initial ensemble into the negative u_1 direction, and thus increases the likelihood (over different instances of the random number generator) of finding the minimizer u=w_*.","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"gif(anim_two_minima, \"two_minima.gif\", fps = 1) # hide","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"","category":"page"},{"location":"literated/loss_minimization/","page":"Minimization Loss","title":"Minimization Loss","text":"This page was generated using Literate.jl.","category":"page"},{"location":"parameter_distributions/#parameter-distributions","page":"Prior distributions","title":"Defining prior distributions","text":"","category":"section"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"Bayesian inference begins with an explicit prior distribution. This page describes the interface EnsembleKalmanProcesses provides for specifying priors on parameters, via the ParameterDistributions module (src/ParameterDistributions.jl).","category":"page"},{"location":"parameter_distributions/#Summary","page":"Prior distributions","title":"Summary","text":"","category":"section"},{"location":"parameter_distributions/#ParameterDistribution-objects","page":"Prior distributions","title":"ParameterDistribution objects","text":"","category":"section"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"A prior is specified by a ParameterDistribution object, which has three components:","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"The distribution itself, given as a ParameterDistributionType object. This includes standard Julia Distributions, GaussianRandomFields as well as empirical/sample-based distributions, and thus can be univariate, multivariate or functional. To clarify, despite our use of the term \"Kalman processes,\" the prior distribution is not required to be Gaussian.\nA constraint (or array of constraints) on the domain of the distribution, given as a ConstraintType or Array{ConstraintType} object (the latter case builds a multivariate constraint as the Cartesian product of one-dimensional Constraints). This is used to enforce physical parameter values during inference: the model is never evaluated at parameter values outside the constrained region, and the posterior distribution will only be supported there.\nThe parameter name, given as a String.","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"In multiparameter settings, one should define one ParameterDistribution per parameter, and then concatenate these either in the constructor or with combine_distributions. This is illustrated below and in the Example combining several distributions.","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"note: What's up with the notation u, ϕ, and θ?\nParameters in unconstrained spaces are often denoted u or theta in the literature. In the code, method names featuring _u imply the return of a computational, unconstrained parameter.Parameters in physical/constrained spaces are often denoted mathcalT^-1(u), mathcalT^-1(theta), or phi in the literature (for some bijection mathcalT mapping to the unbounded space). In the code, method names featuring _ϕ imply the return of a physical, constrained parameter, and will always require a prior as input to perform the transformations internally.For more notations see our Glossary.","category":"page"},{"location":"parameter_distributions/#constrained-gaussian","page":"Prior distributions","title":"Recommended constructor","text":"","category":"section"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"constrained_gaussian() is a streamlined constructor for ParameterDistributions which addresses the most common use case; more general forms of the constructor are documented below, but we highly recommend that users begin here when it comes to specifying priors, only using the general constructor when necessary.","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"Usage:","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"lower_bound = 0.0\nupper_bound = 1.0\nμ_1 = 0.5\nμ_2 = 0.5\nσ_1 = 0.25\nσ_2 = 0.25","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"using EnsembleKalmanProcesses.ParameterDistributions # for `constrained_gaussian`, `combine_distributions`\nprior_1 = constrained_gaussian(\"param_1\", μ_1, σ_1, lower_bound, upper_bound)\nprior_2 = constrained_gaussian(\"param_2\", μ_2, σ_2, 0.0, Inf, repeats=3)\nprior = combine_distributions([prior_1, prior_2])","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"prior_1 is a ParameterDistribution describing a prior distribution for a parameter named \"param_1\" taking values on the interval [lower_bound, upper_bound]; the prior distribution has approximate mean μ_1 and standard deviation σ_1.","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"prior_2 is a ParameterDistribution describing a 3-dimensional prior distribution for a parameter named \"param_2\" with each dimensions taking independent values on the half-open interval [0.0, Inf); the marginals of this prior distribution have approximate mean μ_2 and standard deviation σ_2.","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"The use case constrained_gaussian() addresses is when prior information is qualitative, and exact distributions of the priors are unknown: i.e., the user is only able to specify the physical and likely ranges of prior parameter values at a rough, qualitative level. constrained_gaussian() does this by constructing a ParameterDistribution corresponding to a Gaussian \"squashed\" to fit in the given constraint interval, such that the \"squashed\" distribution has the specified mean and standard deviation (e.g. prior_2 above is a log-normal for each dimension).","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"The parameters of the Gaussian are chosen automatically (depending on the constraint) to reproduce the desired μ and σ — per the use case, other details of the form of the prior distribution shouldn't be important for downstream inference!                ","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"note: Slow/Failed construction?\nThe most common case of slow or failed construction is when requested parameters place too much mass at the hard boundary. A typical case is when the requested variance satisfies sigma approx mathrmdist(mumathrmboundary) Such priors can be defined, but not with our convenience constructor. If this is not the case but you still get failures please let us know!","category":"page"},{"location":"parameter_distributions/#Plotting","page":"Prior distributions","title":"Plotting","text":"","category":"section"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"For quick visualization we have a plot recipe for ParameterDistribution types. This will plot marginal histograms for all dimensions of the parameter distribution. For example, ","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"# with values:\n# e.g. lower_bound = 0.0, upper_bound = 1.0\n# μ_1 = 0.5, σ_1 = 0.25\n# μ_2 = 0.5, σ_2 = 0.25\n\nusing Plots\nplot(prior) ","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"One can also access the underlying Gaussian distributions in the unconstrained space with","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"using Plots\nplot(prior, constrained=false) ","category":"page"},{"location":"parameter_distributions/#Recommended-constructor-Simple-example","page":"Prior distributions","title":"Recommended constructor - Simple example","text":"","category":"section"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"Task: We wish to create a prior for a one-dimensional parameter. Our problem dictates that this parameter is bounded between 0 and 1; domain knowledge leads us to expect it should be around 0.7. The parameter is called point_seven.","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"We're told that the prior mean is 0.7; we choose a prior standard deviation of 0.15 to be sufficiently wide without putting too much mass at the upper bound. The constructor is then","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"using EnsembleKalmanProcesses.ParameterDistributions # for `constrained_gaussian`\nprior = constrained_gaussian(\"point_seven\", 0.7, 0.15, 0.0, 1.0)\nnothing # hide","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"# instead of importing ParameterDistributions & dependencies to call constructor,\n# which would make docs build longer and more fragile, simply hard-code Normal()\n# parameters found by constrained_gaussian constructor\n\nusing Distributions\nusing Plots\nPlots.default(lw=2)\n\nN = 50\nx_eval = collect(-5:6/400:5)\n\n#bounded in [0.0, 1.0]\ntransform_unconstrained_to_constrained(x) = 1.0 - 1.0 / (exp(x) + 1)\ndist= pdf.(Normal(0.957711, 0.78507), x_eval)\nconstrained_x_eval = transform_unconstrained_to_constrained.(x_eval)\n\np2 = plot(constrained_x_eval, dist) \nvline!([0.7]) \ntitle!(\"Prior pdf\")","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"The pdf of the constructed prior distribution (in the physical, constrained space) looks like:","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"p = plot(p2, legend=false, size = (450, 450)) #hide","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"In Simple example revisited below, we repeat this example \"manually\" with the general constructor.","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"note: What if I want to impose the same prior on many parameters?\nThe recommended constructor can be called as constrained_gaussian(...; repeats = n) to return a combined prior formed by n identical priors.","category":"page"},{"location":"parameter_distributions/#ParameterDistribution-struct","page":"Prior distributions","title":"ParameterDistribution struct","text":"","category":"section"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"This section provides more details on the components of a ParameterDistribution object.","category":"page"},{"location":"parameter_distributions/#ParameterDistributionType","page":"Prior distributions","title":"ParameterDistributionType","text":"","category":"section"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"The ParameterDistributionType struct wraps four types for specifying different types of prior distributions:","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"The Parameterized type is initialized using a Julia Distributions.jl object. Samples are drawn randomly from the distribution object.\nThe VectorOfParameterized type is initialized with a vector of distributions.\nThe Samples type is initialized using a two dimensional array. Samples are drawn randomly (with replacement) from the columns of the provided array.\nThe FunctionParameterDistributionType struct defines parameters specified as fields over a domain. More detail can be found here.","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"warning: Warning\nWe recommend that the distributions be unbounded (see next section), as the filtering algorithms in EnsembleKalmanProcesses are not guaranteed to preserve constraints unless defined through the ConstraintType mechanism.","category":"page"},{"location":"parameter_distributions/#ConstraintType","page":"Prior distributions","title":"ConstraintType","text":"","category":"section"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"The inference algorithms implemented in EnsembleKalmanProcesses assume unbounded parameter domains. To be able to handle constrained parameter values consistently, the ConstraintType  defines a bijection between the physical, constrained parameter domain and an unphysical, unconstrained domain in which the filtering takes place. This bijection is specified by the functions transform_constrained_to_unconstrained and transform_unconstrained_to_constrained, which are built from either predefined constructors or user-defined constraint functions given as arguments to the ConstraintType constructor. ","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"We provide the following predefined constructors which implement mappings that handle the most common constraints:","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"no_constraint(): The parameter is unconstrained and takes values in (-∞, ∞) (mapping is the identity).\nbounded_below(lower_bound): The parameter takes values in [lower_bound, ∞).\nbounded_above(upper_bound): The parameter takes values in (-∞, upper_bound].\nbounded(lower_bound,upper_bound): The parameter takes values on the interval [lower_bound, upper_bound].","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"These are demonstrated in ConstraintType Examples.","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"Currently we only support multivariate constraints which are the Cartesian product of the one-dimensional ConstraintTypes. Every component of a multidimensional parameter must have an associated constraint, so, e.g. for a multivariate ParameterDistributionType of dimension p the user must provide a p-dimensional Array{ConstraintType}. A VectorOfParameterized distribution built with distributions of dimension p and q has dimension p+q.","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"note: Note\nWhen a nontrivial ConstraintType is given, the general constructor assumes the ParameterDistributionType is specified in the unconstrained space; the actual prior pdf is then the composition of the ParameterDistributionType's pdf with the transform_unconstrained_to_constrained transformation. We provide constrained_gaussian to define priors directly in the physical, constrained space.","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"warning: Warning\nIt is up to the user to ensure any custom mappings transform_constrained_to_unconstrained and transform_unconstrained_to_constrained are inverses of each other.","category":"page"},{"location":"parameter_distributions/#The-name","page":"Prior distributions","title":"The name","text":"","category":"section"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"This is simply a String used to identify different parameters in multi-parameter situations, as in the methods below.","category":"page"},{"location":"parameter_distributions/#function-parameter-type","page":"Prior distributions","title":"FunctionParameterDistributionType","text":"","category":"section"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"Learning a function distribution is useful when one wishes to obtain a parametric representation of a function that is (relatively) agnostic of the underlying grid discretization. Most practical implementations involve posing a restrictive class of functions by truncation of a spectral decomposition. The function is then represented as a set of coefficients of these modes (known as degrees of freedom), rather than directly through the values at evaluation points.","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"As a subtype of ParameterDistributionType, we currently support one option for specifying prior distributions over functions:","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"The GaussianRandomFieldInterface type is initialized with a Gaussian Random Field object and the GRF package. Currently we support objects from GaussianRandomFields.jl with package GRFJL(). Gaussian random fields allow the definition of scalar function distributions defined over a uniform mesh on interval, rectangular, and hyper-rectangular domains.","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"As with other ParameterDistributions, a function distribution, is built from a name, a FunctionPameterDistributionType struct and a constraint, here only one, placed on the scalar output space of the function using a Constraint().","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"note: constraints\nThe transformation transform_unconstrained_to_constrained, will map from (unconstrained) degrees of freedom, to (constrained) evaluations of the function on a numerical grid. In particular, the transform_constrained_to_unconstrained is no longer the inverse of this map, it now simply maps from constrained evaluations to unconstrained evaluations on the grid.","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"We provide an example construction here.","category":"page"},{"location":"parameter_distributions/#ParameterDistribution-constructor","page":"Prior distributions","title":"ParameterDistribution constructor","text":"","category":"section"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"The Recommended constructor, constrained_gaussian(), is described above. For more general cases in which the prior needs to be specified in more detail, a ParameterDistribution may be constructed \"manually\" from its component objects:","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"using EnsembleKalmanProcesses.ParameterDistributions # for `ParameterDistribution`, `combine_distributions`\nusing Distributions\ndistribution_1 = Parameterized(Normal(0,1))\ndistribution_2 = Parameterized(Normal(0,1))\nconstraint_1 = no_constraint()\nconstraint_2 = no_constraint()\nname_1 = \"m\"\nname_2 = \"mm\"","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"using EnsembleKalmanProcesses.ParameterDistributions # for `ParameterDistribution`, `combine_distributions`\nprior_1 = ParameterDistribution(distribution_1, constraint_1, name_1)\nprior_2 = ParameterDistribution(distribution_2, constraint_2, name_2)\nprior = combine_distributions( [prior_1, prior_2])\nnothing # hide","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"Arguments may also be provided as a Dict:","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"using EnsembleKalmanProcesses.ParameterDistributions # for `ParameterDistribution`\ndict_1 = Dict(\"distribution\" => distribution_1, \"constraint\" => constraint_1, \"name\" => name_1)\ndict_2 = Dict(\"distribution\" => distribution_2, \"constraint\" => constraint_2, \"name\" => name_2)\nprior = ParameterDistribution( [dict_1, dict_2] )\nnothing # hide","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"We provide Additional Examples below; see also examples in the package examples/ and unit tests found in test/ParameterDistributions/runtests.jl.","category":"page"},{"location":"parameter_distributions/#ParameterDistribution-methods","page":"Prior distributions","title":"ParameterDistribution methods","text":"","category":"section"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"These functions typically return a Dict with ParameterDistribution.name as a keys, or an Array if requested:","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"get_name: returns the name(s) of parameters in the ParameterDistribution.\nget_distribution: returns the distributions (ParameterDistributionType objects) in the ParameterDistribution. Note that this is not the prior pdf used for inference if nontrivial constraints have been applied.\nmean, var, cov, sample, logpdf: mean, variance, covariance, logpdf or samples the Julia Distribution if Parameterized, or draws from the list of samples if Samples. Extends the StatsBase definitions. Note that these do not correspond to the prior pdf used for inference if nontrivial constraints have been applied.\ntransform_unconstrained_to_constrained: Applies the constraint mappings.\ntransform_constrained_to_unconstrained: Applies the inverse constraint mappings.","category":"page"},{"location":"parameter_distributions/#Additional-Examples","page":"Prior distributions","title":"Additional Examples","text":"","category":"section"},{"location":"parameter_distributions/#Simple-example-revisited","page":"Prior distributions","title":"Simple example revisited","text":"","category":"section"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"To illustrate what the constrained_gaussian constructor is doing, in this section we repeat the Recommended constructor - Simple example given above, using the \"manual,\" general-purpose constructor. Let's bring in the packages we will require","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"using EnsembleKalmanProcesses.ParameterDistributions # for `bounded`, `Parameterized`, and `ParameterDistribution` \nusing Distributions # for `Normal`\nnothing # hide","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"Then we initialize the constraint first,","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"constraint = bounded(0, 1)\nnothing # hide","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"This defines the following transformation to the  constrained space (and also its inverse)","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"transform_unconstrained_to_constrained(x) = exp(x) / (exp(x) + 1)\nnothing # hide","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"The prior mean should be around 0.7 (in the constrained space), and one can find that the push-forward of a particular normal distribution, namely, transform_unconstrained_to_constrained(Normal(mean = 1, sd = 0.5)) gives a prior pdf with 95% of its mass between [0.5, 0.88]. ","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"This is the main difference from the use of the constrained_gaussian constructor: in that example, the constructor numerically solved for the parameters of the Normal() which would reproduce the requested μ, σ for the physical, constrained quantity (since no closed-form transformation for the moments exists.)","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"distribution = Parameterized(Normal(1, 0.5))\nnothing # hide","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"Finally we attach the name","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"name = \"point_seven\"\nnothing # hide","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"and the distribution is created by either:","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"prior = ParameterDistribution(distribution, constraint, name)\nnothing # hide","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"or","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"prior_dict = Dict(\"distribution\" => distribution, \"constraint\" => constraint, \"name\" => name)\nprior = ParameterDistribution(prior_dict)\nnothing # hide","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"using Distributions\nusing Plots\nPlots.default(lw=2)\n\nN = 50\nx_eval = collect(-3:6/400:3)\n\n#bounded in [0.0, 1.0]\ntransform_unconstrained_to_constrained(x) = exp(x) / (exp(x) + 1)\ndist= pdf.(Normal(1, 0.5), x_eval)\nconstrained_x_eval = transform_unconstrained_to_constrained.(x_eval)\n\np1 = plot(x_eval, dist,) \nvline!([1.0]) \ntitle!(\"Normal(1, 0.5)\")\n\np2 = plot(constrained_x_eval, dist) \nvline!([transform_unconstrained_to_constrained(1.0)]) \ntitle!(\"Constrained Normal(1, 0.5)\")","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"The pdf of the Normal distribution and its transform to the physical, constrained space are:","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"p = plot(p1, p2, legend=false, size = (900, 450)) #hide","category":"page"},{"location":"parameter_distributions/#samples-example","page":"Prior distributions","title":"Sample-based distribution","text":"","category":"section"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"We repeat the work of Simple example revisited, but now assuming that to create our prior, we only have samples given by the histogram:","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"# instead of importing ParameterDistributions & dependencies to call constructor,\n# which would make docs build longer and more fragile, simply hard-code Normal()\n# parameters found by constrained_gaussian constructor\n\nusing Distributions\nusing Plots\nPlots.default(lw=2)\n\nN = 5000\n\n#bounded in [0.0, 1.0]\ntransform_unconstrained_to_constrained(x) = 1.0 - 1.0 / (exp(x) + 1)\nsamples = rand(Normal(0.957711, 0.78507), N)\nconstrained_samples = transform_unconstrained_to_constrained.(samples)\n\np3 = histogram(constrained_samples, bins=50) \nvline!([0.7]) \ntitle!(\"Prior of samples\")","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"p = plot(p3, legend=false, size = (450, 450)) #hide","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"Imagine we do not know this distribution is bounded. To create a ParameterDistribution one can take a matrix constrained_samples whose columns are this data:","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"using EnsembleKalmanProcesses.ParameterDistributions # for `Samples`, `no_constraint`, `ParameterDistribution`, `bounded`\nconstrained_samples = [0.1 0.2 0.3 0.4] # hide\ndistribution = Samples(constrained_samples)\nconstraint = no_constraint()\nname = \"point_seven\"\nprior = ParameterDistribution(distribution, constraint, name)\nnothing # hide","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"note: Note\nThis naive implementation will not enforce any boundaries during the algorithm implementation.","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"Imagine that we know about the boundedness of this distribution, then, as in Simple example revisited, we define the constraint","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"constraint = bounded(0, 1)\nnothing # hide","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"which stores the transformation:","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"unconstrained_samples = constraint.constrained_to_unconstrained.(constrained_samples)\nnothing # hide","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"This maps the samples into an unbounded space, giving the following histogram:","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"# instead of importing ParameterDistributions & dependencies to call constructor,\n# which would make docs build longer and more fragile, simply hard-code Normal()\n# parameters found by constrained_gaussian constructor\n\nusing Distributions\nusing Plots\nPlots.default(lw=2)\n\nN = 5000\n\n# bounded in [0.0, 1.0]\n# transform_unconstrained_to_constrained(x) = 1.0 - 1.0 / (exp(x) + 1.0)\n transform_constrained_to_unconstrained(x) = log(1.0 / (1.0 - x) - 1.0)\nunconstrained_samples = rand(Normal(0.957711, 0.78507), N)\n\np3 = histogram(unconstrained_samples, bins=50) \nvline!([transform_constrained_to_unconstrained(0.7)]) \ntitle!(\"Prior of samples\")","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"p = plot(p3, legend=false, size = (450, 450)) #hide","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"As before we define a Samples distribution from matrix whose columns are the (now unconstrained) samples, along with a name to create the ParameterDistribution.","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"distribution = Samples(unconstrained_samples)\nname = \"point_seven\"\nprior = ParameterDistribution(distribution, constraint, name)\nnothing # hide","category":"page"},{"location":"parameter_distributions/#Example-combining-several-distributions","page":"Prior distributions","title":"Example combining several distributions","text":"","category":"section"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"To show how to combine priors in a more complex setting (e.g. for an entire parametrized process), we create a 25-dimensional parameter distribution from three dictionaries.","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"Bring in the packages!","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"using EnsembleKalmanProcesses.ParameterDistributions\n# for `bounded_below`, `bounded`, `Constraint`, `no_constraint`,\n#     `Parameterized`, `Samples`,`VectorOfParameterized`,\n#     `ParameterDistribution`, `combine_distributions`\nusing LinearAlgebra  # for `SymTridiagonal`, `Matrix`\nusing Distributions # for `MvNormal`, `Beta`\nnothing # hide","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"The first parameter is a 3-dimensional distribution, with the following bound constraints on parameters in physical space:","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"c1 = repeat([bounded_below(0)], 3)\nnothing # hide","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"We know that a multivariate normal represents its distribution in the transformed (unbounded) space. Here we take a tridiagonal covariance matrix.","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"diagonal_val = 0.5 * ones(3)\nudiag_val = 0.25 * ones(2)\nmean = ones(3)\ncovariance = Matrix(SymTridiagonal(diagonal_val, udiag_val))\nd1 = Parameterized(MvNormal(mean, covariance)) # 3D multivariate normal\nnothing # hide","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"We also provide a name","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"name1 = \"constrained_mvnormal\"\nnothing # hide","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"The second parameter is a 2-dimensional one. It is only given by 4 samples in the transformed space - (where one will typically generate samples). It is bounded in the first dimension by the constraint shown, there is a user provided transform for the second dimension - using the default constructor.","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"d2 = Samples([1.0 5.0 9.0 13.0; 3.0 7.0 11.0 15.0]) # 4 samples of 2D parameter space\n\ntransform = (x -> 3 * x + 14)\njac_transform = (x -> 3)\ninverse_transform = (x -> (x - 14) / 3)\nabstract type Affine <: ConstraintType end\n\nc2 = [bounded(10, 15),\n      Constraint{Affine}(transform, jac_transform, inverse_transform, nothing)]\nname2 = \"constrained_sampled\"\nnothing # hide","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"The final parameter is 4-dimensional, defined as a list of i.i.d univariate distributions we make use of the VectorOfParameterized type","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"d3 = VectorOfParameterized(repeat([Beta(2,2)],4))\nc3 = repeat([no_constraint()],4)\nname3 = \"Beta\"\nnothing # hide","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"The full prior distribution for this setting is created either through building simple distributions and combining","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"u1 = ParameterDistribution(d1, c1, name1)\nu2 = ParameterDistribution(d2, c2, name2)\nu3 = ParameterDistribution(d3, c3, name3)\nu = combine_distributions( [u1, u2, u3])\nnothing # hide","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"or an array of the parameter specifications as dictionaries.","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"param_dict1 = Dict(\"distribution\" => d1, \"constraint\" => c1, \"name\" => name1)\nparam_dict2 = Dict(\"distribution\" => d2, \"constraint\" => c2, \"name\" => name2)\nparam_dict3 = Dict(\"distribution\" => d3, \"constraint\" => c3, \"name\" => name3)\nu = ParameterDistribution([param_dict1, param_dict2, param_dict3])\nnothing # hide","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"We can visualize the marginals of the constrained distributions,","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"using Plots\nplot(u)","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"and the unconstrained distributions similarly,","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"using Plots\nplot(u, constrained = false)","category":"page"},{"location":"parameter_distributions/#function-example","page":"Prior distributions","title":"Function Distribution Example","text":"","category":"section"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"Here, define a function parameter distribution on 01 times 12 , bounded by -5-3 and with correlation lengthscales 0.05. First, we get the packages:","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"using EnsembleKalmanProcesses.ParameterDistributions # For `ParameterDistribution`\nusing Random, Distributions # for `rand` and `Normal`\nusing Plots\n# We must `import` the GRF package, rather than call a `using` statement here\nimport GaussianRandomFields # for `GaussianRandomFields`","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"then, we use the  GaussianRandomFields.jl package to define the distribution of choice. This distribution is unbounded. Here we take a Matern kernel, and define our evaluation grid on the domain. We choose 30 degrees of freedom (dofs), so this function distribution is specified through the value of 30 learnable coefficients. ","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"const GRF = GaussianRandomFields\n# Define a `GaussianRandomFields` object\ninput_dim = 2 # Define a 2D -> 1D function\ndofs = 30 # the number of modes defining the distribution\npoints = [collect(0:0.01:1), collect(1:0.02:2)] # the 2D domain grid (uniform in each dimension)\n\ngrfjl_obj = GRF.GaussianRandomField(\n   GRF.CovarianceFunction(input_dim, GRF.Matern(0.05, 2)),\n   GRF.KarhunenLoeve(dofs),\n   points...,\n) # the Gaussian Random Field object from the package\nnothing # hide","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"We define our parameter distribution wrapper, where GRFJL() indicates the GRF package used. We also impose bounds into an interval -5-3 (here applied to the output space).","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"grf = GaussianRandomFieldInterface(grfjl_obj, GRFJL()) # our wrapper\npd = ParameterDistribution(\n    Dict(\n        \"distribution\" => grf,\n        \"constraint\" => bounded(-5, -3), \n        \"name\" => \"func_in_min5_min3\",\n    )\n) # The ParameterDistribution with constraint in the output space\nnothing # hide","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"We plot 4 samples of this distribution. Samples are taken over the (30-dimensional) degrees of freedom, and then we apply the transform_unconstrained_to_costrained map to (i) build the function distribution, (ii) evaluate it on the numerical grid, and (iii) constrain the output with our prescribed bounds.","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"shape = [length(pp) for pp in points]\nsamples_constrained_flat = [transform_unconstrained_to_constrained(pd, rand(Normal(0,1), dofs)) for i = 1:4] \nplts = [contour(points..., reshape(samples_constrained_flat[i], shape...)', fill = true,) for i =1:4]\nplot(plts..., legend=false, size=(800,800)) ","category":"page"},{"location":"parameter_distributions/#ConstraintType-Examples","page":"Prior distributions","title":"ConstraintType Examples","text":"","category":"section"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"For each for the predefined ConstraintTypes, we present animations of the resulting constrained prior distribution for","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"using EnsembleKalmanProcesses.ParameterDistributions, Distributions # hide\nμ = 0 # hide\nσ = 1 # hide\ndistribution = Parameterized(Normal(μ, σ))\nnothing # hide","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"where we vary μ and σ respectively. As noted above, in the presence of a nontrivial constraint, μ and σ will no longer correspond to the mean and standard deviation of the prior distribution (which is taken in the physical, constrained space).","category":"page"},{"location":"parameter_distributions/#Without-constraints:-\"constraint\"-no_constraints()","page":"Prior distributions","title":"Without constraints: \"constraint\" => no_constraints()","text":"","category":"section"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"The following specifies a prior based on an unconstrained Normal(0.5, 1) distribution:","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"using EnsembleKalmanProcesses.ParameterDistributions # for `Parameterized`, `no_constraint`, `ParameterDistribution`\nusing Distributions # for `Normal`\n\nparam_dict = Dict(\n\"distribution\" => Parameterized(Normal(0.5, 1)),\n\"constraint\" => no_constraint(),\n\"name\" => \"unbounded_parameter\",\n)\n\nprior = ParameterDistribution(param_dict)\nnothing # hide","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"where no_constraint() automatically defines the identity constraint map","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"transform_unconstrained_to_constrained(x) = x\nnothing # hide","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"The following plots show the effect of varying μ and σ in the constrained space (which is trivial here):","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"using Distributions\nusing Plots\nPlots.default(lw=2)\n\nN = 50\nx_eval = collect(-5:10/200:5)\nmean_varying = collect(-3:6/(N+1):3)\nsd_varying = collect(0.1:2.9/(N+1):3)\n\n# no constraint\ntransform_unconstrained_to_constrained(x) = x\n\nmean0norm(n) = pdf.(Normal(0, sd_varying[n]), x_eval)\nsd1norm(n) = pdf.(Normal(mean_varying[n], 1), x_eval)\nconstrained_x_eval = transform_unconstrained_to_constrained.(x_eval)\n\np1 = plot(constrained_x_eval, mean0norm.(1))\nvline!([transform_unconstrained_to_constrained(0)])\n\np2 = plot(constrained_x_eval, sd1norm.(1))\nvline!([transform_unconstrained_to_constrained(mean_varying[1])])\n\np = plot(p1, p2, layout=(1, 2), size = (900, 450), legend = false)\n \nanim_unbounded = @animate for n = 1:length(mean_varying)\n   #set new y data \n   p[1][1][:y] = mean0norm(n) \n   p[1][:title] = \"Transformed Normal(0, \" * string(round(sd_varying[n], digits=3)) * \")\" \n   p[2][1][:y] = sd1norm(n) \n   p[2][2][:x] = [transform_unconstrained_to_constrained(mean_varying[n]),\n                  transform_unconstrained_to_constrained(mean_varying[n]),\n                  transform_unconstrained_to_constrained(mean_varying[n])]\n\n   p[2][:title] = \"Transformed Normal(\" * string(round(mean_varying[n], digits=3)) * \", 1)\"\nend ","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"gif(anim_unbounded, \"anim_unbounded.gif\", fps = 5) # hide","category":"page"},{"location":"parameter_distributions/#Bounded-below-by-0:-\"constraint\"-bounded_below(0)","page":"Prior distributions","title":"Bounded below by 0: \"constraint\" => bounded_below(0)","text":"","category":"section"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"The following specifies a prior for a parameter which is bounded below by 0 (i.e. its only physical values are positive), and which has a Normal(0.5, 1) distribution in the unconstrained space:","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"using EnsembleKalmanProcesses.ParameterDistributions # for `Parameterized`, `bounded_below`, `ParameterDistribution`\nusing Distributions # for `Normal`\n\nparam_dict = Dict(\n\"distribution\" => Parameterized(Normal(0.5, 1)),\n\"constraint\" => bounded_below(0),\n\"name\" => \"bounded_below_parameter\",\n)\n\nprior = ParameterDistribution(param_dict)\nnothing # hide","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"where bounded_below(0) automatically defines the constraint map","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"transform_unconstrained_to_constrained(x) = exp(x)\nnothing # hide","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"The following plots show the effect of varying μ and σ in the physical, constrained space:","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"using Distributions\nusing Plots\nPlots.default(lw=2)\n\nN = 50\nx_eval = collect(-5:10/400:5)\nmean_varying = collect(-1:5/(N+1):4)\nsd_varying = collect(0.1:3.9/(N+1):4)\n\n#bounded below by 0\ntransform_unconstrained_to_constrained(x) = exp(x)\n\nmean0norm(n) = pdf.(Normal(0,sd_varying[n]), x_eval)\nsd1norm(n) = pdf.(Normal(mean_varying[n], 1), x_eval)\nconstrained_x_eval = transform_unconstrained_to_constrained.(x_eval)\n\np1 = plot(constrained_x_eval, mean0norm.(1))\nvline!([transform_unconstrained_to_constrained(0)])\n\np2 = plot(constrained_x_eval, sd1norm.(1))\nvline!([transform_unconstrained_to_constrained(mean_varying[1])])\n\np = plot(p1,p2, layout=(1,2), size = (900,450), legend=false)\n \nanim_bounded_below = @animate for n = 1:length(mean_varying) \n   #set new y data  \n   p[1][1][:y] = mean0norm(n) \n   p[1][:title] = \"Transformed Normal(0, \" * string(round(sd_varying[n], digits=3)) * \")\"\n   p[2][1][:y] = sd1norm(n) \n   p[2][2][:x] = [transform_unconstrained_to_constrained(mean_varying[n]),\n                  transform_unconstrained_to_constrained(mean_varying[n]),\n                  transform_unconstrained_to_constrained(mean_varying[n])]\n   p[2][:title] = \"Transformed Normal(\" * string(round(mean_varying[n], digits=3)) * \", 1)\"\nend ","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"gif(anim_bounded_below, \"anim_bounded_below.gif\", fps = 5) # hide","category":"page"},{"location":"parameter_distributions/#Bounded-above-by-10.0:-\"constraint\"-bounded_above(10)","page":"Prior distributions","title":"Bounded above by 10.0: \"constraint\" => bounded_above(10)","text":"","category":"section"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"The following specifies a prior for a parameter which is bounded above by ten, and which has a Normal(0.5, 1) distribution in the unconstrained space:","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"using EnsembleKalmanProcesses.ParameterDistributions # for `Parameterized`, `bounded_above`, `ParameterDistribution`\nusing Distributions\n\nparam_dict = Dict(\n\"distribution\" => Parameterized(Normal(0.5, 1)),\n\"constraint\" => bounded_above(10),\n\"name\" => \"bounded_above_parameter\",\n)\nprior = ParameterDistribution(param_dict)\nnothing # hide","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"where bounded_above(10) automatically defines the constraint map","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"transform_unconstrained_to_constrained(x) = 10 - exp(-x)\nnothing # hide","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"The following plots show the effect of varying μ and σ in the physical, constrained space:","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"using Distributions\nusing Plots\nPlots.default(lw=2)\n\nN = 50\nx_eval = collect(-5:4/400:5)\nmean_varying = collect(-1:5/(N+1):4)\nsd_varying = collect(0.1:3.9/(N+1):4)\n\n#bounded above by 10.0\ntransform_unconstrained_to_constrained(x) = 10 - exp(-x)\n\nmean0norm(n) = pdf.(Normal(0, sd_varying[n]), x_eval)\nsd1norm(n) = pdf.(Normal(mean_varying[n], 1), x_eval)\nconstrained_x_eval = transform_unconstrained_to_constrained.(x_eval)\n\np1 = plot(constrained_x_eval, mean0norm.(1))\nvline!([transform_unconstrained_to_constrained(0)])\n\np2 = plot(constrained_x_eval, sd1norm.(1))\nvline!([transform_unconstrained_to_constrained(mean_varying[1])])\n\np = plot(p1, p2, layout=(1, 2), size = (900, 450), legend=false)\n \nanim_bounded_above = @animate for n = 1:length(mean_varying)[1]\n  #set new y data\n   p[1][1][:y] = mean0norm(n)\n   p[1][:title] = \"Transformed Normal(0, \" * string(round(sd_varying[n], digits=3)) * \")\"\n   p[2][1][:y] = sd1norm(n)\n   p[2][2][:x] = [transform_unconstrained_to_constrained(mean_varying[n]),\n                  transform_unconstrained_to_constrained(mean_varying[n]),\n                  transform_unconstrained_to_constrained(mean_varying[n])]\n   p[2][:title] = \"Transformed Normal(\" * string(round(mean_varying[n], digits=3)) * \", 1)\"\nend ","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"gif(anim_bounded_above, \"anim_bounded_above.gif\", fps = 5) # hide","category":"page"},{"location":"parameter_distributions/#Bounded-between-5-and-10:-\"constraint\"-bounded(5,-10)","page":"Prior distributions","title":"Bounded between 5 and 10: \"constraint\" => bounded(5, 10)","text":"","category":"section"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"The following specifies a prior for a parameter whose physical values lie in the range between 5 and 10, and which has a Normal(0.5, 1) distribution in the unconstrained space:","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"using EnsembleKalmanProcesses.ParameterDistributions# for `Parameterized`, `bounded`, `ParameterDistribution`\nusing Distributions # for `Normal`\n\nparam_dict = Dict(\n\"distribution\" => Parameterized(Normal(0.5, 1)),\n\"constraint\" => bounded(5, 10),\n\"name\" => \"bounded_parameter\",\n)\n\nprior = ParameterDistribution(param_dict)\nnothing # hide","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"where bounded(-1, 5) automatically defines the constraint map","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"transform_unconstrained_to_constrained(x) = 10 - 5 / (exp(x) + 1)\nnothing # hide","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"The following plots show the effect of varying μ and σ in the physical, constrained space:","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"using Distributions\nusing Plots\nPlots.default(lw=2)\n\nN = 50\nx_eval = collect(-10:20/400:10)\nmean_varying = collect(-3:2/(N+1):3)\nsd_varying = collect(0.1:0.9/(N+1):10)\n\n#bounded in [5.0, 10.0]\ntransform_unconstrained_to_constrained(x) = 10 - 5 / (exp(x) + 1)\n\nmean0norm(n) = pdf.(Normal(0, sd_varying[n]), x_eval)\nsd1norm(n) = pdf.(Normal(mean_varying[n], 1), x_eval)\nconstrained_x_eval = transform_unconstrained_to_constrained.(x_eval)\n\np1 = plot(constrained_x_eval, mean0norm.(1))\nvline!([transform_unconstrained_to_constrained(0)])\n\np2 = plot(constrained_x_eval, sd1norm.(1))\nvline!([transform_unconstrained_to_constrained(mean_varying[1])])\n\np = plot(p1, p2, layout=(1, 2), size = (900, 450), legend=false)\n \nanim_bounded = @animate for n = 1:length(mean_varying)\n   #set new y data\n   p[1][1][:y] = mean0norm(n)\n   p[1][:title] = \"Transformed Normal(0, \" * string(round(sd_varying[n], digits=3)) * \")\"\n   p[2][1][:y] = sd1norm(n)\n   p[2][2][:x] = [transform_unconstrained_to_constrained(mean_varying[n]),\n                  transform_unconstrained_to_constrained(mean_varying[n]),\n                  transform_unconstrained_to_constrained(mean_varying[n])]\n   p[2][:title] = \"Transformed Normal(\" * string(round(mean_varying[n], digits=3)) * \", 1)\"\nend ","category":"page"},{"location":"parameter_distributions/","page":"Prior distributions","title":"Prior distributions","text":"gif(anim_bounded, \"anim_bounded.gif\", fps = 10) # hide","category":"page"},{"location":"internal_data_representation/#Wrapping-up-data","page":"Internal data representation","title":"Wrapping up data","text":"","category":"section"},{"location":"internal_data_representation/","page":"Internal data representation","title":"Internal data representation","text":"To provide a consistent form for data (such as observations, parameter ensembles, model evaluations) across the package, we store the data in simple wrappers internally.","category":"page"},{"location":"internal_data_representation/","page":"Internal data representation","title":"Internal data representation","text":"Data is always stored as columns of AbstractMatrix. That is, we obey the format","category":"page"},{"location":"internal_data_representation/","page":"Internal data representation","title":"Internal data representation","text":"[ data dimension x number of data samples ]","category":"page"},{"location":"internal_data_representation/#The-DataContainer","page":"Internal data representation","title":"The DataContainer","text":"","category":"section"},{"location":"internal_data_representation/","page":"Internal data representation","title":"Internal data representation","text":"A DataContainer is constructed initially by copying and perhaps transposing matrix data","category":"page"},{"location":"internal_data_representation/","page":"Internal data representation","title":"Internal data representation","text":"dc = DataContainer(abstract_matrix; data_are_columns = true)","category":"page"},{"location":"internal_data_representation/","page":"Internal data representation","title":"Internal data representation","text":"The flag data_are_columns indicates whether the provided data is stored column- or row-wise. The data is retrieved with","category":"page"},{"location":"internal_data_representation/","page":"Internal data representation","title":"Internal data representation","text":"get_data(dc)","category":"page"},{"location":"internal_data_representation/#The-PairedDataContainer","page":"Internal data representation","title":"The PairedDataContainer","text":"","category":"section"},{"location":"internal_data_representation/","page":"Internal data representation","title":"Internal data representation","text":"A PairedDataContainer stores pairs of inputs and outputs in the form of DataContainers. It is constructed from two data matrices, or from two DataContainers.","category":"page"},{"location":"internal_data_representation/","page":"Internal data representation","title":"Internal data representation","text":"pdc = PairedDataContainer(input_matrix, output_matrix; data_are_columns = true)\npdc = PairedDataContainer(input_data_container, output_data_container)","category":"page"},{"location":"internal_data_representation/","page":"Internal data representation","title":"Internal data representation","text":"Data is retrieved with","category":"page"},{"location":"internal_data_representation/","page":"Internal data representation","title":"Internal data representation","text":"get_data(pdc) # returns both inputs and outputs\nget_inputs(pdc)\nget_outputs(pdc)","category":"page"},{"location":"learning_rate_scheduler/#learning-rate-schedulers","page":"Learning rate schedulers","title":"Learning Rate Schedulers (a.k.a) Timestepping","text":"","category":"section"},{"location":"learning_rate_scheduler/#Overview","page":"Learning rate schedulers","title":"Overview","text":"","category":"section"},{"location":"learning_rate_scheduler/","page":"Learning rate schedulers","title":"Learning rate schedulers","text":"We demonstrate the behaviour of different learning rate schedulers through solution of a nonlinear inverse problem.","category":"page"},{"location":"learning_rate_scheduler/","page":"Learning rate schedulers","title":"Learning rate schedulers","text":"In this example we have a model that produces the exponential of a sinusoid f(A v) = exp(A sin(t) + v) forall t in 02pi. Given an initial guess of the parameters as A^* sim mathcalN(21) and v^* sim mathcalN(025), the inverse problem is to estimate the parameters from a noisy observation of only the maximum and mean value of the true model output.","category":"page"},{"location":"learning_rate_scheduler/","page":"Learning rate schedulers","title":"Learning rate schedulers","text":"We shall compare the following configurations of implemented schedulers. ","category":"page"},{"location":"learning_rate_scheduler/","page":"Learning rate schedulers","title":"Learning rate schedulers","text":"Fixed, \"long\" timestep DefaultScheduler(0.5) - orange\nFixed, \"short\" timestep DefaultScheduler(0.02) - green\nAdaptive timestep (designed originally to ensure EKS remains stable) EKSStableScheduler() Kovachki & Stuart 2018 - red\nMisfit controlling timestep (Terminating) DataMisfitController() Iglesias & Yang 2021 - purple\nMisfit controlling timestep (Continuing beyond Terminate condition) DataMisfitController(on_terminate=\"continue\") - brown","category":"page"},{"location":"learning_rate_scheduler/","page":"Learning rate schedulers","title":"Learning rate schedulers","text":"One can define the schedulers as","category":"page"},{"location":"learning_rate_scheduler/","page":"Learning rate schedulers","title":"Learning rate schedulers","text":"scheduler = DefaultScheduler(0.5) # fixed stepsize, default values: 1","category":"page"},{"location":"learning_rate_scheduler/","page":"Learning rate schedulers","title":"Learning rate schedulers","text":"Then when constructing an EnsembleKalmanProcess, one uses the keyword argument","category":"page"},{"location":"learning_rate_scheduler/","page":"Learning rate schedulers","title":"Learning rate schedulers","text":"ekpobj = EKP.EnsembleKalmanProcess(args...; scheduler = scheduler, kwargs...)","category":"page"},{"location":"learning_rate_scheduler/","page":"Learning rate schedulers","title":"Learning rate schedulers","text":"A variety of other schedulers can be defined similarly:","category":"page"},{"location":"learning_rate_scheduler/","page":"Learning rate schedulers","title":"Learning rate schedulers","text":"scheduler = MutableScheduler(2) # modifiable stepsize\nscheduler = EKSStableScheduler(numerator=10.0, nugget = 0.01) # Stable for EKS\nscheduler = DataMisfitController(on_terminate = \"continue\") # non-terminating","category":"page"},{"location":"learning_rate_scheduler/","page":"Learning rate schedulers","title":"Learning rate schedulers","text":"Please see the learning rate schedulers API for defaults and other details","category":"page"},{"location":"learning_rate_scheduler/#Early-termination","page":"Learning rate schedulers","title":"Early termination","text":"","category":"section"},{"location":"learning_rate_scheduler/","page":"Learning rate schedulers","title":"Learning rate schedulers","text":"Early termination can be implemented in the calibration loop as ","category":"page"},{"location":"learning_rate_scheduler/","page":"Learning rate schedulers","title":"Learning rate schedulers","text":"using EnsembleKalmanProcesses # for get_ϕ_final, update_ensemble!\n# given\n# * the number of iterations `N_iter`\n# * a prior `prior`\n# * a forward map `G`\n# * the EKP object `ekpobj`\n\nfor i in 1:N_iter\n    params_i = get_ϕ_final(prior, ekpobj)\n    g_ens = G(params_i)\n    terminated = update_ensemble!(ekpobj, g_ens) # check for termination\n    if !isnothing(terminated) # if termination is flagged, break the loop\n       break\n    end\nend ","category":"page"},{"location":"learning_rate_scheduler/#Timestep-and-termination-time","page":"Learning rate schedulers","title":"Timestep and termination time","text":"","category":"section"},{"location":"learning_rate_scheduler/","page":"Learning rate schedulers","title":"Learning rate schedulers","text":"Recall, for example for EKI, we perform updates of our ensemble of parameters j=1dotsJ at step n = 1dotsN_mathrmit using","category":"page"},{"location":"learning_rate_scheduler/","page":"Learning rate schedulers","title":"Learning rate schedulers","text":"theta_n+1^(j) = theta_n^(j) - dfracDelta t_nJsum_k=1^J left langle mathcalG(theta_n^(k)) - barmathcalG_n    Gamma_y^-1 left ( mathcalG(theta_n^(j)) - y right ) right rangle theta_n^(k)","category":"page"},{"location":"learning_rate_scheduler/","page":"Learning rate schedulers","title":"Learning rate schedulers","text":"where barmathcalG_n is the mean value of mathcalG(theta_n) across ensemble members. We denote the current time t_n = sum_i=1^nDelta t_i, and the termination time as T = t_N_mathrmit.","category":"page"},{"location":"learning_rate_scheduler/","page":"Learning rate schedulers","title":"Learning rate schedulers","text":"note: Note\nAdaptive Schedulers typically try to make the biggest update that controls some measure of this update. For example, EKSStableScheduler() controls the frobenius norm of the update, while DataMisfitController() controls the Jeffrey divergence between the two steps. Largely they follow a pattern of scheduling very small initial timesteps, leading to much larger steps at later times.","category":"page"},{"location":"learning_rate_scheduler/","page":"Learning rate schedulers","title":"Learning rate schedulers","text":"There are two termination times that the theory indicates are useful","category":"page"},{"location":"learning_rate_scheduler/","page":"Learning rate schedulers","title":"Learning rate schedulers","text":"T=1: In the linear Gaussian case, the theta_N_mathrmit will represent the posterior distribution. In nonlinear case it should still provide an approximation to the posterior distribution. Note that as the posterior does not necessarily optimize the data-misfit we find bartheta_N_mathrmit (the ensemble mean) provides a conservative estimate of the true parameters, while retaining spread. It is noted in Iglesias & Yang 2021 that with small enough (or well chosen) step-sizes this estimate at T=1 satisfies a discrepancy principle with respect to the observational noise.\nTto infty: Though theoretical concerns have been made with respect to continuation beyond T=1 for inversion methods such as EKI, in practice we commonly see better optimization of the data-misfit, and thus better representation bartheta_N_mathrmit to the true parameters. As expected this procedure leads to ensemble collapse, and so no meaningful information can be taken from the posterior spread, and the optimizer is not likely to be the posterior mode.","category":"page"},{"location":"learning_rate_scheduler/#The-experiment-with-EKI-and-UKI","page":"Learning rate schedulers","title":"The experiment with EKI & UKI","text":"","category":"section"},{"location":"learning_rate_scheduler/","page":"Learning rate schedulers","title":"Learning rate schedulers","text":"We assess the schedulers by solving the inverse problem with EKI and UKI (we average results over 100 initial ensembles in the case of EKI). We will not draw comparisons between EKI and UKI here, rather we use them to observe consistent behavior in the schedulers. Shown below are the solution plots of one solve with each timestepper, for both methods. ","category":"page"},{"location":"learning_rate_scheduler/","page":"Learning rate schedulers","title":"Learning rate schedulers","text":"(Image: Solution EKI) (Image: Solution UKI)","category":"page"},{"location":"learning_rate_scheduler/","page":"Learning rate schedulers","title":"Learning rate schedulers","text":"Top: EKI, Bottom: UKI. Left: The true model over 02pi (black), and solution schedulers (colors). Right: The noisy observation (black) of mean and max of the model; the distribution it was sampled from (gray-ribbon), and the corresponding ensemble-mean approximation given from each scheduler (colors).","category":"page"},{"location":"learning_rate_scheduler/","page":"Learning rate schedulers","title":"Learning rate schedulers","text":"To assess the timestepping we show the convergence plot against the algorithm iteration we measure two quantities.","category":"page"},{"location":"learning_rate_scheduler/","page":"Learning rate schedulers","title":"Learning rate schedulers","text":"error (solid) is defined by frac1N_enssum^N_ens_i=1  theta_i - theta^* ^2 where theta_i are ensemble members and theta^* is the true value used to create the observed data.\nspread (dashed) is defined by frac1N_enssum^N_ens_i=1  theta_i - bartheta ^2 where theta_i are ensemble members and bartheta is the mean over these members.","category":"page"},{"location":"learning_rate_scheduler/","page":"Learning rate schedulers","title":"Learning rate schedulers","text":"(Image: Error vs spread EKI) (Image: Error vs spread UKI)","category":"page"},{"location":"learning_rate_scheduler/","page":"Learning rate schedulers","title":"Learning rate schedulers","text":"Top: EKI. Bottom: UKI. Left: the error and spread of the different timesteppers at over iterations of the algorithm for a single run. Right: the error and spread of the different timesteppers at their final iterations, (for EKI, averaged from 100 initial conditions).","category":"page"},{"location":"learning_rate_scheduler/","page":"Learning rate schedulers","title":"Learning rate schedulers","text":"Finding the Posterior (terminating at T=1):","category":"page"},{"location":"learning_rate_scheduler/","page":"Learning rate schedulers","title":"Learning rate schedulers","text":"DMC with termination (purple), closely mimics a small-fixed timestep (green) that finishes stepping at T=1. Both retain more spread than other approaches, and DMC is far more efficient, typically terminating after around 10-20 steps, where fixed-stepping takes 50. We see that (for this experiment) this is a conservative estimate, as continuing to solve (e.g. brown) until later times often leads to a better error while still retaining similar \"error vs spread\" curves (before inevitable collapse). This is consistent with the concept of approximating the posterior, over seeking an optimizer.\nThe behavior observed in UKI is similar to EKI","category":"page"},{"location":"learning_rate_scheduler/","page":"Learning rate schedulers","title":"Learning rate schedulers","text":"Optimizing the objective function (continuing T to infty):","category":"page"},{"location":"learning_rate_scheduler/","page":"Learning rate schedulers","title":"Learning rate schedulers","text":"Large fixed step (orange). This is very efficient, but can get stuck when drawn too large, (perhaps unintuitive from a gradient-descent perspective). It typically also collapses the ensemble. On average it gives lower error to the true parameters than DMC. \nBoth EKSStable and DMC with continuation schedulers, perform very similarly. Both retain good ensemble spread during convergence, and collapse after finding a local optimum. This optimum on average has the best error to the true parameters in this experiment. They appear to consistently find the same optimum as Ttoinfty but DMC finds this in fewer iterations.\nThe UKI behavior is largely similar to EKI here, except that ensemble spread is retained in the Ttoinfty limit in all cases, from inflation of the parameter covariance (Sigma_omega) within our implementation. ","category":"page"},{"location":"learning_rate_scheduler/#DMC-as-a-default-in-future?","page":"Learning rate schedulers","title":"DMC as a default in future?","text":"","category":"section"},{"location":"learning_rate_scheduler/","page":"Learning rate schedulers","title":"Learning rate schedulers","text":"This experiment motivates the possibility of making DMC (with/without) continuation a default timestepper in future releases, for EKI/SEKI/UKI. Currently we will retain constant timestepping as default while we investigate further.","category":"page"},{"location":"learning_rate_scheduler/","page":"Learning rate schedulers","title":"Learning rate schedulers","text":"warning: Ensemble Kalman Sampler\nWe observe blow-up in EKS, when not using the EKSStableScheduler.","category":"page"},{"location":"examples/darcy/#Learning-the-permiability-field-in-a-Darcy-flow","page":"Darcy flow","title":"Learning the permiability field in a Darcy flow","text":"","category":"section"},{"location":"examples/darcy/","page":"Darcy flow","title":"Darcy flow","text":"In this example, we illustrate a simple function learning problem. We are presented with an unknown field that is discretized with a finite-dimensional approximation (e.g. spatial discretization). When learning this field, if one represents each pointwise value at a gridpoint as a parameter, increasing the spatial resolution leads to increasingly high dimensional learning problems, thus giving poor computational scaling and increasingly ill-posed inverse problems from fixed data. If instead, we treat the approximation as a discretized function living in a function space, then one can learn coefficients of a basis of this function space. Since it is commonly the case that functions have relatively low effective dimension in this space, the dependence on the spatial discretization only arises in discretization error, which vanishes as resolution is increased.","category":"page"},{"location":"examples/darcy/","page":"Darcy flow","title":"Darcy flow","text":"We will solve for an unknown permeability field kappa governing the pressure field of a Darcy flow on a square 2D domain. To learn about the permeability we shall take few pointwise measurements of the solved pressure field within the domain. The forward solver is a simple finite difference scheme taken and modified from code here.","category":"page"},{"location":"examples/darcy/#Walkthrough-of-the-code","page":"Darcy flow","title":"Walkthrough of the code","text":"","category":"section"},{"location":"examples/darcy/","page":"Darcy flow","title":"Darcy flow","text":"First we load standard packages,","category":"page"},{"location":"examples/darcy/","page":"Darcy flow","title":"Darcy flow","text":"using LinearAlgebra\nusing Distributions\nusing Random\nusing JLD2","category":"page"},{"location":"examples/darcy/","page":"Darcy flow","title":"Darcy flow","text":"the package to define the function distributions,","category":"page"},{"location":"examples/darcy/","page":"Darcy flow","title":"Darcy flow","text":"import GaussianRandomFields # we wrap this so we don't want to use \"using\"\nconst GRF = GaussianRandomFields","category":"page"},{"location":"examples/darcy/","page":"Darcy flow","title":"Darcy flow","text":"and finally the EKP packages.","category":"page"},{"location":"examples/darcy/","page":"Darcy flow","title":"Darcy flow","text":"using EnsembleKalmanProcesses\nusing EnsembleKalmanProcesses.ParameterDistributions\nconst EKP = EnsembleKalmanProcesses","category":"page"},{"location":"examples/darcy/","page":"Darcy flow","title":"Darcy flow","text":"We include the forward solver here.","category":"page"},{"location":"examples/darcy/","page":"Darcy flow","title":"Darcy flow","text":"include(\"GModel.jl\")","category":"page"},{"location":"examples/darcy/","page":"Darcy flow","title":"Darcy flow","text":"We define the spatial domain and discretization,","category":"page"},{"location":"examples/darcy/","page":"Darcy flow","title":"Darcy flow","text":"rng = Random.MersenneTwister(seed)\ndim = 2\nN, L = 80, 1.0\npts_per_dim = LinRange(0, L, N)","category":"page"},{"location":"examples/darcy/","page":"Darcy flow","title":"Darcy flow","text":"To provide a simple test case, we assume that the true function parameter is a particular sample from the function space we set up to define our prior. We choose a value of the truth that doesnt have a vanishingly small probability under the prior defined by a probability distribution over functions; taken to be a family of Gaussian Random Fields (GRF). This function distribution is characterized by a covariance function (Matern) and an appropriate representation (Karhunen-Loeve expansion). The representation is truncated to a finite number of coefficients, the degrees of freedom (dofs), which define the effective dimension of the learning problem that is decoupled from the spatial discretization. Larger dofs may be required to represent multiscale functions, but come at an increased dimension of the parameter space and therefore a typical increase in cost and difficulty of the learning problem. For more details see GaussianRandomFields.jl","category":"page"},{"location":"examples/darcy/","page":"Darcy flow","title":"Darcy flow","text":"smoothness = 2.0\ncorr_length = 0.5\ndofs = 50\n\ngrf = GRF.GaussianRandomField(\n    GRF.CovarianceFunction(dim, GRF.Matern(smoothness, corr_length)),\n    GRF.KarhunenLoeve(dofs),\n    pts_per_dim,\n    pts_per_dim,\n)","category":"page"},{"location":"examples/darcy/","page":"Darcy flow","title":"Darcy flow","text":"We define a wrapper around the GRF, and as the permeability field must be positive we introduce a domain constraint into the function distribution. ","category":"page"},{"location":"examples/darcy/","page":"Darcy flow","title":"Darcy flow","text":"pkg = GRFJL()\ndistribution = GaussianRandomFieldInterface(grf, pkg) # our wrapper from EKP\ndomain_constraint = bounded_below(0) # make κ positive\npd = ParameterDistribution(\n    Dict(\"distribution\" => distribution, \"name\" => \"kappa\", \"constraint\" => domain_constraint),\n) # the fully constrained parameter distribution","category":"page"},{"location":"examples/darcy/","page":"Darcy flow","title":"Darcy flow","text":"Henceforth, the GRF is interfaced in the same manner as any other parameter distribution with regards to interface. We choose the true value by setting all degrees of freedom u_mathrmtrue = -15; this choice is arbitrary, upto not having a vanishingly small mass under the prior. We then use the EKP transform function to build the corresponding instance of the kappa_mathrmtrue.","category":"page"},{"location":"examples/darcy/","page":"Darcy flow","title":"Darcy flow","text":"u_true = -1.5 * ones(dofs,1) # the truth parameter\nκ_true = transform_unconstrained_to_constrained(pd, u_true) # builds and constrains the function.\nκ_true = reshape(κ_true, N, N)","category":"page"},{"location":"examples/darcy/","page":"Darcy flow","title":"Darcy flow","text":"We generate the data sample for the truth in a perfect model setting by evaluating the the model here, and observing the pressure field at a few subsampled points in each dimension (here obs_ΔN, samples every 10 points in each dimension, leading to a 7 times 7 observation grid), and we assume 5% additive observational noise on the measurements.","category":"page"},{"location":"examples/darcy/","page":"Darcy flow","title":"Darcy flow","text":"obs_ΔN = 10 \ndarcy = Setup_Param(pts_per_dim, obs_ΔN, κ_true) \nh_2d = solve_Darcy_2D(darcy, κ_true)\ny_noiseless = compute_obs(darcy, h_2d)\nobs_noise_cov = 0.05^2 * I(length(y_noiseless)) * (maximum(y_noiseless) - minimum(y_noiseless))\ntruth_sample = vec(y_noiseless + rand(rng, MvNormal(zeros(length(y_noiseless)), obs_noise_cov)))","category":"page"},{"location":"examples/darcy/","page":"Darcy flow","title":"Darcy flow","text":"Now we set up the Bayesian inversion algorithm. The prior we have already defined to construct our truth","category":"page"},{"location":"examples/darcy/","page":"Darcy flow","title":"Darcy flow","text":"prior = pd","category":"page"},{"location":"examples/darcy/","page":"Darcy flow","title":"Darcy flow","text":"We define some algorithm parameters, here we take ensemble members larger than the dimension of the parameter space to ensure a full rank ensemble covariance.","category":"page"},{"location":"examples/darcy/","page":"Darcy flow","title":"Darcy flow","text":"N_ens = dofs + 2 # number of ensemble members\nN_iter = 20 # number of EKI iterations","category":"page"},{"location":"examples/darcy/","page":"Darcy flow","title":"Darcy flow","text":"We sample the initial ensemble from the prior, and create the EKP object as an EKI algorithm using the Inversion() keyword, we also use the DataMisfitController() learning rate scheduler","category":"page"},{"location":"examples/darcy/","page":"Darcy flow","title":"Darcy flow","text":"initial_params = construct_initial_ensemble(rng, prior, N_ens) \nekiobj = EKP.EnsembleKalmanProcess(initial_params, truth_sample, obs_noise_cov, Inversion(), scheduler=DataMisfitController())","category":"page"},{"location":"examples/darcy/","page":"Darcy flow","title":"Darcy flow","text":"We perform the inversion loop. Remember that within calls to get_ϕ_final the EKP transformations are applied, thus the ensemble that is returned will be the positively-bounded permeability field evaluated at all the discretization points. Each ensemble member is stored as a column and therefore for uses such as plotting one needs to reshape to the desired dimension.","category":"page"},{"location":"examples/darcy/","page":"Darcy flow","title":"Darcy flow","text":"err = zeros(N_iter)\nfor i in 1:N_iter\n    params_i = get_ϕ_final(prior, ekiobj)\n    g_ens = run_G_ensemble(darcy, params_i)\n    EKP.update_ensemble!(ekiobj, g_ens)\nend","category":"page"},{"location":"examples/darcy/#Inversion-results","page":"Darcy flow","title":"Inversion results","text":"","category":"section"},{"location":"examples/darcy/","page":"Darcy flow","title":"Darcy flow","text":"We plot first the prior ensemble mean and pointwise variance of the permeability field, and also the pressure field solved with the ensemble mean. ","category":"page"},{"location":"examples/darcy/","page":"Darcy flow","title":"Darcy flow","text":"(Image: Darcy prior)","category":"page"},{"location":"examples/darcy/","page":"Darcy flow","title":"Darcy flow","text":"Now we plot the final ensemble mean and pointwise variance of the permeability field, and also the pressure field solved with the ensemble mean.","category":"page"},{"location":"examples/darcy/","page":"Darcy flow","title":"Darcy flow","text":"(Image: Darcy final)","category":"page"},{"location":"examples/darcy/","page":"Darcy flow","title":"Darcy flow","text":"We can compare this with the true permeability and pressure field:","category":"page"},{"location":"examples/darcy/","page":"Darcy flow","title":"Darcy flow","text":"(Image: Darcy truth)","category":"page"},{"location":"glossary/#Glossary","page":"Glossary","title":"Glossary","text":"","category":"section"},{"location":"glossary/","page":"Glossary","title":"Glossary","text":"The following list includes the names and symbols of recurring concepts in EnsembleKalmanProcesses.jl. Some of these variables do not appear in the codebase, which relies on array programming for performance.  Contributions to the codebase require following this notational convention. Similarly, if you find inconsistencies in the documentation or codebase, please report an issue on GitHub.","category":"page"},{"location":"glossary/","page":"Glossary","title":"Glossary","text":"Name Symbol (Theory/Docs) Symbol (Code)\nParameter vector, Parameters (unconstrained space) theta, u, mathcalT(phi) θ,u\nParameter vector, Parameters (physical / constrained space) phi, mathcalT^-1(theta) ϕ\nParameter vector size, Number of parameters p N_par\nEnsemble size J N_ens\nEnsemble particles, members theta^(j) \nNumber of iterations N_rm it N_iter\nObservation vector, Observations, Data vector y y\nObservation vector size, Data vector size d N_obs\nObservational noise eta obs_noise\nObservational noise covariance Gamma_y obs_noise_cov\nHilbert space inner product langle phi  Gamma^-1 psi rangle \nForward map mathcalG G\nDynamical model Psi Ψ\nTransform map (constrained to unconstrained) mathcalT T\nObservation map mathcalH H\nPrior covariance (unconstrained space) Gamma_theta prior_cov\nPrior mean (unconstrained space) m_theta prior_mean","category":"page"},{"location":"API/Inversion/#Ensemble-Kalman-Inversion","page":"Inversion","title":"Ensemble Kalman Inversion","text":"","category":"section"},{"location":"API/Inversion/","page":"Inversion","title":"Inversion","text":"CurrentModule = EnsembleKalmanProcesses","category":"page"},{"location":"API/Inversion/","page":"Inversion","title":"Inversion","text":"Inversion\neki_update","category":"page"},{"location":"API/Inversion/#EnsembleKalmanProcesses.Inversion","page":"Inversion","title":"EnsembleKalmanProcesses.Inversion","text":"Inversion <: Process\n\nAn ensemble Kalman Inversion process\n\n\n\n\n\n","category":"type"},{"location":"API/Inversion/#EnsembleKalmanProcesses.eki_update","page":"Inversion","title":"EnsembleKalmanProcesses.eki_update","text":" eki_update(\n    ekp::EnsembleKalmanProcess{FT, IT, Inversion},\n    u::AbstractMatrix{FT},\n    g::AbstractMatrix{FT},\n    y::AbstractMatrix{FT},\n    obs_noise_cov::Union{AbstractMatrix{CT}, UniformScaling{CT}},\n) where {FT <: Real, IT, CT <: Real}\n\nReturns the updated parameter vectors given their current values and the corresponding forward model evaluations, using the inversion algorithm from eqns. (4) and (5) of Schillings and Stuart (2017).\n\nLocalization is implemented following the ekp.localizer.\n\n\n\n\n\n","category":"function"},{"location":"examples/lorenz_example/#Lorenz-example","page":"Lorenz","title":"Lorenz 96 example","text":"","category":"section"},{"location":"examples/lorenz_example/#Overview","page":"Lorenz","title":"Overview","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"The Lorenz 96 (hereafter L96) example is a toy-problem for the application of the EnsembleKalmanProcesses.jl optimization and approximate uncertainty quantification methodologies. Here is L96 with additional periodic-in-time forcing, we try to determine parameters (sinusoidal amplitude and stationary component of the forcing) from some output statistics. The standard L96 equations are implemented with an additional forcing term with time dependence. The output statistics which are used for learning are the finite time-averaged variances.","category":"page"},{"location":"examples/lorenz_example/#Lorenz-96-equations","page":"Lorenz","title":"Lorenz 96 equations","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"The standard single-scale L96 equations are implemented. The Lorenz 96 system (Lorenz, 1996) is given by ","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"fracd x_id t = (x_i+1 - x_i-2) x_i-1 - x_i + F","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"with i indicating the index of the given longitude. The number of longitudes is given by N. The boundary conditions are given by","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"x_-1 = x_N-1  x_0 = x_N  x_N+1 = x_1","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"The time scaling is such that the characteristic time is 5 days (Lorenz, 1996).  For very small values of F, the solutions x_i decay to F after the initial transient feature. For moderate values of F, the solutions are periodic, and for larger values of F, the system is chaotic. The solution variance is a function of the forcing magnitude. Variations in the base state as a function of time can be imposed through a time-dependent forcing term F(t).","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"A temporal forcing term is defined","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"F = F_s + A sin(omega t)","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"with steady-state forcing F_s, transient forcing amplitude A, and transient forcing frequency omega. The total forcing F must be within the chaotic regime of L96 for all time given the prescribed N.","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"The L96 dynamics are solved with RK4 integration.","category":"page"},{"location":"examples/lorenz_example/#Structure","page":"Lorenz","title":"Structure","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"The main code is located in Lorenz_example.jl which provides the functionality to run the L96 dynamical system, extract time-averaged statistics from the L96 states, and use the time-average statistics for optimization and uncertainty quantification.","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"The L96 system is solved in GModel.jl according to the time integration settings specified in LSettings and the L96 parameters specified in LParams. The types of statistics to be collected are detailed in GModel.jl.","category":"page"},{"location":"examples/lorenz_example/#Lorenz-dynamics-inputs","page":"Lorenz","title":"Lorenz dynamics inputs","text":"","category":"section"},{"location":"examples/lorenz_example/#Dynamics-settings","page":"Lorenz","title":"Dynamics settings","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"The use of the transient forcing term is with the flag, dynamics. Stationary forcing is dynamics=1 (A=0) and transient forcing is used with dynamics=2 (Aneq0). The default parameters are specified in Lorenz_example.jl and can be modified as necessary. The system is solved over time horizon 0 to tend at fixed time step dt.","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"N = 36\ndt = 1/64\nt_start = 800","category":"page"},{"location":"examples/lorenz_example/#Inverse-problem-settings","page":"Lorenz","title":"Inverse problem settings","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"The states are integrated over time Ts_days to construct the time averaged statistics for use by the optimization. The specification of the statistics to be gathered from the states are provided by stats_type. The Ensemble Kalman Process (EKP) settings are","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"N_ens = 20 # number of ensemble members\nN_iter = 5 # number of EKI iterations","category":"page"},{"location":"examples/lorenz_example/#Setting-up-the-Inverse-Problem","page":"Lorenz","title":"Setting up the Inverse Problem","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"The goal is to learn F_s and A based on the time averaged statistics in a perfect model setting. The true parameters are","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"F_true = 8. # Mean F\nA_true = 2.5 # Transient F amplitude\nω_true = 2. * π / (360. / τc) # Frequency of the transient F\nparams_true = [F_true, A_true]\nparam_names = [\"F\", \"A\"]","category":"page"},{"location":"examples/lorenz_example/#Priors","page":"Lorenz","title":"Priors","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"We implement (biased) priors as follows","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"prior_means = [F_true + 1.0, A_true + 0.5]\nprior_stds = [2.0, 0.5 * A_true]\n# constrained_gaussian(\"name\", desired_mean, desired_std, lower_bd, upper_bd)\nprior_F = constrained_gaussian(param_names[1], prior_means[1], prior_stds[1], 0, Inf)\nprior_A = constrained_gaussian(param_names[2], prior_means[2], prior_stds[2], 0, Inf)\npriors = combine_distributions([prior_F, prior_A])","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"We use the recommended constrained_gaussian to add the desired scale and bounds to the prior distribution, in particular we place lower bounds to preserve positivity. ","category":"page"},{"location":"examples/lorenz_example/#Observational-Noise","page":"Lorenz","title":"Observational Noise","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"The observational noise can be generated using the L96 system or prescribed, as specified by var_prescribe. ","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"var_prescribe==false The observational noise is constructed by generating independent instantiations of the L96 statistics of interest at the true parameters for different initial conditions. The empirical covariance matrix is constructed.","category":"page"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"var_prescribe==true The observational noise is prescribed as a Gaussian distribution with prescribed mean and variance.","category":"page"},{"location":"examples/lorenz_example/#Running-the-Example","page":"Lorenz","title":"Running the Example","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"The L96 parameter estimation can be run using julia --project Lorenz_example.jl","category":"page"},{"location":"examples/lorenz_example/#Solution-and-Output","page":"Lorenz","title":"Solution and Output","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"The output will provide the estimated parameters in the constrained ϕ-space. The priors are required in the get-method to apply these constraints.","category":"page"},{"location":"examples/lorenz_example/#Printed-output","page":"Lorenz","title":"Printed output","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"# EKI results: Has the ensemble collapsed toward the truth?\nprintln(\"True parameters: \")\nprintln(params_true)\nprintln(\"\\nEKI results:\")\nprintln(get_ϕ_mean_final(priors, ekiobj))","category":"page"},{"location":"examples/lorenz_example/#Saved-output","page":"Lorenz","title":"Saved output","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"The parameters and forward model outputs will be saved in parameter_storage.jld2 and data_storage.jld2, respectively. The data will be saved in the directory output.","category":"page"},{"location":"examples/lorenz_example/#Plots","page":"Lorenz","title":"Plots","text":"","category":"section"},{"location":"examples/lorenz_example/","page":"Lorenz","title":"Lorenz","text":"A scatter plot animation of the ensemble convergence to the true parameters is saved in the directory output.","category":"page"},{"location":"literated/sinusoid_example/","page":"Simple example","title":"Simple example","text":"EditURL = \"../../../examples/Sinusoid/sinusoid_example.jl\"","category":"page"},{"location":"literated/sinusoid_example/#sinusoid-example","page":"Simple example","title":"Fitting parameters of a sinusoid","text":"","category":"section"},{"location":"literated/sinusoid_example/","page":"Simple example","title":"Simple example","text":"In this example we have a model that produces a sinusoid f(A v) = A sin(phi + t) + v forall t in 02pi, with a random phase phi. Given an initial guess of the parameters as A^* sim mathcalN(21) and v^* sim mathcalN(025), our goal is to estimate the parameters from a noisy observation of the maximum, minimum, and mean of the true model output.","category":"page"},{"location":"literated/sinusoid_example/","page":"Simple example","title":"Simple example","text":"First, we load the packages we need:","category":"page"},{"location":"literated/sinusoid_example/","page":"Simple example","title":"Simple example","text":"using LinearAlgebra, Random\n\nusing Distributions, Plots\n\nusing EnsembleKalmanProcesses\nusing EnsembleKalmanProcesses.ParameterDistributions\nconst EKP = EnsembleKalmanProcesses\nnothing # hide\n\n# Setting up the model and data for our inverse problem","category":"page"},{"location":"literated/sinusoid_example/","page":"Simple example","title":"Simple example","text":"Now, we define a model which generates a sinusoid given parameters theta: an amplitude and a vertical shift. We will estimate these parameters from data. The model adds a random phase shift upon evaluation.","category":"page"},{"location":"literated/sinusoid_example/","page":"Simple example","title":"Simple example","text":"dt = 0.01\ntrange = 0:dt:(2 * pi + dt)\nfunction model(amplitude, vert_shift)\n    phi = 2 * pi * rand(rng)\n    return amplitude * sin.(trange .+ phi) .+ vert_shift\nend\nnothing # hide","category":"page"},{"location":"literated/sinusoid_example/","page":"Simple example","title":"Simple example","text":"Seed for pseudo-random number generator.","category":"page"},{"location":"literated/sinusoid_example/","page":"Simple example","title":"Simple example","text":"rng_seed = 41\nrng = Random.MersenneTwister(rng_seed)\nnothing # hide","category":"page"},{"location":"literated/sinusoid_example/","page":"Simple example","title":"Simple example","text":"We then define G(theta), which returns the observables of the sinusoid given a parameter vector. These observables should be defined such that they are informative about the parameters we wish to estimate. Here, the two observables are the y range of the curve (which is informative about its amplitude), as well as its mean (which is informative about its vertical shift).","category":"page"},{"location":"literated/sinusoid_example/","page":"Simple example","title":"Simple example","text":"function G(u)\n    theta, vert_shift = u\n    sincurve = model(theta, vert_shift)\n    return [maximum(sincurve) - minimum(sincurve), mean(sincurve)]\nend\nnothing # hide","category":"page"},{"location":"literated/sinusoid_example/","page":"Simple example","title":"Simple example","text":"Suppose we have a noisy observation of the true system. Here, we create a pseudo-observation y by running our model with the correct parameters and adding Gaussian noise to the output.","category":"page"},{"location":"literated/sinusoid_example/","page":"Simple example","title":"Simple example","text":"dim_output = 2\n\nΓ = 0.1 * I\nnoise_dist = MvNormal(zeros(dim_output), Γ)\n\ntheta_true = [1.0, 7.0]\ny = G(theta_true) .+ rand(noise_dist)\nnothing # hide\n\n# Solving the inverse problem","category":"page"},{"location":"literated/sinusoid_example/","page":"Simple example","title":"Simple example","text":"We now define prior distributions on the two parameters. For the amplitude, we define a prior with mean 2 and standard deviation 1. It is additionally constrained to be nonnegative. For the vertical shift we define a Gaussian prior with mean 0 and standard deviation 5.","category":"page"},{"location":"literated/sinusoid_example/","page":"Simple example","title":"Simple example","text":"prior_u1 = constrained_gaussian(\"amplitude\", 2, 1, 0, Inf)\nprior_u2 = constrained_gaussian(\"vert_shift\", 0, 5, -Inf, Inf)\nprior = combine_distributions([prior_u1, prior_u2])\nnothing # hide","category":"page"},{"location":"literated/sinusoid_example/","page":"Simple example","title":"Simple example","text":"We now generate the initial ensemble and set up the ensemble Kalman inversion.","category":"page"},{"location":"literated/sinusoid_example/","page":"Simple example","title":"Simple example","text":"N_ensemble = 5\nN_iterations = 5\n\ninitial_ensemble = EKP.construct_initial_ensemble(rng, prior, N_ensemble)\n\nensemble_kalman_process = EKP.EnsembleKalmanProcess(initial_ensemble, y, Γ, Inversion(); rng = rng)\nnothing # hide","category":"page"},{"location":"literated/sinusoid_example/","page":"Simple example","title":"Simple example","text":"We are now ready to carry out the inversion. At each iteration, we get the ensemble from the last iteration, apply G(theta) to each ensemble member, and apply the Kalman update to the ensemble.","category":"page"},{"location":"literated/sinusoid_example/","page":"Simple example","title":"Simple example","text":"for i in 1:N_iterations\n    params_i = get_ϕ_final(prior, ensemble_kalman_process)\n\n    G_ens = hcat([G(params_i[:, i]) for i in 1:N_ensemble]...)\n\n    EKP.update_ensemble!(ensemble_kalman_process, G_ens)\nend\nnothing # hide","category":"page"},{"location":"literated/sinusoid_example/","page":"Simple example","title":"Simple example","text":"Finally, we get the ensemble after the last iteration. This provides our estimate of the parameters.","category":"page"},{"location":"literated/sinusoid_example/","page":"Simple example","title":"Simple example","text":"final_ensemble = get_ϕ_final(prior, ensemble_kalman_process)","category":"page"},{"location":"literated/sinusoid_example/","page":"Simple example","title":"Simple example","text":"To visualize the success of the inversion, we plot model with the true parameters, the initial ensemble, and the final ensemble.","category":"page"},{"location":"literated/sinusoid_example/","page":"Simple example","title":"Simple example","text":"plot(trange, model(theta_true...), c = :black, label = \"Truth\", legend = :bottomright, linewidth = 2)\nplot!(\n    trange,\n    [model(get_ϕ(prior, ensemble_kalman_process, 1)[:, i]...) for i in 1:N_ensemble],\n    c = :red,\n    label = [\"Initial ensemble\" \"\" \"\" \"\" \"\"],\n)\nplot!(trange, [model(final_ensemble[:, i]...) for i in 1:N_ensemble], c = :blue, label = [\"Final ensemble\" \"\" \"\" \"\" \"\"])\n\nxlabel!(\"Time\")","category":"page"},{"location":"literated/sinusoid_example/","page":"Simple example","title":"Simple example","text":"We see that the final ensemble is much closer to the truth. Note that the random phase shift is of no consequence.","category":"page"},{"location":"literated/sinusoid_example/","page":"Simple example","title":"Simple example","text":"","category":"page"},{"location":"literated/sinusoid_example/","page":"Simple example","title":"Simple example","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/template_example/#Template-example","page":"Template","title":"Template example","text":"","category":"section"},{"location":"examples/template_example/","page":"Template","title":"Template","text":"We provide the following template for how the tools may be applied.","category":"page"},{"location":"examples/template_example/","page":"Template","title":"Template","text":"For small examples typically have 2 files.","category":"page"},{"location":"examples/template_example/","page":"Template","title":"Template","text":"DynamicalModel.jl Contains the dynamical model Psi and the observation map mathcalH. The inputs should be the so-called free parameters (in the constrained/physical space that is the input domain of the dynamical model) we are interested in learning, and the output should be the measured data.\nThe example script which contains the inverse problem setup and solve","category":"page"},{"location":"examples/template_example/#The-structure-of-the-example-script","page":"Template","title":"The structure of the example script","text":"","category":"section"},{"location":"examples/template_example/#Create-the-data-and-the-setting-for-the-model","page":"Template","title":"Create the data and the setting for the model","text":"","category":"section"},{"location":"examples/template_example/","page":"Template","title":"Template","text":"Set up the forward model.\nConstruct/load the truth data. ","category":"page"},{"location":"examples/template_example/#Set-up-the-inverse-problem","page":"Template","title":"Set up the inverse problem","text":"","category":"section"},{"location":"examples/template_example/","page":"Template","title":"Template","text":"Define the prior distributions, and generate an initial ensemble.\nInitialize the process tool you would like to use (we recommend you begin with Inversion()). \ninitialize the EnsembleKalmanProcess object","category":"page"},{"location":"examples/template_example/#Solve-the-inverse-problem,-in-a-loop","page":"Template","title":"Solve the inverse problem, in a loop","text":"","category":"section"},{"location":"examples/template_example/","page":"Template","title":"Template","text":"Obtain the current parameter ensemble\nTransform them from the unbounded computational space to the physical space\ncall the forward model on the ensemble of parameters, producing an ensemble of measured data\ncall the update_ensemble! function to generate a new parameter ensemble based on the new data","category":"page"},{"location":"examples/template_example/#Get-the-solution","page":"Template","title":"Get the solution","text":"","category":"section"},{"location":"examples/template_example/","page":"Template","title":"Template","text":"Obtain the final parameter ensemble, compute desired statistics here.\nTransform the final ensemble into the physical space for use in prediction studies with the forward model.","category":"page"},{"location":"localization/#Localization-and-Sampling-Error-Correction-(SEC)","page":"Localization and SEC","title":"Localization and Sampling Error Correction (SEC)","text":"","category":"section"},{"location":"localization/","page":"Localization and SEC","title":"Localization and SEC","text":"Ensemble Kalman inversion (EKI) seeks to find an optimal parameter vector theta in mathbbR^p by minimizing the mismatch between some data y in mathbbR^d and the forward model output mathcalG(theta) in mathbbR^d. Instead of relying on derivatives of the map mathcalG with respect to theta to find the optimum, EKI leverages sample covariances mathrmCov(theta mathcalG) and  mathrmCov(mathcalG mathcalG) diagnosed from an ensemble of J particles,","category":"page"},{"location":"localization/","page":"Localization and SEC","title":"Localization and SEC","text":"   tag1\n   beginaligned\n         mathrmCov(theta mathcalG) = dfrac1Jsum_j=1^J\n        (theta^(j) - m)(mathcalG(theta^(j)) - barmathcalG)^T \n\n        mathrmCov(mathcalG mathcalG) = dfrac1Jsum_j=1^J\n        (mathcalG(theta^(j)) - barmathcalG)(mathcalG(theta^(j)) - barmathcalG)^T \n    endaligned","category":"page"},{"location":"localization/","page":"Localization and SEC","title":"Localization and SEC","text":"where m and barmathcalG are the ensemble averages of theta and mathcalG(theta), respectively.","category":"page"},{"location":"localization/","page":"Localization and SEC","title":"Localization and SEC","text":"For models with just a few (p) parameters, we can typically afford to use J  p ensemble members, such that the sample covariance  mathrmCov(theta mathcalG) is full rank. Using more ensemble members than the number of model parameters, EKI can in theory probe all dimensions of parameter space to find the optimum parameter vector.","category":"page"},{"location":"localization/","page":"Localization and SEC","title":"Localization and SEC","text":"For models with a lot of parameters (e.g., a deep neural network), computational constraints limit the size of the ensemble to J  p or even J ll p members. Due to the characteristics of the EKI update equation, this means that the method can only find the minimum in the (J-1)-dimensional space spanned by the initial ensemble, leaving p-J+1 dimensions unexplored. This is known as the subspace property of EKI. As the dimensional gap p-J increases, we can expect the solution of the algorithm to deteriorate.","category":"page"},{"location":"localization/#Enter-Localization","page":"Localization and SEC","title":"Enter Localization","text":"","category":"section"},{"location":"localization/","page":"Localization and SEC","title":"Localization and SEC","text":"In algebraic terms, the extent to which we can explore the dimensions of parameter space is roughly given by the rank of the matrices in equation (1). In the case J  p, the rank of mathrmCov(theta mathcalG) is limited to mathrmmin(d J-1). It has been shown that the performance of ensemble Kalman methods with J  p can be greatly improved by boosting this rank through an elementwise product with a suitable localization kernel Lambda,","category":"page"},{"location":"localization/","page":"Localization and SEC","title":"Localization and SEC","text":"tag2 mathrmrank(mathrmCov(theta mathcalG) odot Lambda) geq mathrmrank(mathrmCov(theta mathcalG))","category":"page"},{"location":"localization/","page":"Localization and SEC","title":"Localization and SEC","text":"Substituting the covariance mathrmCov(theta mathcalG) by the boosted version defined in the left-hand side of equation (2), EKI is able to break the subspace property and explore additional dimensions in parameter space. Localization is an empirical way of correcting for the sampling error due to a small ensemble size, and so it can also be interpreted as a sampling error correction (SEC) method.","category":"page"},{"location":"localization/#Localization-in-EnsembleKalmanProcesses","page":"Localization and SEC","title":"Localization in EnsembleKalmanProcesses","text":"","category":"section"},{"location":"localization/","page":"Localization and SEC","title":"Localization and SEC","text":"A wide variety of localization kernels are available in EnsembleKalmanProcesses.jl under the module Localizers. The optimal localization kernel will depend on the structure of the problem at hand, so the user is encouraged to try different kernels and review their references in the literature.","category":"page"},{"location":"localization/","page":"Localization and SEC","title":"Localization and SEC","text":"In practice, the localization method is chosen at EnsembleKalmanProcess construction time,","category":"page"},{"location":"localization/","page":"Localization and SEC","title":"Localization and SEC","text":"using Distributions\nusing LinearAlgebra\nusing EnsembleKalmanProcesses\nusing EnsembleKalmanProcesses.ParameterDistributions\nusing EnsembleKalmanProcesses.Localizers\nconst EKP = EnsembleKalmanProcesses\n\np = 10; d = 20; J = 6\n\n# Construct initial ensemble\npriors = ParameterDistribution[]\nfor i in 1:p\n   push!(priors, ParameterDistribution(Parameterized(Normal(0.0, 0.5)), no_constraint(), string(\"u\", i)))\nend\nprior = combine_distributions(priors)\ninitial_ensemble = EKP.construct_initial_ensemble(prior, J)\n\ny = 10.0 * rand(d)\nΓ = 1.0 * I\n\n# Construct EKP object with localization. Some examples of localization methods:\nlocs = [Delta(), RBF(1.0), RBF(0.1), BernoulliDropout(0.1), SEC(10.0), SECFisher(), SEC(1.0, 0.1)]\nfor loc in locs\n   ekiobj = EKP.EnsembleKalmanProcess(initial_ensemble, y, Γ, Inversion(); localization_method = loc)\nend","category":"page"},{"location":"ensemble_kalman_inversion/#Ensemble-Kalman-Inversion","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"","category":"section"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"One of the ensemble Kalman processes implemented in EnsembleKalmanProcesses.jl is the ensemble Kalman inversion (Iglesias et al, 2013). The ensemble Kalman inversion (EKI) is a derivative-free ensemble optimization method that seeks to find the optimal parameters theta in mathbbR^p in the inverse problem defined by the data-model relation","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"tag1 y = mathcalG(theta) + eta ","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"where mathcalG denotes the forward map, y in mathbbR^d is the vector of observations and eta  in mathbbR^d is additive noise. Note that p is the size of the parameter vector theta and d the size of the observation vector y. Here, we take eta sim mathcalN(0 Gamma_y) from a d-dimensional Gaussian with zero mean and covariance matrix Gamma_y.  This noise structure aims to represent the correlations between observations.","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"The optimal parameters theta^* in mathbbR^p given relation (1) minimize the loss","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"mathcalL(theta y) = langle mathcalG(theta) - y    Gamma_y^-1 left ( mathcalG(theta) - y right ) rangle","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"which can be interpreted as the negative log-likelihood given a Gaussian likelihood.","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"Denoting the parameter vector of the j-th ensemble member at the n-th iteration as theta^(j)_n, its update equation from n to n+1 under EKI is","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"tag2 theta_n+1^(j) = theta_n^(j) - dfracDelta t_nJsum_k=1^J left langle mathcalG(theta_n^(k)) - barmathcalG_n    Gamma_y^-1 left ( mathcalG(theta_n^(j)) - y right ) right rangle theta_n^(k) ","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"where the subscript n=1 dots N_rm it indicates the iteration, J is the number of members in the ensemble, barmathcalG_n is the mean value of mathcalG(theta_n) across ensemble members,","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"barmathcalG_n = dfrac1Jsum_k=1^JmathcalG(theta_n^(k)) ","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"and angle brackets denote the Euclidean inner product. By multiplying with Gamma_y^-1 we render the inner product non-dimensional.","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"The EKI algorithm is considered converged when the ensemble achieves sufficient consensus/collapse in parameter space. The final estimate bartheta_N_rm it is taken to be the ensemble mean at the final iteration,","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"bartheta_N_rm it = dfrac1Jsum_k=1^Jtheta_N_rm it^(k)","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"For typical applications, a near-optimal solution theta can be found after as few as 10 iterations of the algorithm, or 10cdot J evaluations of the forward model mathcalG. The basic algorithm requires J geq p, and better performance is often seen with larger ensembles; a good rule of thumb is to start with J=10p. The algorithm also extends to J  p , using localizers to maintain performance in these situations (see the Localizers.jl module).","category":"page"},{"location":"ensemble_kalman_inversion/#Constructing-the-Forward-Map","page":"Ensemble Kalman Inversion","title":"Constructing the Forward Map","text":"","category":"section"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"The forward map mathcalG maps the space of unconstrained parameters theta in mathbbR^p to the space of outputs y in mathbbR^d. In practice, the user may not have access to such a map directly. Consider a situation where the goal is to learn a set of parameters phi of a dynamical model Psi mathbbR^p rightarrow mathbbR^o, given observations y in mathbbR^d and a set of constraints on the value of phi. Then, the forward map may be constructed as","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"mathcalG = mathcalH circ Psi circ mathcalT^-1","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"where mathcalH mathbbR^o rightarrow mathbbR^d is the observation map and mathcalT is the transformation map from constrained to unconstrained parameter spaces, such that mathcalT(phi) = theta. A family of standard transformation maps and their inverse are available in the ParameterDistributions module.","category":"page"},{"location":"ensemble_kalman_inversion/#Creating-the-EKI-Object","page":"Ensemble Kalman Inversion","title":"Creating the EKI Object","text":"","category":"section"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"An ensemble Kalman inversion object can be created using the EnsembleKalmanProcess constructor by specifying the Inversion() process type.","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"Creating an ensemble Kalman inversion object requires as arguments:","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"An initial parameter ensemble, Array{Float, 2} of size [p × J];\nThe mean value of the observed outputs, a vector of size [d];\nThe covariance of the observational noise, a matrix of size [d × d];\nThe Inversion() process type.","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"A typical initialization of the Inversion() process takes a user-defined prior, a summary of the observation statistics given by the mean y and covariance obs_noise_cov, and a desired number of members in the ensemble,","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"using EnsembleKalmanProcesses\nusing EnsembleKalmanProcesses.ParameterDistributions\n\nJ = 50  # number of ensemble members\ninitial_ensemble = construct_initial_ensemble(prior, J) # Initialize ensemble from prior\n\nekiobj = EnsembleKalmanProcess(initial_ensemble, y, obs_noise_cov, Inversion())","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"See the Prior distributions section to learn about the construction of priors in EnsembleKalmanProcesses.jl. The prior is assumed to be over the unconstrained parameter space where theta is defined. For applications where enforcing parameter bounds is necessary, the ParameterDistributions module provides functions to map from constrained to unconstrained space and vice versa. ","category":"page"},{"location":"ensemble_kalman_inversion/#Updating-the-Ensemble","page":"Ensemble Kalman Inversion","title":"Updating the Ensemble","text":"","category":"section"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"Once the ensemble Kalman inversion object ekiobj has been initialized, any number of updates can be performed using the inversion algorithm.","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"A call to the inversion algorithm can be performed with the update_ensemble! function. This function takes as arguments the ekiobj and the evaluations of the forward map at each member of the current ensemble. The update_ensemble! function then stores the new updated ensemble and the inputted forward map evaluations in ekiobj. ","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"A typical use of the update_ensemble! function given the ensemble Kalman inversion object ekiobj, the dynamical model Ψ and the observation map H is","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"# Given:\n# Ψ (some black box simulator)\n# H (some observation of the simulator output)\n# prior (prior distribution and parameter constraints)\n\nN_iter = 20 # Number of steps of the algorithm\n\nfor n in 1:N_iter\n    ϕ_n = get_ϕ_final(prior, ekiobj) # Get current ensemble in constrained \"ϕ\"-space\n    G_n = [H(Ψ(ϕ_n[:, i])) for i in 1:J]\n    g_ens = hcat(G_n...) # Evaluate forward map \n    update_ensemble!(ekiobj, g_ens) # Update ensemble\nend","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"In the previous update, note that the parameters stored in ekiobj are given in the unconstrained Gaussian space where the EKI algorithm is performed. The map mathcalT^-1 between this unconstrained space and the (possibly constrained) physical space of parameters is encoded in the prior object. The dynamical model Ψ accepts as inputs the parameters in (possibly constrained) physical space, so it is necessary to use the getter get_ϕ_final which applies transform_unconstrained_to_constrained to the ensemble. See the Prior distributions section for more details on parameter transformations.   ","category":"page"},{"location":"ensemble_kalman_inversion/#Solution","page":"Ensemble Kalman Inversion","title":"Solution","text":"","category":"section"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"The EKI algorithm drives the initial ensemble, sampled from the prior, towards the support region of the posterior distribution. The algorithm also drives the ensemble members towards consensus. The optimal parameter θ_optim found by the algorithm is given by the mean of the last ensemble (i.e., the ensemble after the last iteration),","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"θ_optim = get_u_mean_final(ekiobj) # optimal parameter","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"To obtain the optimal value in the constrained space, we use the getter with the constrained prior as input","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"ϕ_optim = get_ϕ_mean_final(prior, ekiobj) # the optimal physical parameter value","category":"page"},{"location":"ensemble_kalman_inversion/#Handling-forward-model-failures","page":"Ensemble Kalman Inversion","title":"Handling forward model failures","text":"","category":"section"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"In situations where the forward model mathcalG represents a diagnostic of a complex computational model, there might be cases where for some parameter combinations theta, attempting to evaluate mathcalG(theta) may result in model failure (defined as returning a NaN from the point of view of this package). In such cases, the EKI update equation (2) must be modified to handle model failures.","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"EnsembleKalmanProcesses.jl implements such modifications through the FailureHandler structure, an input to the EnsembleKalmanProcess constructor. Currently, the only failsafe modification available is SampleSuccGauss(), described in Lopez-Gomez et al (2022).","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"To employ this modification, construct the EKI object as","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"using EnsembleKalmanProcesses\nusing EnsembleKalmanProcesses.ParameterDistributions\n\nJ = 50  # number of ensemble members\ninitial_ensemble = construct_initial_ensemble(prior, J) # Initialize ensemble from prior\n\nekiobj = EnsembleKalmanProcess(\n    initial_ensemble,\n    y,\n    obs_noise_cov,\n    Inversion(),\n    failure_handler_method = SampleSuccGauss())","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"info: Forward model requirements when using FailureHandlers\nThe user must determine if a model run has \"failed\", and replace the output mathcalG(theta) with NaN. The FailureHandler takes care of the rest.","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"A description of the algorithmic modification is included below.","category":"page"},{"location":"ensemble_kalman_inversion/#SampleSuccGauss()","page":"Ensemble Kalman Inversion","title":"SampleSuccGauss()","text":"","category":"section"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"The SampleSuccGauss() modification is based on updating all ensemble members with a distribution given by only the successful parameter ensemble. Let Theta_sn= theta^(1)_sndotstheta^(J_s)_sn be the successful ensemble, for which each evaluation mathcalG(theta^(j)_sn) does not fail, and let theta_fn^(k) be the ensemble members for which the evaluation mathcalG(theta^(k)_fn) fails. The successful ensemble Theta_sn is updated to Theta_sn+1 using expression (2), and each failed ensemble member as","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"    theta_fn+1^(k) sim mathcalN left(m_s n+1 Sigma_s n+1 right)","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"where","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"    m_s n+1 = dfrac1J_ssum_j=1^J_s theta_sn+1^(j) qquad Sigma_s n+1 = mathrmCov(theta_s n+1 theta_s n+1) + kappa_*^-1mu_s1I_p","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"Here, kappa_* is a limiting condition number, mu_s1 is the largest eigenvalue of the sample covariance mathrmCov(theta_s n+1 theta_s n+1) and I_p is the identity matrix of size ptimes p.","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"warning: Warning\nThis modification is not a magic bullet. If large fractions of ensemble members fail during an iteration, this will degenerate the span of the ensemble.","category":"page"},{"location":"ensemble_kalman_inversion/#Sparsity-Inducing-Ensemble-Kalman-Inversion","page":"Ensemble Kalman Inversion","title":"Sparsity-Inducing Ensemble Kalman Inversion","text":"","category":"section"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"We include Sparsity-inducing Ensemble Kalman Inversion (SEKI) to add approximate L^0 and L^1 penalization to the EKI (Schneider, Stuart, Wu, 2020).","category":"page"},{"location":"ensemble_kalman_inversion/","page":"Ensemble Kalman Inversion","title":"Ensemble Kalman Inversion","text":"warning: Warning\nThe algorithm suffers from robustness issues, and therefore we urge caution in using the tool","category":"page"},{"location":"API/Localizers/#Localizers","page":"Localizers","title":"Localizers","text":"","category":"section"},{"location":"API/Localizers/","page":"Localizers","title":"Localizers","text":"CurrentModule = EnsembleKalmanProcesses.Localizers","category":"page"},{"location":"API/Localizers/","page":"Localizers","title":"Localizers","text":"Localizer\nRBF\nBernoulliDropout\nSEC\nSECFisher\nDelta\nNoLocalization","category":"page"},{"location":"API/Localizers/#EnsembleKalmanProcesses.Localizers.Localizer","page":"Localizers","title":"EnsembleKalmanProcesses.Localizers.Localizer","text":"Localizer{LM <: LocalizationMethod, T}\n\nStructure that defines a localize function, based on a localization method.\n\nFields\n\nlocalize::Function\nLocalizing function of the form: cov -> kernel .* cov\n\nConstructors\n\nLocalizer(localization, p, d, J)\nLocalizer(localization, p, d, J, T)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/Localizers.jl:122.\n\nLocalizer(localization, p, d, J)\nLocalizer(localization, p, d, J, T)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/Localizers.jl:128.\n\nLocalizer(localization, p, d, J)\nLocalizer(localization, p, d, J, T)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/Localizers.jl:134.\n\nLocalizer(localization, p, d, J)\nLocalizer(localization, p, d, J, T)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/Localizers.jl:179.\n\nLocalizer(localization, p, d, J)\nLocalizer(localization, p, d, J, T)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/Localizers.jl:199.\n\nLocalizer(localization, p, d, J)\nLocalizer(localization, p, d, J, T)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/Localizers.jl:238.\n\n\n\n\n\n","category":"type"},{"location":"API/Localizers/#EnsembleKalmanProcesses.Localizers.RBF","page":"Localizers","title":"EnsembleKalmanProcesses.Localizers.RBF","text":"RBF{FT <: Real} <: LocalizationMethod\n\nRadial basis function localization method. Covariance terms C_ij are damped through multiplication with a centered Gaussian with standardized deviation d(ij)= vert i-j vert  l.\n\nFields\n\nlengthscale::Real\nLength scale defining the RBF kernel\n\n\n\n\n\n","category":"type"},{"location":"API/Localizers/#EnsembleKalmanProcesses.Localizers.BernoulliDropout","page":"Localizers","title":"EnsembleKalmanProcesses.Localizers.BernoulliDropout","text":"BernoulliDropout{FT <: Real} <: LocalizationMethod\n\nLocalization method that drops cross-covariance terms with probability 1-p, retaining a Hermitian structure.\n\nFields\n\nprob::Real\nProbability of keeping a given cross-covariance term\n\n\n\n\n\n","category":"type"},{"location":"API/Localizers/#EnsembleKalmanProcesses.Localizers.SEC","page":"Localizers","title":"EnsembleKalmanProcesses.Localizers.SEC","text":"SEC{FT <: Real} <: LocalizationMethod\n\nSampling error correction that shrinks correlations by a factor of vert r vert ^alpha, as per Lee (2021). Sparsity of the resulting correlations can be imposed through the parameter r_0.\n\nLee, Y. (2021). Sampling error correction in ensemble Kalman inversion. arXiv:2105.11341 [cs, math]. http://arxiv.org/abs/2105.11341\n\nFields\n\nα::Real\nControls degree of sampling error correction\nr_0::Real\nCutoff correlation\n\n\n\n\n\n","category":"type"},{"location":"API/Localizers/#EnsembleKalmanProcesses.Localizers.SECFisher","page":"Localizers","title":"EnsembleKalmanProcesses.Localizers.SECFisher","text":"SECFisher <: LocalizationMethod\n\nSampling error correction for EKI, as per Lee (2021), but using the method from Flowerdew (2015) based on the Fisher transformation. Correlations are shrinked by a factor determined by the sample correlation and the ensemble size. \n\nFlowerdew, J. (2015). Towards a theory of optimal localisation. Tellus A: Dynamic Meteorology and Oceanography, 67(1), 25257. https://doi.org/10.3402/tellusa.v67.25257\n\nLee, Y. (2021). Sampling error correction in ensemble Kalman inversion. arXiv:2105.11341 [cs, math]. http://arxiv.org/abs/2105.11341\n\n\n\n\n\n","category":"type"},{"location":"API/Localizers/#EnsembleKalmanProcesses.Localizers.Delta","page":"Localizers","title":"EnsembleKalmanProcesses.Localizers.Delta","text":"Dirac delta localization method, with an identity matrix as the kernel.\n\n\n\n\n\n","category":"type"},{"location":"API/Localizers/#EnsembleKalmanProcesses.Localizers.NoLocalization","page":"Localizers","title":"EnsembleKalmanProcesses.Localizers.NoLocalization","text":"Idempotent localization method.\n\n\n\n\n\n","category":"type"},{"location":"API/ParameterDistributions/#ParameterDistributions","page":"ParameterDistributions","title":"ParameterDistributions","text":"","category":"section"},{"location":"API/ParameterDistributions/","page":"ParameterDistributions","title":"ParameterDistributions","text":"CurrentModule = EnsembleKalmanProcesses.ParameterDistributions","category":"page"},{"location":"API/ParameterDistributions/#ParameterDistributionTypes","page":"ParameterDistributions","title":"ParameterDistributionTypes","text":"","category":"section"},{"location":"API/ParameterDistributions/","page":"ParameterDistributions","title":"ParameterDistributions","text":"Parameterized\nSamples\nVectorOfParameterized","category":"page"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.Parameterized","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.Parameterized","text":"Parameterized <: ParameterDistributionType\n\nA distribution constructed from a parameterized formula (e.g Julia Distributions.jl)\n\nFields\n\ndistribution::Distributions.Distribution\nA parameterized distribution\n\n\n\n\n\n","category":"type"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.Samples","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.Samples","text":"Samples{FT <: Real} <: ParameterDistributionType\n\nA distribution comprised of only samples, stored as columns of parameters.\n\nFields\n\ndistribution_samples::AbstractMatrix{FT} where FT<:Real\nSamples defining an empirical distribution, stored as columns\n\n\n\n\n\n","category":"type"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.VectorOfParameterized","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.VectorOfParameterized","text":"VectorOfParameterized <: ParameterDistributionType\n\nA distribution built from an array of Parametrized distributions. A utility to help stacking of distributions where a multivariate equivalent doesn't exist.\n\nFields\n\ndistribution::AbstractVector{DT} where DT<:Distributions.Distribution\nA vector of parameterized distributions\n\n\n\n\n\n","category":"type"},{"location":"API/ParameterDistributions/#Constraints","page":"ParameterDistributions","title":"Constraints","text":"","category":"section"},{"location":"API/ParameterDistributions/","page":"ParameterDistributions","title":"ParameterDistributions","text":"Constraint\nno_constraint\nbounded_below\nbounded_above\nbounded\nlength(c::CType) where {CType <: ConstraintType}\nsize(c::CType) where {CType <: ConstraintType}","category":"page"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.Constraint","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.Constraint","text":"Constraint{T} <: ConstraintType\n\nClass describing a 1D bijection between constrained and unconstrained spaces. Included parametric types for T:\n\nNoConstraint\nBoundedBelow\nBoundedAbove\nBounded\n\nFields\n\nconstrained_to_unconstrained::Function\nA map from constrained domain -> (-Inf,Inf)\nc_to_u_jacobian::Function\nThe jacobian of the map from constrained domain -> (-Inf,Inf)\nunconstrained_to_constrained::Function\nMap from (-Inf,Inf) -> constrained domain\nbounds::Union{Nothing, Dict}\nDictionary of values used to build the Constraint (e.g. \"lowerbound\" or \"upperbound\")\n\n\n\n\n\n","category":"type"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.no_constraint","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.no_constraint","text":"no_constraint()\n\nConstructs a Constraint with no constraints, enforced by maps x -> x and x -> x.\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.bounded_below","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.bounded_below","text":"bounded_below(lower_bound::FT) where {FT <: Real}\n\nConstructs a Constraint with provided lower bound, enforced by maps x -> log(x - lower_bound) and x -> exp(x) + lower_bound.\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.bounded_above","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.bounded_above","text":"bounded_above(upper_bound::FT) where {FT <: Real}\n\nConstructs a Constraint with provided upper bound, enforced by maps x -> log(upper_bound - x) and x -> upper_bound - exp(x).\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.bounded","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.bounded","text":"bounded(lower_bound::Real, upper_bound::Real)\n\nConstructs a Constraint with provided upper and lower bounds, enforced by maps x -> log((x - lower_bound) / (upper_bound - x)) and x -> (upper_bound * exp(x) + lower_bound) / (exp(x) + 1).\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#Base.length-Tuple{CType} where CType<:EnsembleKalmanProcesses.ParameterDistributions.ConstraintType","page":"ParameterDistributions","title":"Base.length","text":"length(c<:ConstraintType)\n\nA constraint has length 1. \n\n\n\n\n\n","category":"method"},{"location":"API/ParameterDistributions/#Base.size-Tuple{CType} where CType<:EnsembleKalmanProcesses.ParameterDistributions.ConstraintType","page":"ParameterDistributions","title":"Base.size","text":"size(c<:ConstraintType)\n\nA constraint has size 1.\n\n\n\n\n\n","category":"method"},{"location":"API/ParameterDistributions/#ParameterDistributions-2","page":"ParameterDistributions","title":"ParameterDistributions","text":"","category":"section"},{"location":"API/ParameterDistributions/","page":"ParameterDistributions","title":"ParameterDistributions","text":"ParameterDistribution\nconstrained_gaussian\nn_samples\nget_name\nget_dimensions\nget_n_samples\nget_all_constraints(::ParameterDistribution)\nget_constraint_type\nget_bounds\nbatch\nget_distribution\nsample\nlogpdf\nmean\nvar\ncov\ntransform_constrained_to_unconstrained(::ParameterDistribution, ::AbstractVector)\ntransform_constrained_to_unconstrained(::ParameterDistribution, ::AbstractMatrix)\ntransform_constrained_to_unconstrained(::ParameterDistribution, ::Dict)\ntransform_unconstrained_to_constrained(::ParameterDistribution, ::AbstractVector)\ntransform_unconstrained_to_constrained(::ParameterDistribution, ::AbstractMatrix)\ntransform_unconstrained_to_constrained(::ParameterDistribution, ::Dict)","category":"page"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution","text":"ParameterDistribution\n\nStructure to hold a parameter distribution, always stored as an array of distributions internally.\n\nFields\n\ndistribution::AbstractVector{PDType} where PDType<:EnsembleKalmanProcesses.ParameterDistributions.ParameterDistributionType\nVector of parameter distributions, defined in unconstrained space\nconstraint::AbstractVector{CType} where CType<:EnsembleKalmanProcesses.ParameterDistributions.ConstraintType\nVector of constraints defining transformations between constrained and unconstrained space\nname::AbstractVector{ST} where ST<:AbstractString\nVector of parameter names\n\nConstructors\n\nParameterDistribution(distribution, constraint, name)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/ParameterDistributions.jl:289.\n\nParameterDistribution(param_dist_dict)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/ParameterDistributions.jl:305.\n\nParameterDistribution(distribution, constraint, name)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/ParameterDistributions.jl:389.\n\nParameterDistribution(distribution_samples, constraint, name)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/ParameterDistributions.jl:431.\n\n\n\n\n\n","category":"type"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.constrained_gaussian","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.constrained_gaussian","text":"constrained_gaussian(\n    name::AbstractString,\n    μ_c::Real,\n    σ_c::Real,\n    lower_bound::Real,\n    upper_bound::Real;\n    repeats = 1,\n    optim_algorithm::Optim.AbstractOptimizer = NelderMead(),\n    optim_kwargs...,\n)\n\nConstructor for a 1D ParameterDistribution consisting of a transformed Gaussian, constrained to have support on [lower_bound, upper_bound], with first two moments μ_c and σ_c^2. The  moment integrals can't be done in closed form, so we set the parameters of the distribution with numerical optimization.\n\nnote: Note\nThe intended use case is defining priors set from user expertise for use in inference  with adequate data, so for the sake of performance we only require that the optimization reproduce μ_c, σ_c to a loose tolerance (1e-5). Warnings are logged when the optimization fails.\n\nnote: Note\nThe distribution may be bimodal for σ_c large relative to the width of the bound interval. In extreme cases the distribution becomes concentrated at the bound endpoints. We regard this as a feature, not a bug, and do not warn the user when bimodality occurs.\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.n_samples","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.n_samples","text":"n_samples(d<:Samples)\n\nThe number of samples in the array.\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.get_name","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.get_name","text":"get_name(pd::ParameterDistribution)\n\nReturns a list of ParameterDistribution names.\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.get_dimensions","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.get_dimensions","text":"get_dimensions(pd::ParameterDistribution; function_parameter_opt = \"dof\")\n\nThe number of dimensions of the parameter space. (Also represents other dimensions of interest for FunctionParameterDistributionTypes with keyword argument)\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.get_n_samples","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.get_n_samples","text":"get_n_samples(pd::ParameterDistribution)\n\nThe number of samples in a Samples distribution\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.get_all_constraints-Tuple{EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution}","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.get_all_constraints","text":"get_all_constraints(pd::ParameterDistribution; return_dict = false)\n\nReturns the (flattened) array of constraints of the parameter distribution. or as a dictionary (\"param_name\" => constraints)\n\n\n\n\n\n","category":"method"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.get_constraint_type","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.get_constraint_type","text":"get_bounds(c::Constraint{T})\n\nGets the parametric type T.\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.get_bounds","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.get_bounds","text":"get_bounds(c::Constraint)\n\nGets the bounds field from the Constraint.\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.batch","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.batch","text":"batch(pd::ParameterDistribution; function_parameter_opt = \"dof\")\n\nReturns a list of contiguous [collect(1:i), collect(i+1:j),... ] used to split parameter arrays by distribution dimensions. function_parameter_opt is passed to ndims in the special case of FunctionParameterDistributionTypes.\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.get_distribution","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.get_distribution","text":"get_distribution(pd::ParameterDistribution)\n\nReturns a Dict of ParameterDistribution distributions, with the parameter names as dictionary keys. For parameters represented by Samples, the samples are returned as a 2D (parameter_dimension x n_samples) array.\n\n\n\n\n\ngets the, distribution over the coefficients\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#StatsBase.sample","page":"ParameterDistributions","title":"StatsBase.sample","text":"sample([rng], pd::ParameterDistribution, [n_draws])\n\nDraws n_draws samples from the parameter distributions pd. Returns an array, with  parameters as columns. rng is optional and defaults to Random.GLOBAL_RNG. n_draws is  optional and defaults to 1. Performed in computational space.\n\n\n\n\n\nsample([rng], d::Samples, [n_draws])\n\nDraws n_draws samples from the parameter distributions d. Returns an array, with  parameters as columns. rng is optional and defaults to Random.GLOBAL_RNG. n_draws is  optional and defaults to 1. Performed in computational space.\n\n\n\n\n\nsample([rng], d::Parameterized, [n_draws])\n\nDraws n_draws samples from the parameter distributions d. Returns an array, with  parameters as columns. rng is optional and defaults to Random.GLOBAL_RNG. n_draws is  optional and defaults to 1. Performed in computational space.\n\n\n\n\n\nsample([rng], d::VectorOfParameterized, [n_draws])\n\nDraws n_draws samples from the parameter distributions d. Returns an array, with  parameters as columns. rng is optional and defaults to Random.GLOBAL_RNG. n_draws is  optional and defaults to 1. Performed in computational space.\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#Distributions.logpdf","page":"ParameterDistributions","title":"Distributions.logpdf","text":"logpdf(pd::ParameterDistribution, xarray::Array{<:Real,1})\n\nObtains the independent logpdfs of the parameter distributions at xarray (non-Samples Distributions only), and returns their sum.\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#Statistics.mean","page":"ParameterDistributions","title":"Statistics.mean","text":"mean(pd::ParameterDistribution)\n\nReturns a concatenated mean of the parameter distributions. \n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#Statistics.var","page":"ParameterDistributions","title":"Statistics.var","text":"var(pd::ParameterDistribution)\n\nReturns a flattened variance of the distributions\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#Statistics.cov","page":"ParameterDistributions","title":"Statistics.cov","text":"cov(pd::ParameterDistribution)\n\nReturns a dense blocked (co)variance of the distributions.\n\n\n\n\n\n","category":"function"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.transform_constrained_to_unconstrained-Tuple{EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution, AbstractVector}","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.transform_constrained_to_unconstrained","text":"transform_constrained_to_unconstrained(pd::ParameterDistribution, x::Array{Array{<:Real,2},1})\n\nApply the transformation to map parameter sample ensembles x from the (possibly) constrained space into unconstrained space. Here, x is an iterable of parameters sample ensembles for different EKP iterations.\n\n\n\n\n\n","category":"method"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.transform_constrained_to_unconstrained-Tuple{EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution, AbstractMatrix}","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.transform_constrained_to_unconstrained","text":"transform_constrained_to_unconstrained(pd::ParameterDistribution, x::Array{Array{<:Real,2},1})\n\nApply the transformation to map parameter sample ensembles x from the (possibly) constrained space into unconstrained space. Here, x is an iterable of parameters sample ensembles for different EKP iterations.\n\n\n\n\n\n","category":"method"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.transform_constrained_to_unconstrained-Tuple{EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution, Dict}","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.transform_constrained_to_unconstrained","text":"transform_constrained_to_unconstrained(d::ParameterDistribution, x::Dict)\n\nApply the transformation to map (possibly constrained) parameter samples x into the unconstrained space. Here, x contains parameter names as keys, and 1- or 2-arrays as parameter samples.\n\n\n\n\n\n","category":"method"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.transform_unconstrained_to_constrained-Tuple{EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution, AbstractVector}","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.transform_unconstrained_to_constrained","text":"transform_unconstrained_to_constrained(pd::ParameterDistribution, x::Array{Array{<:Real,2},1})\n\nApply the transformation to map parameter sample ensembles x from the unconstrained space into (possibly constrained) space. Here, x is an iterable of parameters sample ensembles for different EKP iterations.\n\n\n\n\n\n","category":"method"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.transform_unconstrained_to_constrained-Tuple{EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution, AbstractMatrix}","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.transform_unconstrained_to_constrained","text":"transform_unconstrained_to_constrained(pd::ParameterDistribution, x::Array{Array{<:Real,2},1})\n\nApply the transformation to map parameter sample ensembles x from the unconstrained space into (possibly constrained) space. Here, x is an iterable of parameters sample ensembles for different EKP iterations.\n\n\n\n\n\n","category":"method"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.transform_unconstrained_to_constrained-Tuple{EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution, Dict}","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.transform_unconstrained_to_constrained","text":"transform_unconstrained_to_constrained(d::ParameterDistribution, x::Dict)\n\nApply the transformation to map (possibly constrained) parameter samples x into the unconstrained space. Here, x contains parameter names as keys, and 1- or 2-arrays as parameter samples.\n\n\n\n\n\n","category":"method"},{"location":"API/ParameterDistributions/#FunctionParameterDistributions","page":"ParameterDistributions","title":"FunctionParameterDistributions","text":"","category":"section"},{"location":"API/ParameterDistributions/","page":"ParameterDistributions","title":"ParameterDistributions","text":"GaussianRandomFieldsPackage\nGaussianRandomFieldInterface\nndims(grfi::GaussianRandomFieldInterface)\nget_all_constraints(grfi::GaussianRandomFieldInterface)\ntransform_constrained_to_unconstrained(::GaussianRandomFieldInterface, ::AbstractVector, ::AbstractVector{FT}) where {FT <: Real}\ntransform_constrained_to_unconstrained(::GaussianRandomFieldInterface, ::AbstractVector, ::AbstractMatrix{FT}) where {FT <: Real}\ntransform_unconstrained_to_constrained(::GaussianRandomFieldInterface, ::AbstractVector, ::AbstractVector{FT}) where {FT <: Real}\ntransform_unconstrained_to_constrained(::GaussianRandomFieldInterface, ::AbstractVector, ::AbstractMatrix{FT}) where {FT <: Real}","category":"page"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.GaussianRandomFieldsPackage","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.GaussianRandomFieldsPackage","text":"abstract type GaussianRandomFieldsPackage\n\nType to dispatch which Gaussian Random Field package to use:\n\nGRFJL uses the Julia Package GaussianRandomFields.jl \n\n\n\n\n\n","category":"type"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.GaussianRandomFieldInterface","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.GaussianRandomFieldInterface","text":"struct GaussianRandomFieldInterface <: FunctionParameterDistributionType\n\nGaussianRandomFieldInterface object based on a GRF package. Only a ND->1D output-dimension field interface is implemented.\n\nFields\n\ngaussian_random_field::Any\nGRF object, containing the mapping from the field of coefficients to the discrete function\npackage::EnsembleKalmanProcesses.ParameterDistributions.GaussianRandomFieldsPackage\nthe choice of GRF package\ndistribution::EnsembleKalmanProcesses.ParameterDistributions.ParameterDistribution\nthe distribution of the coefficients that we shall compute with\n\n\n\n\n\n","category":"type"},{"location":"API/ParameterDistributions/#Base.ndims-Tuple{EnsembleKalmanProcesses.ParameterDistributions.GaussianRandomFieldInterface}","page":"ParameterDistributions","title":"Base.ndims","text":"ndims(grfi::GaussianRandomFieldInterface, function_parameter_opt = \"dof\")\n\nProvides a relevant number of dimensions in different circumstances, If function_parameter_opt =\n\n\"dof\"       : returns n_dofs(grfi), the degrees of freedom in the function\n\"eval\"      : returns n_eval_pts(grfi), the number of discrete evaluation points of the function\n\"constraint\": returns 1, the number of constraints in the evaluation space \n\n\n\n\n\n","category":"method"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.get_all_constraints-Tuple{EnsembleKalmanProcesses.ParameterDistributions.GaussianRandomFieldInterface}","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.get_all_constraints","text":"get_all_constraints(grfi::GaussianRandomFieldInterface) = get_all_constraints(get_distribution(grfi))\n\ngets all the constraints of the internally stored coefficient prior distribution of the GRFI\n\n\n\n\n\n","category":"method"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.transform_constrained_to_unconstrained-Union{Tuple{FT}, Tuple{EnsembleKalmanProcesses.ParameterDistributions.GaussianRandomFieldInterface, AbstractVector, AbstractVector{FT}}} where FT<:Real","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.transform_constrained_to_unconstrained","text":"transform_constrained_to_unconstrained(d::GaussianRandomFieldInterface, constraint::AbstractVector, x::AbstractVector)\n\nAssume x is a flattened vector of evaluation points. Remove the constraint from constraint to the output space of the function. Note this is the inverse of transform_unconstrained_to_constrained(...,build_flag=false)\n\n\n\n\n\n","category":"method"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.transform_constrained_to_unconstrained-Union{Tuple{FT}, Tuple{EnsembleKalmanProcesses.ParameterDistributions.GaussianRandomFieldInterface, AbstractVector, AbstractMatrix{FT}}} where FT<:Real","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.transform_constrained_to_unconstrained","text":"transform_constrained_to_unconstrained(d::GaussianRandomFieldInterface, constraint::AbstractVector, x::AbstractMatrix)\n\nAssume x is a matrix with columns as flattened samples of evaluation points. Remove the constraint from constraint to the output space of the function. Note this is the inverse of transform_unconstrained_to_constrained(...,build_flag=false)\n\n\n\n\n\n","category":"method"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.transform_unconstrained_to_constrained-Union{Tuple{FT}, Tuple{EnsembleKalmanProcesses.ParameterDistributions.GaussianRandomFieldInterface, AbstractVector, AbstractVector{FT}}} where FT<:Real","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.transform_unconstrained_to_constrained","text":"transform_unconstrained_to_constrained(d::GaussianRandomFieldInterface, constraint::AbstractVector, x::AbstractVector)\n\nOptional Args build_flag::Bool = true\n\nTwo functions, depending on build_flag If true, assume x is a vector of coefficients. Perform the following 3 maps. \n\nApply the transformation to map (possibly constrained) parameter samples x into the unconstrained space. Using internally stored constraints (given by the coefficient prior)\nBuild the unconstrained (flattened) function sample at the evaluation points from these constrained coefficients.\nApply the constraint from constraint to the output space of the function.\n\nIf false, Assume x is a flattened vector of evaluation points. Apply only step 3. above to x.\n\n\n\n\n\n","category":"method"},{"location":"API/ParameterDistributions/#EnsembleKalmanProcesses.ParameterDistributions.transform_unconstrained_to_constrained-Union{Tuple{FT}, Tuple{EnsembleKalmanProcesses.ParameterDistributions.GaussianRandomFieldInterface, AbstractVector, AbstractMatrix{FT}}} where FT<:Real","page":"ParameterDistributions","title":"EnsembleKalmanProcesses.ParameterDistributions.transform_unconstrained_to_constrained","text":"transform_unconstrained_to_constrained(d::GaussianRandomFieldInterface, constraint::AbstractVector, x::AbstractMatri)\n\nOptional args: build_flag::Bool = true\n\nTwo functions, depending on build_flag If true, assume x is a matrix with columns of coefficient samples. Perform the following 3 maps. \n\nApply the transformation to map (possibly constrained) parameter samples x into the unconstrained space. Using internally stored constraints (given by the coefficient prior)\nBuild the unconstrained (flattened) function sample at the evaluation points from these constrained coefficients.\nApply the constraint from constraint to the output space of the function.\n\nIf false, Assume x is a matrix with columns as flattened samples of evaluation points. Apply only step 3. above to x.\n\n\n\n\n\n","category":"method"},{"location":"API/TOMLInterface/#TOML-interface","page":"TOML Interface","title":"TOML interface","text":"","category":"section"},{"location":"API/TOMLInterface/","page":"TOML Interface","title":"TOML Interface","text":"CurrentModule = EnsembleKalmanProcesses.TOMLInterface","category":"page"},{"location":"API/TOMLInterface/","page":"TOML Interface","title":"TOML Interface","text":"path_to_ensemble_member\nget_parameter_distribution\nget_parameter_values\nsave_parameter_ensemble\nget_admissible_parameters\nget_regularization\nwrite_log_file","category":"page"},{"location":"API/TOMLInterface/#EnsembleKalmanProcesses.TOMLInterface.path_to_ensemble_member","page":"TOML Interface","title":"EnsembleKalmanProcesses.TOMLInterface.path_to_ensemble_member","text":"path_to_ensemble_member(\n    base_path,\n    iteration,\n    member,\n    pad_zeros = 3,\n)\n\nObtains the file path to a specified ensemble member. The likely form is base_path/iteration_X/member_Y/ with X,Y padded with zeros. The file path can be reconstructed with: base_path - base path to where EKP parameters are stored member - number of the ensemble member (without zero padding) iteration - iteration of ensemble method (if =nothing then only the load path is used) pad_zeros - amount of digits to pad to\n\n\n\n\n\nOne can also call this without the iteration level\n\n\n\n\n\n","category":"function"},{"location":"API/TOMLInterface/#EnsembleKalmanProcesses.TOMLInterface.get_parameter_distribution","page":"TOML Interface","title":"EnsembleKalmanProcesses.TOMLInterface.get_parameter_distribution","text":"get_parameter_distribution(param_dict, name)\n\nConstructs a ParameterDistribution for a single parameter\n\nArgs: param_dict - nested dictionary that has parameter names as keys and the                corresponding dictionary of parameter information (in particular,                the parameters' prior distributions and constraints) as values name - parameter name\n\nReturns a ParameterDistribution\n\n\n\n\n\nget_parameter_distribution(param_dict, names)\n\nConstructs a ParameterDistribution for an array of parameters\n\nArgs: param_dict - nested dictionary that has parameter names as keys and the                corresponding dictionary of parameter information (in particular,                the parameters' prior distributions and constraints) as values names - array of parameter names\n\nReturns a ParameterDistribution \n\n\n\n\n\n","category":"function"},{"location":"API/TOMLInterface/#EnsembleKalmanProcesses.TOMLInterface.get_parameter_values","page":"TOML Interface","title":"EnsembleKalmanProcesses.TOMLInterface.get_parameter_values","text":"get_parameter_values(param_dict, names)\n\nGets parameter values from a parameter dictionary, indexing by name.\n\nArgs: param_dict - nested dictionary that has parameter names as keys and the                corresponding dictionary of parameter information (in particular,                the parameters' values) name - iterable parameter names return_type - return type, default \"dict\", otherwise \"array\"\n\n\n\n\n\n","category":"function"},{"location":"API/TOMLInterface/#EnsembleKalmanProcesses.TOMLInterface.save_parameter_ensemble","page":"TOML Interface","title":"EnsembleKalmanProcesses.TOMLInterface.save_parameter_ensemble","text":"save_parameter_ensemble(\n    param_array,\n    param_distribution,\n    default_param_data,\n    save_path,\n    save_file,\n    iteration\n    pad_zeros=3,\napply_constraints=true\n)\n\nSaves the parameters in the given param_array to TOML files. The intended use is for saving the ensemble of parameters after each update of an ensemble Kalman process. Each ensemble member (column of param_array) is saved in a separate directory \"member<j>\" (j=1, ..., Nens). The name of the saved toml file is given by save_file; it is the same for all members. A directory \"iteration<iter>\" is created in `savepath`, which contains all the \"member_<j>\" subdirectories.\n\nArgs: param_array - array of size Nparam x Nens param_distribution - the parameter distribution underlying param_array apply_constraints -  apply the constraints in param_distribution default_param_data - dict of default parameters to be combined and saved with                        the parameters in param_array into a toml file save_path - path to where the parameters will be saved save_file - name of the toml files to be generated iteration - the iteration of the ensemble Kalman process represented by the given          param_array pad_zeros - the amount of zero-padding for the ensemble member number\n\n\n\n\n\nOne can also call this without the iteration level\n\n\n\n\n\n","category":"function"},{"location":"API/TOMLInterface/#EnsembleKalmanProcesses.TOMLInterface.get_admissible_parameters","page":"TOML Interface","title":"EnsembleKalmanProcesses.TOMLInterface.get_admissible_parameters","text":"get_admissible_parameters(param_dict)\n\nFinds all parameters in param_dict that are admissible for calibration.\n\nArgs: param_dict - nested dictionary that has parameter names as keys and the                corresponding dictionaries of parameter information as values\n\nReturns an array of the names of all admissible parameters in param_dict. Admissible parameters must have a key \"prior\" and the value value of this is not set to \"fixed\". This allows for other parameters to be stored within the TOML file.\n\n\n\n\n\n","category":"function"},{"location":"API/TOMLInterface/#EnsembleKalmanProcesses.TOMLInterface.get_regularization","page":"TOML Interface","title":"EnsembleKalmanProcesses.TOMLInterface.get_regularization","text":"get_regularization(param_dict, name)\n\nReturns the regularization information for a single parameter\n\nArgs: param_dict - nested dictionary that has parameter names as keys and the                corresponding dictionary of parameter information as values name - parameter name\n\nReturns a tuple (<regularizationtype>, <regularizationvalue>), where the regularization type is either \"L1\" or \"L2\", and the regularization value is a float. Returns (nothing, nothing) if parameter has no regularization information.\n\n\n\n\n\nget_regularization(param_dict, names)\n\nReturns the regularization information for an array of parameters\n\nArgs: param_dict - nested dictionary that has parameter names as keys and the                corresponding dictionary of parameter information as values names - array of parameter names\n\nReturns an arary of tuples (<regularizationtype>, <regularizationvalue>), with the ith tuple corresponding to the parameter names[i].  The regularization type is either \"L1\" or \"L2\", and the regularization  value is a float. Returns (nothing, nothing) for parameters that have no regularization information.\n\n\n\n\n\n","category":"function"},{"location":"API/TOMLInterface/#EnsembleKalmanProcesses.TOMLInterface.write_log_file","page":"TOML Interface","title":"EnsembleKalmanProcesses.TOMLInterface.write_log_file","text":"write_log_file(param_dict, file_path)\n\nWrites the parameters in param_dict into a .toml file\n\nArgs: param_dict - nested dictionary that has parameter names as keys and the                corresponding dictionaries of parameter information as values file_path - path of the file where parameters are saved\n\n\n\n\n\n","category":"function"},{"location":"examples/ClimateMachine_example/#HPC-interfacing-example:-ClimateMachine","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"","category":"section"},{"location":"examples/ClimateMachine_example/#Overview","page":"HPC interfacing example: ClimateMachine","title":"Overview","text":"","category":"section"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"This examples uses EnsembleKalmanProcesses.jl to calibrate a climate model, showcasing a workflow which is compatible with HPC resources managed with the SLURM workload manager. The workflow is based on read-write input/output files, and as such it is capable of interfacing with dynamical models in different code languages, or with complicated processing stages. The dynamical model for this example is ClimateMachine.jl, an Earth system model currently under development at CliMA.","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"The calibration example makes use of a simple single atmospheric column model configuration with two learnable parameters that control turbulent mixing processes in the lower troposphere. It is also a perfect model experiment, in the sense that the ground truth is generated using the same model and a prescribed combination of parameters. The parameters used to generate the ground truth are (C_smag, C_drag) = (0.21, 0.0011). The evolution of the atmosphere for this setup is strongly influenced by C_smag, and very weakly by C_drag. Thus, we expect the EKP to recover C_smag from observations.","category":"page"},{"location":"examples/ClimateMachine_example/#Prerequisites","page":"HPC interfacing example: ClimateMachine","title":"Prerequisites","text":"","category":"section"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"This example requires ClimateMachine.jl to be installed in the same parent directory as EnsembleKalmanProcesses.jl. You may install ClimateMachine.jl directly from GitHub,","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"$ git clone https://github.com/CliMA/ClimateMachine.jl.git","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"Change into the ClimateMachine.jl directory with ","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"$ cd ClimateMachine.jl","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"and install all the required packages with:","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"$ julia --project -e 'using Pkg; pkg\"instantiate\";'","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"Pre-compile the packages to allow the ClimateMachine.jl to start faster:","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"$ julia --project -e 'using Pkg; pkg\"precompile\"'","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"You can find more information about ClimateMachine.jl here. ClimateMachine.jl is a rapidly evolving software and this example may stop working in the future, please open an issue if you find that to be the case!","category":"page"},{"location":"examples/ClimateMachine_example/#Structure","page":"HPC interfacing example: ClimateMachine","title":"Structure","text":"","category":"section"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"The example makes use of julia and bash scripts for interactions with the workload manager and running multiple forward model evaluations in parallel. The user-triggered script ekp_calibration.sbatch initializes and controls the flow of the calibration process, which in this case is a SLURM queue with job dependencies. The calibration bash scripts, in order of execution and with their associated julia scripts, are","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"ekp_init_calibration: Calls init_calibration.jl, which samples the initial parameter ensemble from a specified prior. The initial ensemble is stored in a set of parameter files.\nekp_single_cm_run: Script called in parallel by the workload manager. Each copy of the script submits a single forward model run (i.e., a ClimateMachine.jl run) given a specific pair of parameters (C_smag, C_drag) read from a corresponding file. The output of each forward model run is stored in a separate NetCDF file.\nekp_cont_calibration: Calls sstep_calibration.jl, which reads from a NetCDF file the output generated by ClimateMachine.jl in step 2 and performs an iteration of the Ensemble Kalman Inversion algorithm, updating the parameter ensemble. The new parameters are stored in new parameter files.","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"This flow follows steps 1->2->3->2->3->... for a user-specified number of iterations.","category":"page"},{"location":"examples/ClimateMachine_example/#Running-the-Example","page":"HPC interfacing example: ClimateMachine","title":"Running the Example","text":"","category":"section"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"From the parent directory of ClimateMachine.jl and EnsembleKalmanProcesses.jl, change into the example directory with","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"$ cd EnsembleKalmanProcesses.jl/examples/ClimateMachine","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"and install all the required packages for the example with:","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"$ julia --project -e 'using Pkg; pkg\"instantiate\";'","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"To run the example using a SLURM workload manager, simply do:","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"$ sbatch ekp_calibration.sbatch","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"The dynamical model outputs (i.e, Psi(phi)) for all runs of ClimateMachine.jl will be stored in NetCDF format in directories identifiable by their version number. Refer to the files version_XX.txt to identify each run with each ensemble member within the XX iteration of the Ensemble Kalman Process. ","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"In this example, the parameters are defined through a Gaussian prior in init_calibration.jl. Hence, the transform from constrained to unconstrained space is the identity map, and we have phi=theta. Overall the forward map mathcalG(theta) is given by applying an observation map mathcalH to the dynamical model output Psi. In this case, mathcalH returns the time average of the horizontal velocity over a specified 30 min interval after initialization. Therefore, the observation vector y contains a time-averaged vertical profile of the horizontal velocity.","category":"page"},{"location":"examples/ClimateMachine_example/#Calibration-Solution","page":"HPC interfacing example: ClimateMachine","title":"Calibration Solution","text":"","category":"section"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"To aggregate the parameter ensembles theta^(1) theta^(2) dots theta^(J) generated during the calibration process, you may use the agg_clima_ekp(...) function located in helper_funcs.jl,","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"$ julia --project","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"include(joinpath(@__DIR__, \"helper_funcs.jl\"))\n\nagg_clima_ekp(2) # This generates the output containing the ensembles for each iteration, input is the number of parameters","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"This will create the JLD file ekp_clima.jld. We may read the file as follows","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"using JLD\n\nθ = load(\"ekp_clima.jld\")[\"ekp_u\"]\nprintln(typeof(θ)) # Array{Array{Float64,2},1}, outer dimension is N_iter, inner Array{Float64,2} of size = (J, p)","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"The optimal parameter vector determined by the ensemble Kalman inversion is the ensemble mean of the particles after the last iteration. Following the previous script,","category":"page"},{"location":"examples/ClimateMachine_example/","page":"HPC interfacing example: ClimateMachine","title":"HPC interfacing example: ClimateMachine","text":"using Statistics\n\nθ_opt = mean(θ[end], dims=1)","category":"page"},{"location":"unscented_kalman_inversion/#Unscented-Kalman-Inversion","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"","category":"section"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"One of the ensemble Kalman processes implemented in EnsembleKalmanProcesses.jl is the unscented Kalman inversion (Huang, Schneider, Stuart, 2022). The unscented Kalman inversion (UKI) is a derivative-free method for approximate Bayesian inference. We seek to find the posterior parameter distribution theta in mathbbR^p from the inverse problem","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":" y = mathcalG(theta) + eta","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"where mathcalG denotes the forward map, y in mathbbR^d is the vector of observations and eta sim mathcalN(0 Gamma_y) is additive Gaussian noise. Note that p is the size of the parameter vector theta and d is taken to be the size of the observation vector y. The UKI algorithm has the following properties","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"UKI has a fixed ensemble size, with members forming a quadrature stencil (rather than the random positioning of the particles from methods such as EKI). There are two quadrature options, symmetric (a 2p + 1-size stencil), and simplex (a p+2-size stencil).\nUKI has uncertainty quantification capabilities, it gives both mean and covariance approximation (no ensemble collapse and no empirical variance inflation) of the posterior distribution, the 3-sigma confidence interval covers the truth parameters for perfect models.","category":"page"},{"location":"unscented_kalman_inversion/#Algorithm","page":"Unscented Kalman Inversion","title":"Algorithm","text":"","category":"section"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"The UKI applies the unscented Kalman filter to the following stochastic dynamical system ","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"beginaligned\n  textrmevolution    theta_n+1 = r + alpha (theta_n  - r) +  omega_n+1 omega_n+1 sim mathcalN(0Sigma_omega)\n  textrmobservation  y_n+1 = mathcalG(theta_n+1) + nu_n+1 nu_n+1 sim mathcalN(0Sigma_nu)\nendaligned","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"The free parameters in the UKI are alpha r Sigma_nu Sigma_omega. The UKI updates both the mean m_n and covariance C_n estimations of the parameter vector theta as following","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"Prediction step :","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"beginaligned\n    hatm_n+1 =  r+alpha(m_n-r)\n    hatC_n+1 =  alpha^2 C_n + Sigma_omega\nendaligned","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"Generate sigma points (\"the ensemble\") :","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"For the sigma_points = symmetric quadrature option, the ensemble is generated as follows.","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"beginaligned\n    hattheta_n+1^0 = hatm_n+1 \n    hattheta_n+1^j = hatm_n+1 + c_j sqrthatC_n+1_j quad (1leq jleq J) \n    hattheta_n+1^j+J = hatm_n+1 - c_j sqrthatC_n+1_jquad (1leq jleq J)\nendaligned","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"where sqrtC_j is the j-th column of the Cholesky factor of C. ","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"Analysis step :","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"   beginaligned\n        haty^j_n+1 = mathcalG(hattheta^j_n+1) qquad haty_n+1 = haty^0_n+1\n         hatC^theta p_n+1 = sum_j=1^2JW_j^c\n        (hattheta^j_n+1 - hatm_n+1 )(haty^j_n+1 - haty_n+1)^T \n        hatC^pp_n+1 = sum_j=1^2JW_j^c\n        (haty^j_n+1 - haty_n+1 )(haty^j_n+1 - haty_n+1)^T + Sigma_nu\n        m_n+1 = hatm_n+1 + hatC^theta p_n+1(hatC^pp_n+1)^-1(y - haty_n+1)\n        C_n+1 = hatC_n+1 - hatC^theta p_n+1(hatC^pp_n+1)^-1hatC^theta p_n+1^T\n    endaligned","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"Where the coefficients c_j W^c_j are given by","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"    beginaligned\n    c_j = asqrtJ qquad W_j^c = frac12a^2J(j=1cdots2N_theta) qquad  a=minsqrtfrac4J  1 \n    endaligned","category":"page"},{"location":"unscented_kalman_inversion/#Choice-of-free-parameters","page":"Unscented Kalman Inversion","title":"Choice of free parameters","text":"","category":"section"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"The free parameters in the unscented Kalman inversion are alpha r Sigma_nu Sigma_omega, which are chosen based on theorems developed in Huang et al, 2021","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"the vector r is set to be the prior mean\nthe scalar alpha in (01 is a regularization parameter, which is used to overcome ill-posedness and overfitting. A practical guide is \nWhen the observation noise is negligible, and there are more observations than parameters (identifiable inverse problem) alpha = 1 (no regularization)\nOtherwise alpha  1. The smaller alpha is, the closer the UKI mean will converge to the prior mean.\nthe matrix Sigma_nu is the artificial observation error covariance. We set Sigma_nu = 2 Gamma_y, which makes the inverse problem consistent. \nthe matrix Sigma_omega is the artificial evolution error covariance. We set Sigma_omega = (2 - alpha^2)Lambda. We choose Lambda as following\nwhen there are more observations than parameters (identifiable inverse problem), Lambda = C_n, which is updated as the estimated covariance C_n in the n-th every iteration. This guarantees the converged covariance matrix is a good approximation to the posterior covariance matrix with an uninformative prior.\notherwise Lambda = C_0, this allows that the converged covariance matrix is a weighted average between the posterior covariance matrix with an uninformative prior and C_0.","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"In short, users only need to change the alpha (α_reg), and the frequency to update the Lambda to the current covariance (update_freq). The user can first try α_reg = 1.0 and update_freq = 0 (corresponding to Lambda = C_0).","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"note: Preventing ensemble divergence\nIf UKI suffers divergence (for example when inverse problems are not well-posed), one can prevent it by using Tikhonov regularization (see Huang, Schneider, Stuart, 2022). It is used by setting the impose_prior = true flag. In this mode, the free parameters are fixed to α_reg = 1.0, update_freq = 1. ","category":"page"},{"location":"unscented_kalman_inversion/#Implementation","page":"Unscented Kalman Inversion","title":"Implementation","text":"","category":"section"},{"location":"unscented_kalman_inversion/#Initialization","page":"Unscented Kalman Inversion","title":"Initialization","text":"","category":"section"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"An unscented Kalman inversion object can be created using the EnsembleKalmanProcess constructor by specifying the Unscented() process type.","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"Creating an ensemble Kalman inversion object requires as arguments:","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"The mean value of the observed outputs, a vector of size [d];\nThe covariance of the observational noise, a matrix of size [d × d];\nThe Unscented() process type.","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"The initialization of the Unscented() process requires prior mean and prior covariance, and the the size of the observation d. And user defined hyperparameters  α_reg and update_freq.","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"using EnsembleKalmanProcesses\nusing EnsembleKalmanProcesses.ParameterDistributions\n\n\n# need to choose regularization factor α ∈ (0,1],  \n# when you have enough observation data α=1: no regularization\nα_reg =  1.0\n# update_freq 1 : approximate posterior covariance matrix with an uninformative prior\n#             0 : weighted average between posterior covariance matrix with an uninformative prior and prior\nupdate_freq = 0\n\nprocess = Unscented(prior_mean, prior_cov; α_reg = α_reg, update_freq = update_freq)\nukiobj = EnsembleKalmanProcess(truth_sample, truth.obs_noise_cov, process)\n","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"Note that no information about the forward map is necessary to initialize the Unscented process. The only forward map information required by the inversion process consists of model evaluations at the ensemble elements, necessary to update the ensemble.","category":"page"},{"location":"unscented_kalman_inversion/#Constructing-the-Forward-Map","page":"Unscented Kalman Inversion","title":"Constructing the Forward Map","text":"","category":"section"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"At the core of the forward map mathcalG is the dynamical model PsimathbbR^p rightarrow mathbbR^o (running Psi is usually where the computational heavy-lifting is done), but the map mathcalG may include additional components such as a transformation of the (unbounded) parameters theta to a constrained domain the dynamical model can work with, or some post-processing of the output of Psi to generate the observations. For example, mathcalG may take the following form:","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"mathcalG = mathcalH circ Psi circ mathcalT^-1","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"where mathcalHmathbbR^o rightarrow mathbbR^d is the observation map and mathcalT is the transformation from the constrained to the unconstrained parameter space, such that mathcalT(phi)=theta. A family of standard transformations and their inverses are available in the ParameterDistributions module.","category":"page"},{"location":"unscented_kalman_inversion/#Updating-the-Ensemble","page":"Unscented Kalman Inversion","title":"Updating the Ensemble","text":"","category":"section"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"Once the unscented Kalman inversion object UKIobj has been initialized, any number of updates can be performed using the inversion algorithm.","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"A call to the inversion algorithm can be performed with the update_ensemble! function. This function takes as arguments the UKIobj and the evaluations of the forward map at each element of the current ensemble. The update_ensemble! function then stores the new updated ensemble and the inputted forward map evaluations in UKIobj.","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"The forward map mathcalG maps the space of unconstrained parameters theta to the outputs y in mathbbR^d. In practice, the user may not have access to such a map directly. And the map is a composition of several functions. The update_ensemble! uses only the evalutaions g_ens but not the forward map  ","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"For implementational reasons, the update_ensemble is performed by computing analysis stage first, followed by a calculation of the next sigma ensemble. The first sigma ensemble is created in the initialization.","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"# Given:\n# Ψ (some black box simulator)\n# H (some observation of the simulator output)\n# prior (prior distribution and parameter constraints)\n\nN_iter = 20 # Number of steps of the algorithm\n \nfor n in 1:N_iter\n    ϕ_n = get_ϕ_final(prior, ukiobj) # Get current ensemble in constrained \"ϕ\"-space\n    G_n = [H(Ψ(ϕ_n[:, i])) for i in 1:J]  # Evaluate forward map\n    g_ens = hcat(G_n...)  # Reformat into `d x N_ens` matrix\n    EnsembleKalmanProcesses.update_ensemble!(ukiobj, g_ens) # Update ensemble\nend","category":"page"},{"location":"unscented_kalman_inversion/#Solution","page":"Unscented Kalman Inversion","title":"Solution","text":"","category":"section"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"The solution of the unscented Kalman inversion algorithm is a Gaussian distribution whose mean and covariance can be extracted from the ''last ensemble'' (i.e., the ensemble after the last iteration). The sample mean of the last ensemble is also the \"optimal\" parameter (θ_optim) for the given calibration problem. These statistics can be accessed as follows: ","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"# mean of the Gaussian distribution, also the optimal parameter for the calibration problem\nθ_optim = get_u_mean_final(ukiobj)\n# covariance of the Gaussian distribution\nsigma_optim = get_u_cov_final(ukiobj)","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"There are two examples: Lorenz96 and Cloudy.","category":"page"},{"location":"unscented_kalman_inversion/#Handling-forward-model-failures","page":"Unscented Kalman Inversion","title":"Handling forward model failures","text":"","category":"section"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"In situations where the forward model mathcalG represents a diagnostic of a complex computational model, there might be cases where for some parameter combinations theta, attempting to evaluate mathcalG(theta) may result in model failure (defined as returning a NaN from the point of view of this package). In such cases, the UKI update equations must be modified to handle model failures.","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"EnsembleKalmanProcesses.jl implements such modifications through the FailureHandler structure, an input to the EnsembleKalmanProcess constructor. Currently, the only failsafe modification available is SampleSuccGauss(), described in Lopez-Gomez et al (2022).","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"To employ this modification, construct the EKI object as","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"using EnsembleKalmanProcesses\nusing EnsembleKalmanProcesses.ParameterDistributions\n\n\n# need to choose regularization factor α ∈ (0,1],  \n# when you have enough observation data α=1: no regularization\nα_reg =  1.0\n# update_freq 1 : approximate posterior covariance matrix with an uninformative prior\n#             0 : weighted average between posterior covariance matrix with an uninformative prior and prior\nupdate_freq = 0\n\nprocess = Unscented(prior_mean, prior_cov; α_reg = α_reg, update_freq = update_freq)\nukiobj = EnsembleKalmanProcess(\n    truth_sample,\n    truth.obs_noise_cov,\n    process,\n    failure_handler_method = SampleSuccGauss())\n","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"info: Forward model requirements when using FailureHandlers\nThe user must determine if a model run has \"failed\", and replace the output mathcalG(theta) with NaN. The FailureHandler takes care of the rest.","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"A description of the algorithmic modification is included below.","category":"page"},{"location":"unscented_kalman_inversion/#SampleSuccGauss-modification","page":"Unscented Kalman Inversion","title":"SampleSuccGauss modification","text":"","category":"section"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"The SampleSuccGauss() modification is based on performing the UKI quadratures over the successful sigma points. Consider the set of off-center sigma points hattheta = hattheta_s cup hattheta_f where hattheta_s^(j),  j=1 dots J_s are successful members and hattheta_f^(k) are not. For ease of notation, consider an ordering of hattheta such that hattheta_s are its first J_s elements, and note that we deal with the central point hattheta^(0) separately. We estimate the covariances mathrmCov_q(mathcalG_n mathcalG_n) and mathrmCov_q(theta_n mathcalG_n) from the successful ensemble,","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"   tag1 mathrmCov_q(theta_n mathcalG_n) approx sum_j=1^J_sw_sj (hattheta_s n^(j) - bartheta_sn)(mathcalG(hattheta_s n^(j)) - barmathcalG_sn)^T","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"   tag2 mathrmCov_q(mathcalG_n mathcalG_n) approx sum_j=1^J_sw_sj (mathcalG(hattheta_s n^(j)) - barmathcalG_sn)(mathcalG(hattheta_s n^(j)) - barmathcalG_sn)^T","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"where the weights at each successful sigma point are scaled up, to preserve the sum of weights,","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"    w_sj = left(dfracsum_i=1^2p w_isum_k=1^J_s w_kright)w_j","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"In equations (1) and (2), the means bartheta_sn and barmathcalG_sn must be modified from the original formulation if the central point hattheta^(0)=m_n results in model failure. If this is the case, then an average is taken across the other (successful) ensemble members","category":"page"},{"location":"unscented_kalman_inversion/","page":"Unscented Kalman Inversion","title":"Unscented Kalman Inversion","text":"   bartheta_sn =\ndfrac1J_ssum_j=1^J_shattheta_s n^(j) qquad   barmathcalG_sn =\ndfrac1J_ssum_j=1^J_smathcalG(hattheta_s n^(j))","category":"page"},{"location":"examples/Cloudy_example/#Cloudy-example","page":"Cloudy","title":"Cloudy Example","text":"","category":"section"},{"location":"examples/Cloudy_example/#Overview","page":"Cloudy","title":"Overview","text":"","category":"section"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"This example is based on Cloudy, a microphysics model that simulates how cloud droplets collide and coalesce into larger drops. Collision-coalescence is a crucial process for the formation of rain. ","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"Cloudy is initialized with a mass distribution of the cloud droplets; this distribution is then evolved in time, with more and more droplets colliding and combining into bigger drops according to the droplet-droplet interactions specified by a collision-coalescence kernel. The evolution is completely determined by the shape of the initial distribution and the form of the kernel.","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"This example shows how ensemble Kalman methods can be used to learn the parameters of the initial cloud droplet mass distribution from observations of the moments of that mass distribution at a later time. The collision-coalescence kernel is assumed to be known, but one could also learn the parameters of the kernel instead of the parameters of the droplet distribution (or both).","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"Cloudy is used here in a \"perfect model\" (aka \"known truth\") setting, which means that the \"observations\" are generated by Cloudy itself, by running it with the true parameter values. In more realistic applications, this parameter estimation procedure will use actual measurements of cloud properties to obtain an estimated droplet mass distribution at a previous time.","category":"page"},{"location":"examples/Cloudy_example/#Prerequisites","page":"Cloudy","title":"Prerequisites","text":"","category":"section"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"In order to run this example, you need to install Cloudy.jl (the \"#master\" lets you install the current master branch):","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"pkg > add Cloudy#master","category":"page"},{"location":"examples/Cloudy_example/#Structure","page":"Cloudy","title":"Structure","text":"","category":"section"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"The file Cloudy_example_eki.jl sets up the inverse problem and solves it using ensemble Kalman inversion, and the file Cloudy_example_uki.jl does the same using unscented Kalman inversion. The file DynamicalModel.jl provides the functionality to run the dynamical model Psi, which in this example is Cloudy.","category":"page"},{"location":"examples/Cloudy_example/#Running-the-Example","page":"Cloudy","title":"Running the Example","text":"","category":"section"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"Once Cloudy is installed, the examples can be run from the julia REPL:","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"# Solve inverse problem using ensemble Kalman inversion\ninclude(\"Cloudy_example_eki.jl\")","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"or","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"# Solve inverse problem using unscented Kalman inversion\ninclude(\"Cloudy_example_uki.jl\")","category":"page"},{"location":"examples/Cloudy_example/#What-Does-Cloudy-Do?","page":"Cloudy","title":"What Does Cloudy Do?","text":"","category":"section"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"The mathematical starting point of Cloudy is the stochastic collection equation (SCE; sometimes also called Smoluchowski equation after Marian Smoluchowski), which describes the time rate of change of f = f(m t), the mass distribution function of liquid water droplets, due to the process of collision and coalescence. The distribution function f depends on droplet mass m and time t and is defined such that f(m) text dm denotes the number of droplets with masses in the interval m m + dm per unit volume. ","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"The stochastic collection equation is an integro-differential equation that can be written as ","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"    fracpartial f(m t)partial t = frac12 int_m=0^infty f(m t) f(m-m t)  mathcalC(m m-m)textdm - f(m t) int_m=0^infty f(m t)mathcalC(m m) textdm ","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"where mathcalC(m m) is the collision-coalescence kernel, which  encapsulates the physics of droplet-droplet interactions – it describes the rate at which two drops of masses m and m come into contact and coalesce into a drop of mass m + m. The first term on the right-hand side of the SCE describes the rate of increase of the number of drops having a mass m due to collision and coalescence of drops of masses m and m-m (where the factor frac12 avoids double counting), while the second term describes the rate of reduction of drops of mass m due to collision and coalescence of drops having a mass m with other drops. ","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"We can rewrite the SCE in terms of the moments M_k of f, which are the prognostic variables in Cloudy. They are defined by","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"    M_k = int_0^infty m^k f(m t) textdm","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"The time rate of change of the k-th moment of f is obtained by multiplying the SCE by m^k and integrating over the entire range of droplet masses (from m=0 to infty), which yields","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"    fracpartial M_k(t)partial t = frac12int_0^infty left((m+m)^k - m^k - m^kright) mathcalC(m m)f(m t)f(m t)  textdm textdm  (1)","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"In this example, the kernel is set to be constant – mathcalC(m m) = B = textconst – and the cloud droplet mass distribution is assumed to be a textGamma(k_t theta_t) distribution, scaled by a factor N_0t which denotes the droplet number concentration:","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"f(m t) = fracN_0tGamma(k_t)theta_t^k m^k_t-1 exp(-mtheta_t)","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"The parameter vector phi_t= N_0t k_t theta_t changes over time (as indicated by the subscript t), as the shape of the distribution evolves. In fact, there is a priori no reason to assume that the distribution would retain its Gamma shape over time, but this is a common assumption that is made in order to solve the closure problem (without this assumption, one would have to keep track of infinitely many moments of the mass distribution in order to uniquely identify the distribution f at each time step, which is obviously not practicable).","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"For Gamma mass distribution functions, specifying the first three moments (M_0, M_1, and M_2) is sufficient to uniquely determine the parameter vector phi_t, hence Cloudy solves equation (1) for k = 0 1 2. This mapping of the parameters of the initial cloud droplet mass distribution to the (zeroth-, first-, and second-order) moments of the distribution at a specified end time is done by DynamicalModel.jl.","category":"page"},{"location":"examples/Cloudy_example/#Setting-up-the-Inverse-Problem","page":"Cloudy","title":"Setting up the Inverse Problem","text":"","category":"section"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"The goal is to learn the distribution  parameters at time t = 0, phi_0 = N_00 k_0 theta_0, from observations y = M_0(t_end) M_1(t_end) M_2(t_end) of the zeroth-, first-, and second-order moments of the distribution at time t_end  0 (where t_end = 1.0 in this example). This is a known truth experiment, in which the true parameters phi_0 texttrue are defined to be:","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"N0_true = 300.0  # number of particles (scaling factor for Gamma distribution)\nθ_true = 1.5597  # scale parameter of Gamma distribution\nk_true = 0.0817  # shape parameter of Gamma distribution","category":"page"},{"location":"examples/Cloudy_example/#Priors","page":"Cloudy","title":"Priors","text":"","category":"section"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"In the code, the priors are constructed as follows:","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"par_names = [\"N0\", \"θ\", \"k\"]\n# constrained_gaussian(\"name\", desired_mean, desired_std, lower_bd, upper_bd)\nprior_N0 = constrained_gaussian(par_names[1], 400, 300, 0.4 * N0_true, Inf)\nprior_θ = constrained_gaussian(par_names[2], 1.0, 5.0, 1e-1, Inf)\nprior_k = constrained_gaussian(par_names[3], 0.2, 1.0, 1e-4, Inf)\npriors = combine_distributions([prior_N0, prior_θ, prior_k])","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"We use the recommended constrained_gaussian to add the desired scale and bounds to the prior distribution, in particular we place lower bounds to preserve positivity (and numerical stability). ","category":"page"},{"location":"examples/Cloudy_example/#Observational-Noise","page":"Cloudy","title":"Observational Noise","text":"","category":"section"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"Cloudy produces output  y = M_0(t_end) M_1(t_end) M_2(t_end), which is assumed to be related to the parameter vector theta according to:","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"    y = mathcalG(theta) + eta","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"where mathcalG = Psi circ mathcalT^-1 is the forward map, and the observational noise eta is assumed to be drawn from a  3-dimensional  Gaussian  with distribution mathcalN(0 Gamma_y). In a perfect model setting, the observational noise represents the internal model variability. Since Cloudy is a purely deterministic model, there is no straightforward way of coming up with a covariance Gamma_y for this internal noise. We decide to use a diagonal covariance with the following entries (variances):","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"Γy = convert(Array, Diagonal([100.0, 5.0, 30.0]))","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"Artificial observations (\"truth samples\") are then generated by adding random samples from eta to G_t, the forward map evaluated for the true parameters:","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"for i in 1:n_samples\n    y_t[:, i] = G_t .+ rand(MvNormal(μ, Γy))\nend\n\ntruth = Observations.Observation(y_t, Γy, data_names)","category":"page"},{"location":"examples/Cloudy_example/#Solution-and-Output","page":"Cloudy","title":"Solution and Output","text":"","category":"section"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"Cloudy_example_eki.jl: The optimal parameter vector determined by the ensemble Kalman inversion is the ensemble mean of the particles after the last iteration, which is printed to standard output. An output directory is created, where two files are stored: parameter_storage_eki.jld2 and data_storage_eki.jld2, which contain all parameters and model output from the ensemble Kalman iterations, respectively (both as DataContainers.DataContainer objects). In addition, an animation is produced that shows the evolution of the ensemble of particles over subsequent iterations of the optimization, both in the computational (unconstrained) and physical (constrained) spaces.\nCloudy_example_uki.jl: In addition to a point estimate of the optimal parameter (which is again given by the ensemble mean of the last iteration and printed to standard output), unscented Kalman inversion also provides a covariance approximation of the posterior distribution. Together, the mean and covariance allow for the reconstruction of a Gaussian approximation of the posterior distribution. The evolution of this Gaussian approximation over subsequent iterations is shown as an animation over the computational (unconstrained) space. All parameters as well as the model output from the unscented Kalman inversion are stored in an output directory, as parameter_storage_uki.jld2 and data_storage_uki.jld2. ","category":"page"},{"location":"examples/Cloudy_example/#Playing-Around","page":"Cloudy","title":"Playing Around","text":"","category":"section"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"If you want to play around with the Cloudy examples, you can e.g. change the type or the parameters of the initial cloud droplet mass distribution (see Cloudy.ParticleDistributions for the available distributions), by modifying these lines:","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"ϕ_true = [N0_true, θ_true, k_true]\ndist_true = ParticleDistributions.GammaPrimitiveParticleDistribution(ϕ_true...)","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"(Don't forget to also change dist_type accordingly).","category":"page"},{"location":"examples/Cloudy_example/","page":"Cloudy","title":"Cloudy","text":"You can also experiment with different noise covariances (Γy), priors, vary the number of iterations (N_iter) or ensemble particles (N_ens), etc.","category":"page"},{"location":"API/SparseInversion/#Sparse-Ensemble-Kalman-Inversion","page":"SparseInversion","title":"Sparse Ensemble Kalman Inversion","text":"","category":"section"},{"location":"API/SparseInversion/","page":"SparseInversion","title":"SparseInversion","text":"CurrentModule = EnsembleKalmanProcesses","category":"page"},{"location":"API/SparseInversion/","page":"SparseInversion","title":"SparseInversion","text":"SparseInversion\nsparse_eki_update\nsparse_qp","category":"page"},{"location":"API/SparseInversion/#EnsembleKalmanProcesses.SparseInversion","page":"SparseInversion","title":"EnsembleKalmanProcesses.SparseInversion","text":"SparseInversion <: Process\n\nA sparse ensemble Kalman Inversion process\n\nFields\n\nγ::AbstractFloat\nupper limit of l1-norm\nthreshold_value::AbstractFloat\nthreshold below which the norm of parameters is pruned to zero\nuc_idx::Union{Colon, AbstractVector}\nindices of parameters included in the evaluation of l1-norm constraint\nreg::AbstractFloat\na small regularization value to enhance robustness of convex optimization\n\nConstructors\n\nSparseInversion(γ, threshold_value, uc_idx, reg)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/SparseEnsembleKalmanInversion.jl:21.\n\nSparseInversion(γ)\n\ndefined at /home/runner/work/EnsembleKalmanProcesses.jl/EnsembleKalmanProcesses.jl/src/SparseEnsembleKalmanInversion.jl:31.\n\nSparseInversion()\n\n\n\n\n\n","category":"type"},{"location":"API/SparseInversion/#EnsembleKalmanProcesses.sparse_eki_update","page":"SparseInversion","title":"EnsembleKalmanProcesses.sparse_eki_update","text":" sparse_eki_update(\n    ekp::EnsembleKalmanProcess{FT, IT, SparseInversion{FT}},\n    u::AbstractMatrix{FT},\n    g::AbstractMatrix{FT},\n    y::AbstractMatrix{FT},\n    obs_noise_cov::Union{AbstractMatrix{CT}, UniformScaling{CT}},\n) where {FT <: Real, CT <: Real, IT}\n\nReturns the sparse updated parameter vectors given their current values and the corresponding forward model evaluations, using the inversion algorithm from eqns. (3.7) and (3.14) of Schneider et al. (2021).\n\nLocalization is applied following Tong and Morzfeld (2022).\n\n\n\n\n\n","category":"function"},{"location":"API/SparseInversion/#EnsembleKalmanProcesses.sparse_qp","page":"SparseInversion","title":"EnsembleKalmanProcesses.sparse_qp","text":"sparse_qp(\n    ekp::EnsembleKalmanProcess{FT, IT, SparseInversion{FT}},\n    v_j::Vector{FT},\n    cov_vv_inv::AbstractMatrix{FT},\n    H_u::SparseArrays.SparseMatrixCSC{FT},\n    H_g::SparseArrays.SparseMatrixCSC{FT},\n    y_j::Vector{FT};\n    H_uc::SparseArrays.SparseMatrixCSC{FT} = H_u,\n) where {FT, IT}\n\nSolving quadratic programming problem with sparsity constraint.\n\n\n\n\n\n","category":"function"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"EditURL = \"../../../examples/SparseLossMinimization/loss_minimization_sparse_eki.jl\"","category":"page"},{"location":"literated/loss_minimization_sparse_eki/#Minimization-of-simple-loss-functions-with-sparse-EKI","page":"Sparse Minimization Loss","title":"Minimization of simple loss functions with sparse EKI","text":"","category":"section"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"First we load the required packages.","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"using Distributions, LinearAlgebra, Random, Plots\n\nusing EnsembleKalmanProcesses\nusing EnsembleKalmanProcesses.ParameterDistributions\nconst EKP = EnsembleKalmanProcesses","category":"page"},{"location":"literated/loss_minimization_sparse_eki/#Loss-function-with-single-minimum","page":"Sparse Minimization Loss","title":"Loss function with single minimum","text":"","category":"section"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"Here, we minimize the loss function","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"G₁(u) = u - u_* ","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"where u is a 2-vector of parameters and u_* is given; here u_* = (1 0).","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"u★ = [1, 0]\nG₁(u) = [sqrt((u[1] - u★[1])^2 + (u[2] - u★[2])^2)]\nnothing # hide","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"We set the seed for pseudo-random number generator for reproducibility.","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"rng_seed = 41\nrng = Random.seed!(Random.GLOBAL_RNG, rng_seed)\nnothing # hide","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"We set a stabilization level, which can aid the algorithm convergence","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"dim_output = 1\nstabilization_level = 1e-3\nΓ_stabilization = stabilization_level * Matrix(I, dim_output, dim_output)","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"The functional is positive so to minimize it we may set the target to be 0,","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"G_target = [0]\nnothing # hide","category":"page"},{"location":"literated/loss_minimization_sparse_eki/#Prior-distributions","page":"Sparse Minimization Loss","title":"Prior distributions","text":"","category":"section"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"As we work with a Bayesian method, we define a prior. This will behave like an \"initial guess\" for the likely region of parameter space we expect the solution to live in. Here we define Normal(02^2) distributions with no constraints","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"prior_u1 = constrained_gaussian(\"u1\", 0, 2, -Inf, Inf)\nprior_u2 = constrained_gaussian(\"u1\", 0, 2, -Inf, Inf)\nprior = combine_distributions([prior_u1, prior_u2])\nnothing # hide","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"note: Note\nIn this example there are no constraints, therefore no parameter transformations.","category":"page"},{"location":"literated/loss_minimization_sparse_eki/#Calibration","page":"Sparse Minimization Loss","title":"Calibration","text":"","category":"section"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"We choose the number of ensemble members and the number of iterations of the algorithm","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"N_ensemble = 20\nN_iterations = 10\nnothing # hide","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"The initial ensemble is constructed by sampling the prior","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"initial_ensemble = EKP.construct_initial_ensemble(rng, prior, N_ensemble)","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"Sparse EKI parameters","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"γ = 1.0\nthreshold_value = 1e-2\nreg = 1e-3\nuc_idx = [1, 2]\n\nprocess = SparseInversion(γ, threshold_value, uc_idx, reg)","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"We then initialize the Ensemble Kalman Process algorithm, with the initial ensemble, the target, the stabilization and the process type (for sparse EKI this is SparseInversion).","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"ensemble_kalman_process = EKP.EnsembleKalmanProcess(initial_ensemble, G_target, Γ_stabilization, process)\nnothing # hide","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"Then we calibrate by (i) obtaining the parameters, (ii) calculate the loss function on the parameters (and concatenate), and last (iii) generate a new set of parameters using the model outputs:","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"for i in 1:N_iterations\n    params_i = get_u_final(ensemble_kalman_process)\n\n    g_ens = hcat([G₁(params_i[:, i]) for i in 1:N_ensemble]...)\n\n    EKP.update_ensemble!(ensemble_kalman_process, g_ens)\nend","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"and visualize the results:","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"u_init = get_u_prior(ensemble_kalman_process)\n\nanim_unique_minimum = @animate for i in 1:N_iterations\n    u_i = get_u(ensemble_kalman_process, i)\n\n    plot(\n        [u★[1]],\n        [u★[2]],\n        seriestype = :scatter,\n        markershape = :star5,\n        markersize = 11,\n        markercolor = :red,\n        label = \"optimum u⋆\",\n    )\n\n    plot!(\n        u_i[1, :],\n        u_i[2, :],\n        seriestype = :scatter,\n        xlims = extrema(u_init[1, :]),\n        ylims = extrema(u_init[2, :]),\n        xlabel = \"u₁\",\n        ylabel = \"u₂\",\n        markersize = 5,\n        markeralpha = 0.6,\n        markercolor = :blue,\n        label = \"particles\",\n        title = \"EKI iteration = \" * string(i),\n    )\nend\nnothing # hide","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"The results show that the minimizer of G_1 is u=u_*.","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"gif(anim_unique_minimum, \"unique_minimum_sparse.gif\", fps = 1) # hide","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"","category":"page"},{"location":"literated/loss_minimization_sparse_eki/","page":"Sparse Minimization Loss","title":"Sparse Minimization Loss","text":"This page was generated using Literate.jl.","category":"page"},{"location":"#EnsembleKalmanProcesses","page":"Home","title":"EnsembleKalmanProcesses","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"EnsembleKalmanProcesses.jl (EKP) is a library of derivative-free Bayesian optimization techniques based on ensemble Kalman Filters, a well known family of approximate filters used for data assimilation. The tools in this library enable fitting parameters found in expensive black-box computer codes without the need for adjoints or derivatives. This property makes them particularly useful when calibrating non-deterministic models, or when the training data are noisy.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Currently, the following methods are implemented in the library:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Ensemble Kalman Inversion (EKI) - The traditional optimization technique based on the Ensemble Kalman Filter EnKF (Iglesias, Law, Stuart, 2013),\nEnsemble Kalman Sampler (EKS) - also obtains a Gaussian Approximation of the posterior distribution, through a Monte Carlo integration (Garbuno-Inigo, Hoffmann, Li, Stuart, 2020),\nUnscented Kalman Inversion (UKI) - also obtains a Gaussian Approximation of the posterior distribution, through a quadrature based integration approach (Huang, Schneider, Stuart, 2022),\nSparsity-inducing Ensemble Kalman Inversion (SEKI) - Additionally adds approximate L^0 and L^1 penalization to the EKI (Schneider, Stuart, Wu, 2020).","category":"page"},{"location":"","page":"Home","title":"Home","text":"Module Purpose\nEnsembleKalmanProcesses.jl Collection of all tools\nEnsembleKalmanProcess.jl Implementations of EKI, EKS, UKI, and SEKI\nObservations.jl Structure to hold observational data\nParameterDistributions.jl Structures to hold prior and posterior distributions\nDataContainers.jl Structure to hold model parameters and outputs\nLocalizers.jl Covariance localization kernels","category":"page"},{"location":"#Learning-the-amplitude-and-vertical-shift-of-a-sine-curve","page":"Home","title":"Learning the amplitude and vertical shift of a sine curve","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"(Image: Ensemble of parameter estimates by iteration)","category":"page"},{"location":"","page":"Home","title":"Home","text":"See full example for the code.","category":"page"},{"location":"#Authors","page":"Home","title":"Authors","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"EnsembleKalmanProcesses.jl is being developed by the Climate Modeling Alliance. The main developers are Oliver R. A. Dunbar and Ignacio Lopez-Gomez.","category":"page"},{"location":"parallel_hpc/#parallel-hpc","page":"Parallelism and HPC","title":"Parallelism and High Performance Computing (HPC)","text":"","category":"section"},{"location":"parallel_hpc/","page":"Parallelism and HPC","title":"Parallelism and HPC","text":"One benefit of ensemble methods is their ability to be parallelized. The parallellism occurs outside of the EKP update, and so is not present in the source code. On this page we provide suggestions for parallelization for two types of problems:","category":"page"},{"location":"parallel_hpc/","page":"Parallelism and HPC","title":"Parallelism and HPC","text":"Run a parallel loop or map within your current Julia session.\nRun a parallel loop through a read/write file interface and workload manager. We provide some utilities to read/write files in a TOML format.","category":"page"},{"location":"parallel_hpc/#Case-1:-Parallel-code-within-Julia","page":"Parallelism and HPC","title":"Case 1: Parallel code within Julia","text":"","category":"section"},{"location":"parallel_hpc/","page":"Parallelism and HPC","title":"Parallelism and HPC","text":"Let's look at the simple example for the Lorenz 96 dynamical system. In particular we'll focus on the evaluations of the Lorenz 96 solver explicitly written in GModel.jl that contains the following loop over the N_ens-sized ensemble:","category":"page"},{"location":"parallel_hpc/","page":"Parallelism and HPC","title":"Parallelism and HPC","text":"function run_ensembles(settings, lorenz_params, nd, N_ens)\n    g_ens = zeros(nd, N_ens)\n    for i in 1:N_ens\n        # run the model with the current parameters, i.e., map θ to G(θ)\n        g_ens[:, i] = lorenz_forward(settings, lorenz_params[i])\n    end\n    return g_ens\nend","category":"page"},{"location":"parallel_hpc/","page":"Parallelism and HPC","title":"Parallelism and HPC","text":"Each ensemble member i runs the Lorenz 96 model with settings configuration, and model parameters lorenz_params[i]. The runs do not interact with each other, and the user has several options to parallelize.","category":"page"},{"location":"parallel_hpc/#Running-examples:","page":"Parallelism and HPC","title":"Running examples:","text":"","category":"section"},{"location":"parallel_hpc/","page":"Parallelism and HPC","title":"Parallelism and HPC","text":"All of the following example cases are covered in distributed_Lorenz_example.jl. At the top of file uncomment one of the following options","category":"page"},{"location":"parallel_hpc/","page":"Parallelism and HPC","title":"Parallelism and HPC","text":"case = multithread \ncase = pmap \ncase = distfor ","category":"page"},{"location":"parallel_hpc/#Multithreading,-@threads","page":"Parallelism and HPC","title":"Multithreading, @threads","text":"","category":"section"},{"location":"parallel_hpc/","page":"Parallelism and HPC","title":"Parallelism and HPC","text":"To parallelize with multithreading, julia must call the file with a prespecified number of threads. For example, for 4 threads, ","category":"page"},{"location":"parallel_hpc/","page":"Parallelism and HPC","title":"Parallelism and HPC","text":"$ julia --project -t 4 distributed_Lorenz_example.jl","category":"page"},{"location":"parallel_hpc/","page":"Parallelism and HPC","title":"Parallelism and HPC","text":"We exploit the multithreading over N_ens ensemble members in this example with the following loop in GModel_multithread.jl:","category":"page"},{"location":"parallel_hpc/","page":"Parallelism and HPC","title":"Parallelism and HPC","text":"function run_ensembles(settings, lorenz_params, nd, N_ens)\n    g_ens = zeros(nd, N_ens)\n    Threads.@threads for i in 1:N_ens\n        # run the model with the current parameters, i.e., map θ to G(θ)\n        g_ens[:, i] = lorenz_forward(settings, lorenz_params[i])\n    end\n    return g_ens\nend","category":"page"},{"location":"parallel_hpc/","page":"Parallelism and HPC","title":"Parallelism and HPC","text":"You can read more about multi-threading here.","category":"page"},{"location":"parallel_hpc/#Parallel-map,-pmap","page":"Parallelism and HPC","title":"Parallel map, pmap","text":"","category":"section"},{"location":"parallel_hpc/","page":"Parallelism and HPC","title":"Parallelism and HPC","text":"When using multiple processes, the Julia environment must first be loaded on each worker processor. We include these lines in the main file","category":"page"},{"location":"parallel_hpc/","page":"Parallelism and HPC","title":"Parallelism and HPC","text":"using Distributed\naddprocs(4; exeflags = \"--project\")","category":"page"},{"location":"parallel_hpc/","page":"Parallelism and HPC","title":"Parallelism and HPC","text":"And we would call the file is called by","category":"page"},{"location":"parallel_hpc/","page":"Parallelism and HPC","title":"Parallelism and HPC","text":"$ julia --project distributed_Lorenz_example","category":"page"},{"location":"parallel_hpc/","page":"Parallelism and HPC","title":"Parallelism and HPC","text":"This ensures that we obtain 4 worker processes that are loaded with julia's current environment specified by --project (unlike when calling julia --project -p 4). We use  pmap to apply a function to each element of the list (i.e the ensemble member configurations). For example, see the following code from GModel_pmap.jl,","category":"page"},{"location":"parallel_hpc/","page":"Parallelism and HPC","title":"Parallelism and HPC","text":"using Distributed\nfunction run_ensembles(settings, lorenz_params, nd, N_ens)\n    g_ens = zeros(nd, N_ens)\n    g_ens[:, :] = vcat(pmap(x -> lorenz_forward(settings, x), lorenz_params)...)\n    return g_ens\nend","category":"page"},{"location":"parallel_hpc/","page":"Parallelism and HPC","title":"Parallelism and HPC","text":"If pmap is called within a module, that module will also need to be loaded on all workers. For this we use the macro @everywhere module XYZ.","category":"page"},{"location":"parallel_hpc/","page":"Parallelism and HPC","title":"Parallelism and HPC","text":"You can read more about pmap here.","category":"page"},{"location":"parallel_hpc/#Distributed-loop,-@distributed-for","page":"Parallelism and HPC","title":"Distributed loop, @distributed for","text":"","category":"section"},{"location":"parallel_hpc/","page":"Parallelism and HPC","title":"Parallelism and HPC","text":"When using multiple processes, the Julia environment must also be loaded on each worker processor. We include these lines in the main file","category":"page"},{"location":"parallel_hpc/","page":"Parallelism and HPC","title":"Parallelism and HPC","text":"using Distributed\naddprocs(4; exeflags = \"--project\")","category":"page"},{"location":"parallel_hpc/","page":"Parallelism and HPC","title":"Parallelism and HPC","text":"And we would call the file is called by","category":"page"},{"location":"parallel_hpc/","page":"Parallelism and HPC","title":"Parallelism and HPC","text":"$ julia --project distributed_Lorenz_example","category":"page"},{"location":"parallel_hpc/","page":"Parallelism and HPC","title":"Parallelism and HPC","text":"When using distributed loops, it is necessary to be able to write to shared memory. To do this we use the SharedArrays package. For example, see the following distributed loop in GModel_distfor ","category":"page"},{"location":"parallel_hpc/","page":"Parallelism and HPC","title":"Parallelism and HPC","text":"using Distributed\nusing SharedArrays\nfunction run_ensembles(settings, lorenz_params, nd, N_ens)\n    g_ens = SharedArray{Float64}(nd, N_ens)\n    @sync @distributed for i in 1:N_ens\n        # run the model with the current parameters, i.e., map θ to G(θ)\n        g_ens[:, i] = lorenz_forward(settings, lorenz_params[i])\n    end\n    return g_ens\nend","category":"page"},{"location":"parallel_hpc/","page":"Parallelism and HPC","title":"Parallelism and HPC","text":"@sync forces the code to wait until all processes in the @distributed for loop are complete before continuing.","category":"page"},{"location":"parallel_hpc/","page":"Parallelism and HPC","title":"Parallelism and HPC","text":"If @distributed for is used within another module, that module will also need to be loaded on each worker processor. For this we use the macro @everywhere module XYZ.","category":"page"},{"location":"parallel_hpc/","page":"Parallelism and HPC","title":"Parallelism and HPC","text":"note: Note\n@distributed for is most performant when there is a large ensemble, N_ens, and the forward map is computationally cheap. Otherwise, pmap is usually the preferred choice.","category":"page"},{"location":"parallel_hpc/","page":"Parallelism and HPC","title":"Parallelism and HPC","text":"You can read more about @distributed here","category":"page"},{"location":"parallel_hpc/#Case-2:-HPC-interface","page":"Parallelism and HPC","title":"Case 2: HPC interface","text":"","category":"section"},{"location":"parallel_hpc/","page":"Parallelism and HPC","title":"Parallelism and HPC","text":"Some applications involve interfacing with non-Julia code or using HPC workload managers. In these cases we suggest using an alternative workflow where one interleaves scripts that launch EKP updates and scripts that runs the model. One possible implementation is the following loop","category":"page"},{"location":"parallel_hpc/","page":"Parallelism and HPC","title":"Parallelism and HPC","text":"Step 0(a). Write an ensemble of parameter files parameters_0_i for i = 1:N_ens, with each parameter file containing a sample from the prior distribution.\nStep 0(b). Construct EKP EKP object and save in jld2 format e.g. ekpobject.jld2.\nfor n = 1,..., N_it, do:\nStep n(a). Run N_ens forward models, with forward model i running with the corresponding parameter file parameters_{n-1}_i. Write processed output data to file data_{n-1}_i.\nStep n(b). Run the ensemble update, by loading both ekpobject.jld2 and reading in the parameter files data_{n-1}_i for i = 1:N_ens. Perform the EKP update step. Write new parameter files parameters_n_i for i = 1:N_ens. Save the ensemble object in ekpobject.jld2\niterate n -> n+1.","category":"page"},{"location":"parallel_hpc/","page":"Parallelism and HPC","title":"Parallelism and HPC","text":"In HPC interfacing example: ClimateMachine we implement a similar loop to interface with a SLURM workload manager for HPC. Here, sbatch scripts are used to run each component of the calibration procedure. The outer loop over the EKP iterations lives in the overarching sbatch script, and for each iteration, the inner loop are realised as \"arrays\" of slurm jobs (1, ..., N_ens), launched for each ensemble member. The code excerpt below, taken from ekp_calibration.sbatch for details), illustrates this procedure:","category":"page"},{"location":"parallel_hpc/","page":"Parallelism and HPC","title":"Parallelism and HPC","text":"# First call to calibrate.jl will create the ensemble files from the priors\nid_init_ens=$(sbatch --parsable ekp_init_calibration.sbatch)\nfor it in $(seq 1 1 $n_it)\ndo\n# Parallel runs of forward model\nif [ \"$it\" = \"1\" ]; then\n    id_ens_array=$(sbatch --parsable --kill-on-invalid-dep=yes --dependency=afterok:$id_init_ens --array=1-$n ekp_single_cm_run.sbatch $it)\nelse\n    id_ens_array=$(sbatch --parsable --kill-on-invalid-dep=yes --dependency=afterok:$id_ek_upd --array=1-$n ekp_single_cm_run.sbatch $it)\nfi\nid_ek_upd=$(sbatch --parsable --kill-on-invalid-dep=yes --dependency=afterok:$id_ens_array --export=n=$n ekp_cont_calibration.sbatch $it)\ndone","category":"page"},{"location":"parallel_hpc/","page":"Parallelism and HPC","title":"Parallelism and HPC","text":"Here a dependency tree is set up in SLURM, which iterates calls to the scripts ekp_single_cm_run.sbatch (which runs the forward model on HPC) and ekp_cont_calibration.sbatch (which performs an EKI update). We find this a smooth workflow that uses HPC resources well, and can likely be set up on other workload managers.","category":"page"},{"location":"parallel_hpc/","page":"Parallelism and HPC","title":"Parallelism and HPC","text":"For more details see the code and docs for the HPC interfacing example: ClimateMachine.","category":"page"},{"location":"API/DataContainers/#DataContainers","page":"DataContainers","title":"DataContainers","text":"","category":"section"},{"location":"API/DataContainers/","page":"DataContainers","title":"DataContainers","text":"CurrentModule = EnsembleKalmanProcesses.DataContainers","category":"page"},{"location":"API/DataContainers/","page":"DataContainers","title":"DataContainers","text":"DataContainer\nPairedDataContainer\nsize","category":"page"},{"location":"API/DataContainers/#EnsembleKalmanProcesses.DataContainers.DataContainer","page":"DataContainers","title":"EnsembleKalmanProcesses.DataContainers.DataContainer","text":"DataContainer{FT <: Real}\n\nContainer to store data samples as columns in an array.\n\n\n\n\n\n","category":"type"},{"location":"API/DataContainers/#EnsembleKalmanProcesses.DataContainers.PairedDataContainer","page":"DataContainers","title":"EnsembleKalmanProcesses.DataContainers.PairedDataContainer","text":"PairedDataContainer{FT <: Real}\n\nStores input - output pairs as data containers, there must be an equal number of inputs and outputs.\n\n\n\n\n\n","category":"type"},{"location":"API/DataContainers/#Base.size","page":"DataContainers","title":"Base.size","text":"size(dc::DataContainer, idx::IT) where {IT <: Integer}\n\nReturns the size of the stored data. If idx provided, it returns the size along dimension idx.\n\n\n\n\n\nsize(pdc::PairedDataContainer, idx::IT) where {IT <: Integer}\n\nReturns the sizes of the inputs and ouputs along dimension idx (if provided).\n\n\n\n\n\nsize(c<:ConstraintType)\n\nA constraint has size 1.\n\n\n\n\n\n","category":"function"}]
}
